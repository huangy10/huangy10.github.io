<!DOCTYPE html>












  


  


<html class="theme-next pisces use-motion han-js-rendered no-han-space" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  <!--  -->
  
  
  <link rel="stylesheet" href="https://ethantw.github.io/Han/latest/han.min.css">



  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">














  
  
  
  

  

  

  

  

  

  







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">



  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico?v=7.1.0">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="首先我们要解决的是「什么是增强学习」，即Reinforcement Learning，的问题。为了学习这一点我选择了比较权威的增强学习教材，Reinforcement Learning: An Introduction。这本书是增强学习的开创者安德鲁·巴托和理查德·S·萨顿所撰写。 之前接触过的典型的神经网络，比如用于处理图像输入的卷积神经网络(CNN)和擅长处理自然语言的递归神经网络(RNN)，">
<meta property="og:type" content="website">
<meta property="og:title" content="增强学习">
<meta property="og:url" content="http://www.codewoody.com/knowledge-base/academic/ml/reinforcement-learning/index.html">
<meta property="og:site_name" content="治部少辅">
<meta property="og:description" content="首先我们要解决的是「什么是增强学习」，即Reinforcement Learning，的问题。为了学习这一点我选择了比较权威的增强学习教材，Reinforcement Learning: An Introduction。这本书是增强学习的开创者安德鲁·巴托和理查德·S·萨顿所撰写。 之前接触过的典型的神经网络，比如用于处理图像输入的卷积神经网络(CNN)和擅长处理自然语言的递归神经网络(RNN)，">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://imgs.codewoody.com/uploads/big/026c46d807fe6b3416d4b736dbe7a3fe.png">
<meta property="og:image" content="https://imgs.codewoody.com/uploads/big/bca0560bf5558327eff54e3e0c1cada6.png">
<meta property="og:image" content="https://imgs.codewoody.com/uploads/big/4d2725e97cfc8038c1a27710ae082c0b.png">
<meta property="og:updated_time" content="2020-03-05T15:52:46.621Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="增强学习">
<meta name="twitter:description" content="首先我们要解决的是「什么是增强学习」，即Reinforcement Learning，的问题。为了学习这一点我选择了比较权威的增强学习教材，Reinforcement Learning: An Introduction。这本书是增强学习的开创者安德鲁·巴托和理查德·S·萨顿所撰写。 之前接触过的典型的神经网络，比如用于处理图像输入的卷积神经网络(CNN)和擅长处理自然语言的递归神经网络(RNN)，">
<meta name="twitter:image" content="https://imgs.codewoody.com/uploads/big/026c46d807fe6b3416d4b736dbe7a3fe.png">



  <link rel="alternate" href="/atom.xml" title="治部少辅" type="application/atom+xml">



  
  
  <link rel="canonical" href="http://www.codewoody.com/knowledge-base/academic/ml/reinforcement-learning/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>增强学习 | 治部少辅</title>
  




  <script async src="//www.googletagmanager.com/gtag/js?id=UA-114736006-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-114736006-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">治部少辅</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">大一大万大吉</h1>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-knowledge menu-item-active">

    
    
    
      
    

    

    <a href="/knowledge-base" rel="section"><i class="menu-item-icon fa fa-fw fa-book"></i> <br>knowledge</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-weekly">

    
    
    
      
    

    

    <a href="/categories/Weekly/" rel="section"><i class="menu-item-icon fa fa-fw fa-newspaper-o"></i> <br>weekly</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-updates">

    
    
    
      
    

    

    <a href="/update" rel="section"><i class="menu-item-icon fa fa-fw fa-edit"></i> <br>updates</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-channel">

    
    
    
      
    

    

    <a href="https://t.me/everthingaboutbullshit" rel="noopener" target="_blank"><i class="menu-item-icon fa fa-fw fa-signal"></i> <br>channel</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  

  

  <a href="https://github.com/huangy10" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

    
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
    

  


          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h2 class="post-title" itemprop="name headline">增强学习

</h2>

<div class="post-meta">
  
  


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/knowledge-base/">KNOWLEDGE-BASE</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/knowledge-base/academic/">ACADEMIC</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/knowledge-base/academic/ml/">ML</a></li>
          
        
      
    
      
      
        
          <li>REINFORCEMENT-LEARNING</li>
        
      
    
      
      
    
  </ul>


</div>

</header>

      
      
      
      <div class="post-body han-init-context">
        
        
          <p>首先我们要解决的是「什么是增强学习」，即Reinforcement Learning，的问题。为了学习这一点我选择了比较权威的增强学习教材，<a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement Learning: An Introduction</a>。这本书是增强学习的开创者安德鲁·巴托和理查德·S·萨顿所撰写。</p>
<p>之前接触过的典型的神经网络，比如用于处理图像输入的卷积神经网络(CNN)和擅长处理自然语言的递归神经网络(RNN)，其特点都是在已有数据集上训练处模型以后，将模型用于生产环境。模型的训练和模型的使用是分离的。其实从人类自身的经验来说，学习和应用的过程其实是一体的。人通过与环境的交互一边学习，一边应用，互相促进。增强学习就是基于这种方式的机器学习手段。具体而言：</p>
<blockquote>
<p>We explore the desing for machines that are effective in solving learning problems of scientific or economic interest, evaluating the designs through mathematical analysis or computational experiments. <strong>The approach we explore, called reinforcement learning, is much more forcused on goad-directed learning from interaction than are other approaches to machine learning</strong></p>
</blockquote>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p>增强学习的定义为</p>
<blockquote>
<p>Reinforcement learning is learning what to do -- how to mapping situations to actions -- so as to maximize a numerical reward signal.</p>
</blockquote>
<p>即，增强学习是用学习如何做出决策，也就是建立起状态(Situation)到行动(Action)的映射，以使得目标奖励信号最大化。注意这里的奖励可能并不是即时，而是可能出现在后续的状态中。因此，增强学习最鲜明的特点是<strong>尝试-错误(trial-and-error)</strong>，以及<strong>延时奖励(delayed reward)</strong>。</p>
<blockquote>
<p>需要指出的是，很多以-ing结尾的术语其实表示了多重的概念。在增强学习中，Reinforcement Learning同时表达了，问题、方案、以及研究领域。在增强学习中要格外注意问题与方案这两个概念。</p>
</blockquote>
<p>增强学习的概念来源于动态系统控制理论(Dynamic system theory)，特别的，来自于的非全知<span class="foot-note-span">【原文作incompletely-known】</span>的马尔科夫决策过程的最优化控制。在增强学习中，一个agent必须要要有感知环境状态的能力，具有能够影响状态的行动力。同时这个agent还需要被赋予一个与环境有关的执行目标。这三个要素就是感知，行动和目标 (Sensation, action and goal)。任何用于解决这类问题的方法都可以被视为增强学习方法。</p>
<h3 id="增强学习和监督学习的区别">增强学习和监督学习的区别</h3>
<p>增强学习不同于监督学习(Supervised learning)。监督学习是从一组由外部监督者提供的标注好的训练集合上学习。这种训练集一般提供了一组环境，以及各个环境下应该采取的决策(label)。监督学习的目标一般是将控制策略进行推广，一般化，是的控制器能够对于没有出现在训练集上的环境也能做出反应。不过这种学习方式不符合增强学习从交互中学习的特点(Learning from interaction)。对于交互式问题，一般很难找到有充足代表性和足够正确的训练集。</p>
<h3 id="增强学习与非监督学习的区别">增强学习与非监督学习的区别</h3>
<p>非监督学习一般的目的是寻找数据集合中的隐藏结构性。非监督学习的这种特点对于增强学习非常有用，但是只涵盖了增强学习的一部分概念。</p>
<p>综上，作者认为增强学习是于监督学习，非监督学习之后的第三种机器学习范式。</p>
<h3 id="增强学习面临的挑战">增强学习面临的挑战</h3>
<p>增强学习需要解决的一个独有的问题是，如何平衡探索(Exploration)和使用(Exploitation)。为了在奖励函数上获得好的增益，agent一般会偏好已经尝试过的好的策略。但同时为了发现更多的这种好的策略，agent还需要尝试新的决策。</p>
<blockquote>
<p>The agent has to exploit what is already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future.</p>
</blockquote>
<p>针对Exploration-Exploitation的矛盾由来已久。而这类问题其实不是监督学习或者非监督学习所关心的。这也是为什么，作者要将增强学习列为新的机器学习范式。</p>
<h3 id="增强学习元素">增强学习元素</h3>
<p>增强学习的系统的主要元素包括：</p>
<ol type="1">
<li>a policy</li>
<li>a reward signal</li>
<li>a value function</li>
<li>(Optionally) a model of the environment</li>
</ol>
<h4 id="a-policy">A Policy</h4>
<p>政策定义了agent在给定时刻的行为方式。粗略来说，政策是从环境中感知到的状态到需要执行的操作的映射。有时候，Policy可能是一个简单的函数或者一个查询表，而有时候这个映射可能会涉及非常复杂的运算。</p>
<h4 id="a-reward-signal">A Reward Signal</h4>
<p>Reward Signal定义了增强学习的目标。每次迭代时，环境向agent发送一个单独的数字(reward)。Agent在长期的执行中致力于最大化整体reward。Reward Signal定义了Agent遇到的事件的好与坏。在生物学意义上，reward可以类比于生物在与环境交互中得到的愉悦或者痛苦的体验。同时，Reward也是agent面临的问题的特征的直接表现。Reward是政策调整的最主要因素。</p>
<h4 id="a-value-function">A Value Function</h4>
<p>在已经有了Reward Signal的基础上引入Value函数的意义在与评估政策在更长期的尺度上的影响。简单来说，一个状态的value是之后未来agent能够收益到的reward的总量。Reward定义了环境能够基于的立即的、直接的响应，value则考虑了长期的影响。</p>
<p>进一步我们可以发现，一般Reward的定义是非常直接的，如何定于Value则是在建模过程中需要慎重考虑的问题。事实上我们可以发信啊在增强学习的研究中，最重要的一步通常是给出有效的估计Value的方法。</p>
<blockquote>
<p>The central role of value estimation is arguably the most important thing that has been learned about reinforcement learning over the last six decades.</p>
</blockquote>
<h4 id="a-model-of-the-environment">A Model of the Environment</h4>
<p>对环境的建模就是对于问题本身的建模。模型要求能够反映环境的特点。在给定一个状态和行为时，模型要能够预测下一个时刻的状态以及相应的Reward。Model被用来做长期的计划性质的决策。上面提到Model是可选的因为还存在Model-free的方法。在Model-free的方法中，agent通过trial-and-error的策略来和环境交互。</p>
<h2 id="tabular-soluation-methods">Tabular Soluation Methods</h2>
<blockquote>
<p>为了准确性，之后术语部分还是直接使用英文记录</p>
</blockquote>
<p>在这个部分通过一些比较简单的问题形式来探讨Reinforcement Learning的核心概念。在这些例子中，State和Action空间足够小，可以表示为数组或者表(tables)。这种情况下，理论上的我们是可以找到精确的最优解的。这和后面我们要说的Approximiate methods不同。</p>
<h3 id="多臂老虎机问题multi-armed-bandits">多臂老虎机问题(Multi-armed Bandits)</h3>
<p>考虑下面的学习问题。你不断重复地面临一个有<span class="math inline">\(k\)</span>个不同选项（即Actions)的选择问题。在每轮选择之后你会获得一个量化的Reward，Reward的分布是一个依赖于你的决策的平稳分布<span class="foot-note-span">【平稳分布即这个分布不是时变的，强调只依赖于你的决策本身】</span>。你的目的是最大化一段时间（比如1000轮选择）的总收益。</p>
<p>这是多臂老虎机问题的的原始形式。多臂老虎机问题是对<a href="https://zh.wikipedia.org/wiki/%E8%A7%92%E5%AD%90%E6%A9%9F" target="_blank" rel="noopener">老虎机</a>的一个类比，只不过在这个问题中，有<span class="math inline">\(k\)</span>个拉杆。每个动作的选择都像是拉动老虎机的拉杆，而奖励就是击中奖的收益。 通过反复的动作选择，您可以将动作集中在最佳杠杆上，从而最大程度地赢取奖金。 另一个比喻是，医生为一系列重病患者选择实验治疗。 每个动作都是治疗的选择，每个奖励都是患者的生存或福祉。 如今赌博机问题有时会指代更加泛化的问题集合，但是在这里我们只用来称呼这个简单的版本。</p>
<p>在我们的K臂老虎机问题中，<span class="math inline">\(k\)</span>种行动中的每一种都有一个期望奖励。我们称之为这种行动的<i>value</i>。我们记在时刻<span class="math inline">\(t\)</span>选择的行为时<span class="math inline">\(A_t\)</span>，对应的奖励是<span class="math inline">\(R_t\)</span><span class="foot-note-span">【注意区分这里是时刻的奖励，前面说的value是期望】</span>，对于任意行为<span class="math inline">\(a\)</span>的value，我们记为$q_*(a)，这是一个期望值：</p>
<p><span class="math display">\[
q_{*}(a) \doteq \mathbb{E}\left[R_{t} | A_{t}=a\right]
\]</span></p>
<p>如果你能够知道每个行为的value值，那么解决k臂老虎机问题就非常简单了，只需要总是选择value最高的行为即可。我们假设你并不确定知道每个行为的value值，但是你可以对其有一个估计。我们记在时刻<span class="math inline">\(t\)</span>的估计值为<span class="math inline">\(Q_t(a)\)</span>。我们希望<span class="math inline">\(Q_t(a)\)</span>能够接近<span class="math inline">\(q_*(a)\)</span>。</p>
<p>如果你始终维护对行为value值的估计，那么在任何时刻你总是能找到估计值最大的那个行为。我们将这些行为成为贪婪行为(Greedy Actions)。当你选择这些行为时，我们称你正在利用(Exploiting)对于这些行为的value的现有知识。如果你没有选择这些贪婪行为，则表示你正在进行探索(Exploring)，因为这可以使你改善对非贪婪行为的value值的估计。利用现有知识的选择可以让你在单步操作中最大化期望奖励，但是探索可以在长期的尺度上产生更大的value总量。例如，假设贪婪行动的价值是确定的，而其他几项行动的评估结果几乎相同，但存在很大的不确定性。这种不确定性使得这些其他动作中的至少一个实际上可能比贪婪的动作要好，但是您不知道哪个。如果你在决策前还有很多时间，那么探索非贪婪的行动并发现其中哪个比贪婪的行动更好。在短期内，探索过程中的奖励较低，但从长期来看，奖励较高，因为发现更好的操作后，您可以多次使用它们。由于不可能通过任何一个单一的动作选择来进行探索和利用，因此人们经常提到探索与利用之间的“冲突”。</p>
<p>在任何特殊的场景下，探索还是利用哪个更好，会以一种很复杂的方式依赖于value估计精确值，不确定性，以及剩余的时间步骤。对于特定的k臂匪徒问题的数学形式，有很多复杂的方法可以平衡探索和利用。不过这些方法对于平稳性和先验证知识有很强的假设，这些假设要么难以实现，要么很难获取。</p>
<p>在这里我们无需担心需要以很复杂的方式去平衡探索和利用，我们只需要关注在整体上的平衡问题。在本章中，我们介绍了几种用于解决K臂强盗问题的简单平衡方法，并表明它们比总是利用的方法好得多。 平衡探索与开发的需要是强化学习中出现的一个独特挑战。我们的第K臂强盗问题的简单性使我们能够以特别清晰的形式展示这一点。</p>
<h3 id="行动-价值方法">行动-价值方法</h3>
<p>我们从研究对行为的value估计，及基于这些估计做出行为决策的方法开始。这类方法我们称为行动-价值(Action-Value)方法。一个行为的value的真值是行为被选择时产生的平均回报。一个直接的方法直接对于我们已经取得的奖励取平均。</p>
<p><span class="math display">\[
Q_t(a) \doteq \frac{\text{sum of rewards when} a \text{taken prior to} t}{\text{number of times} a \text{taken prior to} t} = \frac{\sum_{i=1}^{t-1} R_{i} \cdot \mathbb{1}_{A_{i}=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_{i}=a}}
\]</span></p>
<p>其中<span class="math inline">\(\mathbb{1}_{predicte}\)</span>为指示函数，当下面的条件为真时取1，否则为0。当上式中分母为0时，<span class="math inline">\(Q_t(a)\)</span>为默认值，例如0。当分母为无穷大时，理论上上式就会收敛到<span class="math inline">\(q_*(a)\)</span>。我们把这种估计行为value值的方法称为<i>sample-average</i>方法。 当然，这只是估计行动价值的一种方法，不一定是最佳方法。 尽管如此，现在让我们继续使用这种简单的估算方法，然后转向如何将估算值用于选择行动的问题。</p>
<p>最简单的行动选择策略是选择value估计值最高的行动，即上一节中定义的贪婪策略。如果贪婪策略给出选项不只一种，则在其中随机选择。这种贪婪选择策略可以表示为:</p>
<p><span class="math display">\[
A_{t} \doteq \underset{a}{\arg \max } Q_{t}(a)
\]</span></p>
<p>贪婪策略总是利用现有的知识来最大化当即的收益，其并不花费任何时间来尝试哪些明显更差一些的行为来看看这些行为会不会产生更好的结果。一个简单的改进策略是在大多数时候采用贪婪策略，但是偶尔，例如以概率<span class="math inline">\(\varepsilon\)</span>，在所有行为中以均等的概率随机选择。我们将这种策略称为<span class="math inline">\(\varepsilon\)</span>-<i>greedy</i>方法。这种方法的好处是，随着步骤的增加，所有的行为都可以被尝试无限次， 从而确保所有的<span class="math inline">\(Q_t(a)\)</span>都会收敛到<span class="math inline">\(q_*(a)\)</span>。这也意味着选中最优的行为的概率会收敛到超过<span class="math inline">\(1-\epsilon\)</span>的水平的，接近于1。上面这些都是理论上渐进收敛的保证，在实际使用中是什么效果还有待考察。</p>
<h3 id="臂测试问题">10臂测试问题</h3>
<p>我们用一个具体的尝试来看看贪婪策略和<span class="math inline">\(\varepsilon\)</span>-<i>greedy</i>方法的效果，并进行对比。这里使用到的测试数据是2000个随机生成的10臂老虎机问题，即<span class="math inline">\(k=10\)</span>。对于每个老虎机问题，例如如下图所示，每个行为的的value值<span class="math inline">\(q_*(a), a = 1, 2, \dots, 10\)</span>，基于一个<span class="math inline">\(N(0,1)\)</span>的正态分布随机选出。实际的奖励值<span class="math inline">\(R_t\)</span>则是以<span class="math inline">\(q_*(A_t)\)</span>为均值，方差为1的正态分布。</p>
<p><img src="https://imgs.codewoody.com/uploads/big/026c46d807fe6b3416d4b736dbe7a3fe.png"></p>
<p>在这个测试集合的基础上我们可以评估学习方法的性能。对于每一个赌博机问题，我们执行1000步以观察模型的改善情况。然后我们尝试2000组不同参数定义的赌博机问题，从而得到一个平均的性能评估结果。</p>
<p>下图比较了贪婪策略和两个<span class="math inline">\(\varepsilon\)</span>-<i>greedy</i>策略(<span class="math inline">\(\varepsilon = 0.01\)</span>和<span class="math inline">\(\epsilon= 0.1\)</span>)。其中上面的图给出了随着训练步数的增加，获得期望奖励的增长情况。贪婪策略在初期能够有更快的增长，但是最终的水平要更低一些。理论上每一步的最佳改进是1.55，但是的贪婪策略之取得了大约1的增益。贪婪策略失败的原因在于其常常被困在次优行为附近。下面的图显示了贪婪策略大约只在三分之一的任务中选中了最优的行为，在其他三分之二的情形下，最优行为的初始采样结果可能不佳，因此贪婪策略就再不会尝试选择这一选项。<span class="math inline">\(\varepsilon\)</span>-<i>greedy</i>方法最终表现的更好，因为其总是尝试新的行为并提升其发现最优行为的能力。当<span class="math inline">\(\varepsilon=0.1\)</span>时，由于尝试的比例更多，所以可以更快速地发现最佳的方法。但是选中最优行为的概率被局限为<span class="math inline">\(91%\)</span>。<span class="math inline">\(\varepsion=0.01\)</span>时，发现最优策略会比较慢，但最终的稳定的性能表现会优于<span class="math inline">\(\varepsilon=0.1\)</span>的情形。</p>
<p><img src="https://imgs.codewoody.com/uploads/big/bca0560bf5558327eff54e3e0c1cada6.png"></p>
<p>不过，<span class="math inline">\(\varepsilon\)</span>-<i>greedy</i>方法和贪婪策略的区别还是取决于任务类型。设想如果奖励分布的方差非常大，那么噪声水平会带来更大的影响，使得贪婪策略更难选中最优的行动。但是反过来如果奖励分布的方差为0，那么贪婪策略总是可以在第一时间选中最优的行为。即便在这种确定性的情形中，如果我们弱化其他的假设，使用探索功能也会带来更大的优势。例如假设赌博任务是非平稳的，即行为的value真值会随着时间变化而变化。</p>
<h3 id="增量计算实现">增量计算实现</h3>
<p>这部介绍的是如何将之前提到的行动-价值方法的计算过程优化。为了简化问题表达我们先关注一个单独的行为。令<span class="math inline">\(R_i\)</span>表示第<span class="math inline">\(i\)</span>次选中这个行为的奖励，让<span class="math inline">\(Q_n\)</span>表示这这个行为已经被选中<span class="math inline">\(n - 1\)</span>次以后的行为value的估计值。那么</p>
<p><span class="math display">\[
Q_{n} \doteq \frac{R_{1}+R_{2}+\cdots+R_{n-1}}{n-1}
\]</span></p>
<p>直接的实现方法是把所有的<span class="math inline">\(R_i\)</span>存下来，然后计算均值。但是这样在大规模的问题中会消耗非常多的计算资源。下面我们推到递推的计算方法：</p>
<p><span class="math display">\[
\begin{aligned}
Q_{n+1} &amp;=\frac{1}{n} \sum_{i=1}^{n} R_{i} \\
&amp;=\frac{1}{n}\left(R_{n}+\sum_{i=1}^{n-1} R_{i}\right) \\
&amp;=\frac{1}{n}\left(R_{n}+(n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_{i}\right) \\
&amp;=\frac{1}{n}\left(R_{n}+(n-1) Q_{n}\right) \\
&amp;=\frac{1}{n}\left(R_{n}+n Q_{n}-Q_{n}\right) \\
&amp;=Q_{n}+\frac{1}{n}\left[R_{n}-Q_{n}\right]
\end{aligned}
\]</span></p>
<p>这种递推方式在后续还会非常常见，其一般形式为</p>
<p><span class="math display">\[
NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]
\]</span></p>
<p>这里的<span class="math inline">\(StepSize\)</span>一般记为的<span class="math inline">\(\alpha\)</span>，或者<span class="math inline">\(\alpha_t(a)\)</span></p>
<h3 id="追踪非平稳问题">追踪非平稳问题</h3>
<p>上面说的用平均方法估计value的方法只适用于平稳的老虎机问题，即奖励的分布要是不变的。这个部分我们来讨论非平稳问题的处理。</p>
<p>非平稳问题的处理方法一般是使用一个常量的<span class="math inline">\(StepSize\)</span>参数</p>
<p><span class="math display">\[
Q_{n+1} \doteq Q_{n}+\alpha\left[R_{n}-Q_{n}\right]
\]</span></p>
<p>其中<span class="math inline">\(\alpha \in (0, 1]\)</span>为一个常数，<span class="math inline">\(Q_{n + 1}\)</span>为过去的奖励的加权平均。注意到</p>
<p><span class="math display">\[
\begin{aligned}
Q_{n+1} &amp;=Q_{n}+\alpha\left[R_{n}-Q_{n}\right] \\
&amp;=\alpha R_{n}+(1-\alpha) Q_{n} \\
&amp;=\alpha R_{n}+(1-\alpha)\left[\alpha R_{n-1}+(1-\alpha) Q_{n-1}\right] \\
&amp;=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} Q_{n-1} \\
&amp;=\alpha R_{n}+(1-\alpha) \alpha R_{n-1}+(1-\alpha)^{2} \alpha R_{n-2}+\\
&amp; \cdots+(1-\alpha)^{n-1} \alpha R_{1}+(1-\alpha)^{n} Q_{1} \\
&amp;=(1-\alpha)^{n} Q_{1}+\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} R_{i}
\end{aligned}
\]</span></p>
<p>且<span class="math inline">\((1 - \alpha)^n + \sum_{i = 1}^{n}\alpha(1 - \alpha)^{n - i} = 1\)</span>，因此<span class="math inline">\(Q_{n+1}\)</span>可以被称为<span class="math inline">\(R_i\)</span>的加权平均。且加权系数的值决定了时间上越近的奖励权重越高，过去的奖励在新的行动中的贡献会指数下降。故这种方法也被称为<i>exponential recency-weighted average</i>。</p>
<p><span class="math inline">\(\alpha\)</span>也可以随时间变化，记为<span class="math inline">\(\alpha_n(a)\)</span>。当<span class="math inline">\(\alpha_n(a) = 1 / n\)</span>时，表达式退化成简单平均，这意味着在样本数量足够大时可以收敛到真值。随机过程理论给出了这样的结论：当<span class="math inline">\(\{\alpha_n(a)\}\)</span>序列符合下面的要求时，基于这一些结算的<span class="math inline">\(Q_{n}\)</span>收敛到value的真值:</p>
<p><span class="math display">\[
\sum_{n=1}^{\infty} \alpha_{n}(a)=\infty \quad \text { and } \quad \sum_{n=1}^{\infty} \alpha_{n}^{2}(a)&lt;\infty
\]</span></p>
<p>第一个条件表明<span class="math inline">\(StepSize\)</span>足够大，以克服任何初始条件和随机波动的影响；第二个条件表明步长足够小以使得最后的和收敛。</p>
<p>注意到对于简单平均算法，即<span class="math inline">\(\alpha_n(a) = 1 / n\)</span>时，这两个条件都能得到满足，但是当<span class="math inline">\(\alpha_n(a)\)</span>为常数时，第二个条件显然无法满足，这意味着采用这种计算方法，最终的估计值无法收敛，而是会持续波动。由于问题本身是非平稳的，这种不收敛的特性反而是符合我们的要求的。</p>
<h3 id="乐观初始值">乐观初始值</h3>
<p>上面讨论的方法都依赖与如何确认最初的行为Value估计值，<span class="math inline">\(Q_1(a)\)</span>。用统计学的语言来说，这些方法由于其初始估计值的选择是有偏的(<i>biased</i>)。对于<i>sample-average</i>方法来说，如果所有的行为都被选择了至少一次，那么这种偏差是可以消除的。但是当我们使用了常量的<span class="math inline">\(\alpha\)</span>值时，初始值引入的偏差就会一直存在（虽然会不断减少）。在实践中，这种偏差一般不是问题，有时候也会显得非常有用。不过，缺点是这里的初始值选择会成为用户不得不设置了的初始超参数。不过好处是这也让我们能够为模型提前注入一些先验信息（奖励的期望水平）。</p>
<p>初始值的设置也可以被用来促进探索过程。假设不把这些初始值设置为0，而是设置成<span class="math inline">\(+5\)</span>。注意到的<span class="math inline">\(q_*(a)\)</span>是从<span class="math inline">\(N(0,1)\)</span>的正态分布中生成的，那么<span class="math inline">\(+5\)</span>的奖励就是一个非常高（或者说乐观，Optimistic）的值，这会促使模型去执行探索：在初始阶段，无论选择什么样的行为都会让模型失望，故促使模型多次尝试不同的行为。而随着不断迭代，初始注入的这个很高的奖励值会被「平均掉」，故使得真实的行为奖励能够被正确地选择出来。下图给出了性能对比：</p>
<p><img src="https://imgs.codewoody.com/uploads/big/4d2725e97cfc8038c1a27710ae082c0b.png"></p>
<p>在初始阶段，采用了乐观的初始值的方法性能比价差，因为这种方法进行了更多的探索。不过这些探索工作使得这种方法的性能可以后来居上。不过注意这种乐观初始值的方法并不是一种广泛有用的鼓励探索的方案，例如这种方法并不适合非平稳问题。</p>
<h3 id="置信区间上界行为选择">置信区间上界行为选择</h3>
<p>需要进行探索，因为操作值估计的准确性始终存在不确定性。贪婪的行为是目前看来最好的行为，但其他一些行为实际上可能会更好。<span class="math inline">\(\varepsilon\)</span>-<i>greedy</i>行为选择策迫使用户选择一些非贪婪的策略，不过这种策略会平等地尝试所有可能的行为选项，并没有对于次优选项和不确定性<span class="foot-note-span">【这里指value估计值的不确定性】</span>的偏好。更好的方法是一些更有潜力成为最优的行为中进行选择。这里的说的「潜力」取决于这些行为的value估计接近于最大值的程度，或者估计值中的不确定性的多少。一个有效的方法是根据下面的标准选择</p>
<p><span class="math display">\[
A_{t} \doteq \underset{a}{\arg \max }\left[Q_{t}(a)+c \sqrt{\frac{\ln t}{N_{t}(a)}}\right]
\]</span></p>
<p>其中<span class="math inline">\(\ln t\)</span>是自然对数，<span class="math inline">\(N_t(a)\)</span>代表 <span class="math inline">\(a\)</span> 在时刻<span class="math inline">\(t\)</span>之前已经被选择的次数，变量<span class="math inline">\(c\)</span>则控制了探索的偏好程度。如果<span class="math inline">\(N_t(a) = 0\)</span>，则<span class="math inline">\(a\)</span>被认为是最大化的行为选项。</p>
<p>置信区间上界(Upper confidence bound)行为选择的概念，上面的平方根项是对<span class="math inline">\(a\)</span>的value值的估计的不确定性（方差）度量。上面的式子中待最大化的项是一系列的<span class="math inline">\(a\)</span>的value的可能真值的上界，其中<span class="math inline">\(c\)</span>决定了置信度水平。每次<span class="math inline">\(a\)</span>被选择时，不确定性下降。另一方面，如果别的行为被选择，会导致<span class="math inline">\(a\)</span>的不确定性上升。在这个定义下，不同的选项的被选择的概率得到了均衡。在充分长的训练时间内，所有的行为都可以被选择。</p>
<h3 id="梯度赌博机算法">梯度赌博机算法</h3>
<p>到目前为止，在本章中，我们已经考虑了估计行为的value值并使用这些估计值选择行为的方法。 这通常是一种好方法，但并非唯一可行的方法。在本部分我们为每种行为<span class="math inline">\(a\)</span>学习一个量化的偏好值、<span class="math inline">\(H_t(a)\)</span>。这个偏好值越大，对应的行为被选择的几率就越大。偏好值没法从奖励的角度解释。偏好值的绝对大小没有意义，重要的是相对值。最终这些偏好值使用Softmax函数处理给出选择概率:</p>
<p><span class="math display">\[
\operatorname{Pr}\left\{A_{t}=a\right\} \doteq \frac{e^{H_{t}(a)}}{\sum_{b=1}^{k} e^{H_{t}(b)}} \doteq \pi_{t}(a)
\]</span></p>
<p>这里我们还引入了一个新的标记 <span class="math inline">\(\pi_t(a)\)</span>，为在时刻<span class="math inline">\(t\)</span>选择行动<span class="math inline">\(a\)</span>的概率。在初始状态下，所有的行为的偏好都是相同，故每个行为被选择的概率是均等的。</p>
<p>在Softmax形式定义的问题上我们可以使用随机梯度下降法进行学习。在每一步选择<span class="math inline">\(A_t\)</span>行为后，收到奖励<span class="math inline">\(R_t\)</span>，则行为偏好按照如下的方式进行更新：</p>
<p><span class="math display">\[
\begin{array}{cc}
H_{t+1}\left(A_{t}\right) \doteq H_{t}\left(A_{t}\right)+\alpha\left(R_{t}-\bar{R}_{t}\right)\left(1-\pi_{t}\left(A_{t}\right)\right), &amp; \text { and } \\
H_{t+1}(a) \doteq H_{t}(a)-\alpha\left(R_{t}-\bar{R}_{t}\right) \pi_{t}(a), &amp; \text { for all } a \neq A_{t}
\end{array}
\]</span></p>
<p>其中<span class="math inline">\(\alpha &gt; 0\)</span>为前面提到的<span class="math inline">\(StepSize\)</span>参数，<span class="math inline">\(\bar{R}_t \in \mathbb{R}\)</span>为所有到<span class="math inline">\(t\)</span>时刻位置的奖励的平均值（注意不包括<span class="math inline">\(R_t\)</span>)，这个平均值也可以用前面的章节提到的增量方法进行计算。上面的计算的基本原理是如果某个行为产生的奖励高于平均值，就增加这个行为的概率，反之降低这个概率。</p>
<h3 id="关联搜索contextual-bandits">关联搜索(Contextual Bandits)</h3>

        
      </div>
      
      
      
    </div>
    


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/knowledge-base/">KNOWLEDGE-BASE</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/knowledge-base/academic/">ACADEMIC</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/knowledge-base/academic/ml/">ML</a></li>
          
        
      
    
      
      
        
          <li>REINFORCEMENT-LEARNING</li>
        
      
    
      
      
    
  </ul>


    
    
    
  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://imgs.codewoody.com/uploads/big/0330eb8242d9bfc4873c11450f0ab691.jpg" alt="治部少辅">
            
              <p class="site-author-name" itemprop="name">治部少辅</p>
              <div class="site-description motion-element" itemprop="description">晚来天雨雪，能饮一杯无？</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">117</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">14</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">61</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/huangy10" title="GitHub &rarr; https://github.com/huangy10" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.zhihu.com/people/woody-huang" title="知乎 &rarr; https://www.zhihu.com/people/woody-huang" rel="noopener" target="_blank"><i class="fa fa-fw fa-zhihu"></i>知乎</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.jianshu.com/u/927273827560" title="简书 &rarr; https://www.jianshu.com/u/927273827560" rel="noopener" target="_blank"><i class="fa fa-fw fa-jianshu"></i>简书</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#reinforcement-learning"><span class="nav-number">1.</span> <span class="nav-text">Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#增强学习和监督学习的区别"><span class="nav-number">1.1.</span> <span class="nav-text">增强学习和监督学习的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增强学习与非监督学习的区别"><span class="nav-number">1.2.</span> <span class="nav-text">增强学习与非监督学习的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增强学习面临的挑战"><span class="nav-number">1.3.</span> <span class="nav-text">增强学习面临的挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增强学习元素"><span class="nav-number">1.4.</span> <span class="nav-text">增强学习元素</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#a-policy"><span class="nav-number">1.4.1.</span> <span class="nav-text">A Policy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-reward-signal"><span class="nav-number">1.4.2.</span> <span class="nav-text">A Reward Signal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-value-function"><span class="nav-number">1.4.3.</span> <span class="nav-text">A Value Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-model-of-the-environment"><span class="nav-number">1.4.4.</span> <span class="nav-text">A Model of the Environment</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tabular-soluation-methods"><span class="nav-number">2.</span> <span class="nav-text">Tabular Soluation Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多臂老虎机问题multi-armed-bandits"><span class="nav-number">2.1.</span> <span class="nav-text">多臂老虎机问题(Multi-armed Bandits)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#行动-价值方法"><span class="nav-number">2.2.</span> <span class="nav-text">行动-价值方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#臂测试问题"><span class="nav-number">2.3.</span> <span class="nav-text">10臂测试问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增量计算实现"><span class="nav-number">2.4.</span> <span class="nav-text">增量计算实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#追踪非平稳问题"><span class="nav-number">2.5.</span> <span class="nav-text">追踪非平稳问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#乐观初始值"><span class="nav-number">2.6.</span> <span class="nav-text">乐观初始值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#置信区间上界行为选择"><span class="nav-number">2.7.</span> <span class="nav-text">置信区间上界行为选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度赌博机算法"><span class="nav-number">2.8.</span> <span class="nav-text">梯度赌博机算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关联搜索contextual-bandits"><span class="nav-number">2.9.</span> <span class="nav-text">关联搜索(Contextual Bandits)</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">治部少辅</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.1.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/affix.js?v=7.1.0"></script>

  <script src="/js/schemes/pisces.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  
  
  <script id="dsq-count-scr" src="https://codewoody.disqus.com/count.js" async></script>


<script>
  var disqus_config = function() {
    this.page.url = "http://www.codewoody.com/knowledge-base/academic/ml/reinforcement-learning/index.html";
    this.page.identifier = "knowledge-base/academic/ml/reinforcement-learning/index.html";
    this.page.title = '增强学习';
    };
  function loadComments() {
    var d = document, s = d.createElement('script');
    s.src = 'https://codewoody.disqus.com/embed.js';
    s.setAttribute('data-timestamp', '' + +new Date());
    (d.head || d.body).appendChild(s);
  }
  
    loadComments();
  
</script>





  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      },
      
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  
<script>
  $('.highlight').each(function(i, e) {
    var $wrap = $('<div>').addClass('highlight-wrap');
    $(e).after($wrap);
    $wrap.append($('<button>').addClass('copy-btn').append('Copy').on('click', function(e) {
      var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
        return $(e).text();
      }).toArray().join('\n');
      var ta = document.createElement('textarea');
      var yPosition = window.pageYOffset || document.documentElement.scrollTop;
      ta.style.top = yPosition + 'px'; // Prevent page scroll
      ta.style.position = 'absolute';
      ta.style.opacity = '0';
      ta.readOnly = true;
      ta.value = code;
      document.body.appendChild(ta);
      ta.select();
      ta.setSelectionRange(0, code.length);
      ta.readOnly = false;
      var result = document.execCommand('copy');
      
      ta.blur(); // For iOS
      $(this).blur();
    })).on('mouseleave', function(e) {
      var $b = $(this).find('.copy-btn');
      setTimeout(function() {
        $b.text('Copy');
      }, 300);
    }).append(e);
  })
</script>


  

  


  <script src="https://ethantw.github.io/Han/latest/han.min.js"></script>
  
  <script>
    void function(){
      // window.hinst = Han(document.querySelector('.post-body')).setRoutine([
      //   'initCond',
      //   'renderElem',
      //   'renderJiya',
      //   'renderHanging',
      //   'renderHWS',
      //   'correctBasicBD',
      //   'substCombLigaWithPUA'
      // ]).render()
      document.getElementById("refs")
          .setAttribute("lang", "en_US");
    }()

    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
