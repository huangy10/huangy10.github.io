<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ns3 在模块中使用第三方库]]></title>
    <url>%2Fposts%2F53831%2F</url>
    <content type="text"><![CDATA[ns3使用了waf编译系统，因此在ns3中尝试引入第三方模块时，就没有make那么直接了。 其实思路的核心还是想办法最终为编译器提供-L和-I的设置。这个过程我们通过wscript中的configure函数来实现。这里我们假设模块使用的库的位置放在模块源码目录下的libs子目录。库的名字为example-lib。目录结构如下: 1234libs└── example-lib ├── include └── libexample-lib.a 其中，include文件夹内为头文件，libexample-lib.a为静态库文件。 修改wscript文件中的configure函数，如下 12345678def configure(conf): root_dir = conf.path.abspath() example_lib_dir = os.path.join(root_dir, "libs/example-lib") conf.env.append_value("LINKFLAGS", ["-L%s/" % example_lib_dir]) conf.env.append_value("LIB", ["example-lib"]) conf.env.append_value("CPPFLAGS", ["-I%s/include" % example_lib_dir, ]) 修改configure函数之后要重新运行./waf configure命令来让设置生效。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ns3 wscript: 自动寻找需编译的源文件]]></title>
    <url>%2Fposts%2F3426%2F</url>
    <content type="text"><![CDATA[在ns3的编译体系中，每个module会包含一个名为wscript的python脚本来提供编译信息。例如，一个通过create-module.py创建的module中的wscript为 12345678910111213141516171819202122232425262728293031# -*- Mode: python; py-indent-offset: 4; indent-tabs-mode: nil; coding: utf-8; -*-# def options(opt):# pass# def configure(conf):# conf.check_nonfatal(header_name='stdint.h', define_name='HAVE_STDINT_H')def build(bld): module = bld.create_ns3_module('example-module', ['core']) module.source = [ 'model/example-module.cc', 'helper/example-module-helper.cc', ] module_test = bld.create_ns3_module_test_library('example-module') module_test.source = [ 'test/example-module-test-suite.cc', ] headers = bld(features='ns3header') headers.module = 'example-module' headers.source = [ 'model/example-module.h', 'helper/example-module-helper.h', ] if bld.env.ENABLE_EXAMPLES: bld.recurse('examples') # bld.ns3_python_bindings() 其中，module.source中包含需要编译的.cc源文件，而headers.source中包含对应的头文件。每次新建C++源代码文件时，都需要手动添加到这里的列表中。下面我给出一个自动从module的model, helper目录下搜索源文件的方法： 123456789101112131415161718192021222324252627282930313233343536373839404142# -*- Mode: python; py-indent-offset: 4; indent-tabs-mode: nil; coding: utf-8; -*-import os# def options(opt):# pass# def configure(conf):# conf.check_nonfatal(header_name='stdint.h', define_name='HAVE_STDINT_H')def _list_sources(bld, suffix): root_dir = bld.path.abspath() res = [ x for x in [os.path.join("model", y) for y in os.listdir(os.path.join(root_dir, "model"))] if x.endswith(suffix) ] res += [ x for x in [os.path.join("helper", y) for y in os.listdir(os.path.join(root_dir, "helper"))] if x.endswith(suffix) ] return resdef build(bld): module = bld.create_ns3_module('example-module', ["core"]) module.source = _list_sources(bld, ".cc") module_test = bld.create_ns3_module_test_library('example-module') module_test.source = [ 'test/mix-autonomy-test-suite.cc', ] headers = bld(features='ns3header') headers.module = 'example-module' headers.source = _list_sources(bld, ".h") if bld.env.ENABLE_EXAMPLES: bld.recurse('examples') # bld.ns3_python_bindings() 注意不要直接套用上面的范例文件，需要将&quot;example-module&quot;的名字改为你的module的名字]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝塞尔曲线原理]]></title>
    <url>%2Fposts%2F50553%2F</url>
    <content type="text"><![CDATA[这是一篇转载文章。Bézier curve(贝塞尔曲线)是应用于二维图形应用程序的数学曲线。 曲线定义：起始点、终止点（也称锚点）、控制点。通过调整控制点，贝塞尔曲线的形状会发生变化。 1962年，法国数学家Pierre Bézier第一个研究了这种矢量绘制曲线的方法，并给出了详细的计算公式，因此按照这样的公式绘制出来的曲线就用他的姓氏来命名，称为贝塞尔曲线。 抛物线三切线定理 设\(P_0\)，\(P_0^2\)，\(P_2\)是一跳抛物线上顺序不同的三个点。过\(P_0\)和\(P2\)的切线交于\(P_1\)。过\(P_0^2\)的切线交\(P_0 P_1\)和\(P_2 P_1\)相交于\(P_0^1\)和\(P_1^1\)，则有如下比例成立： \[\begin{equation} \frac{P_{0} P_{0}^{1}}{P_{0}^{1} P_{1}}=\frac{P_{1} P_{1}^{1}}{P_{1}^{1} P_{2}}=\frac{P_{0}^{1} P_{0}^{2}}{P_{0}^{2} P_{1}^{1}} \end{equation}\] 抛物线三切线定理示意图 此即为抛物线的三切线定理。 二次贝塞尔曲线 当\(P_0\)，\(P_2\)固定时，引入参数\(t\)，令上述比例值为\(t:(1-t)\)，即有： \[\begin{equation} \begin{array}{l}{P_{0}^{1}=(1-t) P_{0}+t P_{1}} \\ {P_{1}^{1}=(1-t) P_{1}+t P_{2}} \\ {P_{0}^{2}=(1-t) P_{0}^{1}+t P_{1}^{1}}\end{array} \end{equation}\] 将第一，二个式子代入第三个有： \[\begin{equation} P_{0}^{2}=(1-t)^{2} P_{0}+2 t(1-t) P_{1}+t^{2} P_{2} \end{equation}\] 当\(t\)从0变到1时，\(P_0^2\)点经过的轨迹即为上图中的抛物线，也即为由三顶点\(P_0\), \(P_1\), \(P_2\)决定的一条二次贝塞尔曲线。也可以认为这条二次贝塞尔曲线是由两个前顶点\((P_0, P_1)\)以及两个后顶点\((P_1, P_2)\)决定的。 更高阶的贝塞尔曲线 类似于二次贝塞尔曲线的推导过程，我们可以推广到更高阶的贝塞尔曲线。 由四个控制点定义的三次Bezier曲线\(P_0^3\)可被定义为分别由\((P_0,P_1,P_2)\)和\((P_1,P_2,P_3)\)确定的二条二次Bezier曲线的线性组合，由\((n+1)\)个控制点\(P_i(i=0,1,...,n)\)定义的n次Bezier曲线\(P_0^n\)可被定义为分别由前、后\(n\)个控制点定义的两条\((n-1)\)次Bezier曲线\(P_0^{n-1}\)与\(P+0^{n-1}\)的线性组合： \[\begin{equation} P_{0}^{n}=(1-t) P_{0}^{n-1}+t P_{1}^{n-1} \quad t \in[0,1] \end{equation}\] 由此可以得到Bezier曲线的踢腿计算公式 \[\begin{equation} P_{i}^{k}=\left\{\begin{array}{c}{P_{i}} &amp; {k=0} \\ {(1-t) P_{i}^{k-1}+t P_{i+1}^{k-1}} &amp; {k=1,2, \cdots, n, i=0,1, \cdots, n-k}\end{array}\right. \end{equation}\] 这就是de Castelijau算法。 贝塞尔曲线原理动图 一阶贝塞尔曲线 二阶贝塞尔曲线 三阶贝塞尔曲线 四阶贝塞尔曲线 五阶贝塞尔曲线]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>转载</tag>
        <tag>processing</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Volume的权限问题]]></title>
    <url>%2Fposts%2F25188%2F</url>
    <content type="text"><![CDATA[这里我们要解决的是使用Docker过程中常见的Volume权限问题。具体而言，当我们用-v将宿主机的路径绑定到Docker镜像的内部路径时，有时候会导致Docker镜像缺少对这个目录的访问权限，从而导致进程出错。 Why 当我们绑定宿主目录到镜像时，如果该目录不存在，Docker也会自动创建该目录。这种方式创建出来的目录的拥有者是root用户。如果该目录已经存在，那么其拥有者就取决于宿主配置的情况了。 由于Docker内部的用户空间和宿主的用户空间是独立的，如果镜像内运行进程的用户和宿主目录的拥有者不符合，就会出现权限问题。 How to solve it 由于镜像内和宿主的用户名空间是不同的，所以通过用户名的方式来变更宿主目录的所有权会失效。然而，事实上用户系统是通过uid来标识不同的用户的，我们只需要将宿主的路径的拥护者改为镜像内用户相通的uid即可。镜像内用户的uid可以通过如下方式查看，例如： 12jovyan@8fed6b266a3c:~$ iduid=1000(jovyan) gid=100(users) groups=100(users) 继而再修改宿主机上对应目录的拥有者： 1sudo chown -R 1000 /path/to/volume Further Research 上面的方法可以解决Volume访问权限的问题，不过会产生潜在的漏洞。从镜像内获得的uid在宿主上可能表示的是不同的用户，在宿主机上修改目录的拥有者会导致数据被同一服务器上的其他用户访问，带来安全性上的问题。 另一方面，如果有多个镜像需要共享一个Volume，而他们内部的运行用户的uid不同的话，就需要在宿主上进行更加复杂的用户以及组的配置。 更优雅的执行方法有下面两种： Use Named Volume Named Volumes 由容器自行配置权限问题 Reference 谈谈 Docker Volume 之权限管理（一） What is the (best) way to manage permissions for Docker shared volumes? Why Docker Data Containers (Volumes!) are Good Use volumes Different Types of Volumes]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>debug</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书破万卷,下笔如有神]]></title>
    <url>%2Fposts%2F60435%2F</url>
    <content type="text"><![CDATA[能够获得暴利的职业，都有一个共同特点：可扩展性（scaling），一次劳动可以服务成千上万的人。 软件、电影、游戏行业都具有可扩展性，作品的生产成本是固定的，但可以被消费无数次，所以有巨大的获利空间，创造出许许多多的富豪。另一方面，理发师、厨师、出租车司机一次劳动，只能服务少数几个人，就不具有可扩展性，很难获得暴利，生存得很辛苦。 最近，我读到美国一个风险投资家的文章。他说了一句发人深思的话： &quot;写作是最具可扩展性的活动。你呆在家里，不去参加活动/会议，只是在网上写下自己的想法，然后你就具有了最好的可扩展性。&quot; 我想了一下，还真是这样。你写了一篇文章，想让其他人看到，只要到处张贴就行了。每次转贴，就是扩展了一次。这比其他产品的扩展容易多了。面包师傅想要更多的人尝到自己的面包，只能多开面包店；网站要扩展，只能购买更多的服务器。相比之下，文字的扩展简直是零成本。 大公司每年花费数十亿美元用于广告，以求人们关注他们的产品。但是，一个好的作家可以免费获得这种扩展性。这就是为什么你应该把自己的想法写下来的原因，这么好的免费传播渠道，为什么不用呢？你以为，写下来不会有人看。错，其实是有人会看到的，如果他们觉得有价值，就会帮你传播出去。 这篇文章转载自阮一峰的博客。这篇文章其实说了一个非常简洁明了却价值巨大的道理，也给我们启示：我们应该如何规划自己的职业道路。只是靠出售自己的时间，即便是清北的同学，也只是能做到一个尚算富裕，但是辛苦中产阶级。要更上一层楼，还是需要手握资本。而怎么获得资本呢？其实就是靠文章里说的“可扩展性的工作”。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandoc渲染引擎导致Hexo Tag渲染失败的临时解决办法]]></title>
    <url>%2Fposts%2F62502%2F</url>
    <content type="text"><![CDATA[在Hexo+Next: 使用Latex公式这篇文章中我发现在使用Pandoc作为Hexo的渲染引擎时，Hexo的标签功能会有问题，具体表现为Hexo的标签内部的内容会输出markdown源码，而非渲染后的html。 问题研究 经过我的研究，这是因为hexo-render-pandoc在注册自己的renderer时，只注册了异步渲染的renderer，而没有注册同步渲染的renderer，而Hexo的标签中主要是用同步renderer。以当时我使用的NexT的note标签为例。其实现代码为： 12345678910'use strict';function postNote(args, content) &#123; return `&lt;div class="note $&#123;args.join(' ')&#125;"&gt; $&#123;hexo.render.renderSync(&#123;text: content, engine: 'markdown'&#125;).split('\n').join('')&#125; &lt;/div&gt;`;&#125;hexo.extend.tag.register('note', postNote, &#123;ends: true, async: true&#125;);hexo.extend.tag.register('subnote', postNote, &#123;ends: true, async: true&#125;); 由于没有注册同步渲染器，这里的hexo.render.renderSync渲染会失败，从而返回的是content中的原本内容，也即Markdown形式的源码。 解决办法 彻底的解决办法，自然是在hexo-render-pandoc中同时注册同步渲染器。不过我自己尝试之后发现作为同步渲染器，pandoc和Hexo使用模板引擎貌似有冲突。更细致深入的修改最好还是由原作者来进行（我已经提交了Issue）。 这里我给出一个临时的解决办法：既然hexo-render-pandoc只注册了异步渲染代码，那么我们在Tag的实现代码中调用异步渲染的接口就可以了。仍然以NexT主题的note标签为例，可以将代码修改成： 12345678910111213141516'use strict';function postNote(args, content) &#123; return hexo.render.render(&#123;text: content, engine: 'markdown'&#125;) .then(function (res) &#123; return `&lt;div class="note $&#123;args.join(' ')&#125;"&gt; $&#123;res.split('\n').join('')&#125; &lt;/div&gt;` &#125;) // return `&lt;div class="note $&#123;args.join(' ')&#125;"&gt; // $&#123;hexo.render.renderSync(&#123;text: content, engine: 'markdown'&#125;).split('\n').join('')&#125; // &lt;/div&gt;`;&#125;hexo.extend.tag.register('note', postNote, &#123;ends: true, async: true&#125;);hexo.extend.tag.register('subnote', postNote, &#123;ends: true, async: true&#125;); 经过这样修改就可以了。不过这种方法仍然只是权宜之计，要是去修改每个Tag的实现，就太繁琐了。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Next: 使用Latex公式]]></title>
    <url>%2Fposts%2F20215%2F</url>
    <content type="text"><![CDATA[这次更换主题的很大一个动因就是因为在NexT这个主题上，开启Latex的支持很方便。网上关于这方面的文章其实不少，但是大部分都不全面，照本宣科下来，很可能不能用。这些教程一般就给了_config.yml文件的配置以及pandoc依赖安装，但是一些关键细节缺失了。这篇文章里我梳理了一下整个流程。 0. Reference 英语好的话，其实可以尝试直接阅读官方文档。 1. Install Dependencies Next支持mathjax和katex两种渲染方式，其中katex的速度更快，但是对于Latex的支持有一定的限制。所以除非你的博客数量实在是过于庞大，不然就可以直接使用mathjax。 mathjax可以选用下面两种渲染引擎的中的任一一种 hexo-renderer-kramed hexo-render-pandoc 使用hexo-render-pandoc还需要安装pandoc渲染引擎。其安装方法可以参考 pandoc官网。如果在macOS上可以使用 Homebrew安装. 这里以pandoc为例： 123# 需要先卸载默认的渲染引擎npm un hexo-renderer-marked --savenpm i hexo-renderer-pandoc --save 替换渲染器之后会导致NexT note功能出现问题，note内的元素内容无法渲染，会输出markdown源代码。 这个问题我在hexo-render-pandoc上提了一个Issue，看原作者什么时候能够更新解决吧。 2. Configuration 配置NexT主题的_config.yml文件 12345math: enable: true ... engine: mathjax #engine: katex 很多文章都漏掉了在配置中一个重要的信息：在主题配置math下有一个名为per_page的选项，其值为true或者false。这个选项用来控制是否对每个篇文章都渲染数学公式。默认情况下是true，这意味只对Front Matter中含有mathjax: true的文章进行公式渲染。将per_page设置为false，则会对每一篇文章都尝试进行公式渲染。 由于公式渲染时一个很费时的操作，因此还是保持默认配置，通过Front Matter进行渲染控制. 3. How to use 3.1 行内嵌套公式 如：质能方程\(e=mc^2\) 1如：质能方程$e=mc^2$ 3.2 独占一行的公式 如： \[ 1=\sum_{i=0}^{m}\sum_{k=0}^{W_i-1}b_{i,k}=\sum_{i=0}^{m}b_{i,0}\sum_{k=0}^{W_i-1}\frac{W_i-k}{W_i}=\sum_{i=0}^{m}b_{i,0}\frac{W_i+1}{2}\\ =\frac{b_{0,0}}{2}\left[W\left(\sum_{i=0}^{m-1}(2p)^i+\frac{(2p)^m}{1-p}\right) + \frac{1}{1-p}\right] \] 12345如：$$1=\sum_&#123;i=0&#125;^&#123;m&#125;\sum_&#123;k=0&#125;^&#123;W_i-1&#125;b_&#123;i,k&#125;=\sum_&#123;i=0&#125;^&#123;m&#125;b_&#123;i,0&#125;\sum_&#123;k=0&#125;^&#123;W_i-1&#125;\frac&#123;W_i-k&#125;&#123;W_i&#125;=\sum_&#123;i=0&#125;^&#123;m&#125;b_&#123;i,0&#125;\frac&#123;W_i+1&#125;&#123;2&#125;\\=\frac&#123;b_&#123;0,0&#125;&#125;&#123;2&#125;\left[W\left(\sum_&#123;i=0&#125;^&#123;m-1&#125;(2p)^i+\frac&#123;(2p)^m&#125;&#123;1-p&#125;\right) + \frac&#123;1&#125;&#123;1-p&#125;\right]$$ 更多latex的使用方法，请参考官方文档]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[清华GPA事件备份:2019.04.16]]></title>
    <url>%2Fposts%2F34235%2F</url>
    <content type="text"><![CDATA[大图预警]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Haproxy支持Ipv6]]></title>
    <url>%2Fposts%2F12489%2F</url>
    <content type="text"><![CDATA[Haproxy Haproxy is a reliable, high performance TCP/HTTP Load Balancer 这是官网对于Haproxy的介绍，其作用的类似于Nginx，是一个均衡负载的服务器。其相比于Nginx的好处是其代理TCP流量的功能配置起来非常的简单。我这里主要拿Haproxy来配置Shadowsocks的跳板机。 前一段时间，GFW的墙好像又加高了，很多时候在教育网外连接服务器不是很可靠。所以我考虑干脆在教育网环境下做一个跳板服务器，这样在外面可以先跳到教育网，然后再从教育网过墙。 教育网的另一个好处是有IPv6。貌似IPv6上面的拦截比较弱，而且，绝大多数的高校对于IPv6都是免流量费的。因此，我们可以从IPv4公口进，然后走IPv6出。 How to 不过，问题是通过apt安装的haproxy是不支持IPv6的！ 我们只能自己动手从源码编译了： 12345wget http://www.haproxy.org/download/1.7/src/haproxy-1.7.2.tar.gztar -xzf haproxy-1.7.2.tar.gzcd haproxy-1.7.2make TARGET=linux2626 USE_GETADDRINFO=1sudo make install]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[政史:珍珠港事件前日方决策过程梳理]]></title>
    <url>%2Fposts%2F10462%2F</url>
    <content type="text"><![CDATA[在知乎上看到的比较好的回答，作为备份放在这里： 原文链接：https://www.zhihu.com/question/306368870/answer/639051842 archive.is上的备份网页：http://archive.is/4dOL3 珍珠港俯视图（1941年10月30日） 以下是原文内容： 因为再不突袭，日本人这大东亚战争就算是白打了。 咱们这里需要补充一个小知识，在20世纪30年代末，日本的能源结构是这样的：80%的石油来自美国，10%的石油来自东印度群岛，只有7%左右的石油可以自给。那么问题来了，日本从918开始折腾到40年，找到能替代美国的石油生产地了么？ 没有。不仅石油高度依赖美国，铁和各种军需物资都严重依赖美国，甚至可以这么说，要是没有美国人提供的这些物资，日本这侵华战争就根本打不起来。 1937年美国对日出口总值为2.89亿美元,其中石油、精炼油、废钢铁、原棉这四项战略物资就达1,42亿美元,约占二分之一。以石油一项而论,日本所需石油来自美国的份额,1937年占80%,1939年占85%。——齐世荣.绥靖政策研究.,北京：首都师范大学出版社,1998：413 据统计，1937－1938年，日本从美国进口的军需品占其军需品总进口额的55%。1937年，美对日废钢铁的出口量是1931年的40倍之多。——杨玉圣.中国人的美国观.上海: 复旦大学出版社，1997：152 （1937、1938年日本）从美国输入铁合金为77.53%和82.71%,铜的比例高达95.18%和90.89%,煤油及其产品为6.271%和65.57%,汽车及零件为9.241%和64%,飞机及零件为70.19%和7.692%,金属工作母机为69.53%和67.09%。——沈庆林.中国抗战时期的国际援助.上海人民出版社,2000：53 这就是二战中最讽刺的地方，美国人是日本人在二战前期最主要石油来源国，而且没有之一。日本人觉得不能任由老美这么掐着自己的脖子，于是1934年出台了《石油工业法》表示要加强本国石油产业建设，效果……一般吧；然后37年又搞了个《合成油法案》，表示要开动大日本帝国先进之科技手段以煤改油，效果……更一般。 考虑到1937年日本的财政收入才47亿日元，而以当时的汇率来看大概3日元可以折合1美元，大家可以感受一下每年日美之间几亿美元的进口额意味着什么。而更倒霉的是虽然侵华战争开始以后日军进展极其顺利，然而中国是个贫穷的农业国，重工业基础极其薄弱，而工业建设又是个耗时巨大、烧钱极多的活儿……所以日本人十分郁闷地发现自己这仗是越打越大，然而钱却都进了美国人的兜。 这就是为什么37年日本人在扬子江上炸了美国船，罗斯福把这事按下去的原因之一——顺便一提，日本人为了平事掏出来了2214007.36刀，1937年的两百万美元啊……日本人在占领区甚至搞了一次“抵制美货”的闹剧，然而美国人对此并不在意：有本事你抵制我的石油啊？ 美国国内群众此时对日本人的反感情绪已经是十分强烈了，甚至有几百名学生代表参与了焚烧日本丝绸的行动。等到了1938年美国政府觉得实在不能再这么容忍日本人了！必须上点手段了！于是政府向上百家工厂写信：建议不要跟日本人做生意。 这就是赫赫有名的“道义禁运” 6月11日,赫尔在记者招待上公开谴贵对和平居民的空袭轰炸,随后他写给美国148家注册出口飞机和飞机部件的厂家,表示:政府强烈反对把飞机和航空设备出售给世界上任何对和平居民进行轰炸的国家——赫尔回忆录·第一卷：569 道义禁运的效果极其明显，立竿见影，日本人37年从美国进口的飞机及部件达到248.4万美元，38年为1745.4万美元，增长了7倍。 这么拖拖拖一直拖到39年重庆轰炸，美国人民一看艾玛这太惨了，咱们真的不能再卖给日本人石油了，罗斯福表示那不成啊，你不卖给他石油了他狗急跳墙去打英荷东印度群岛怎么办？为了避免战争扩大咱们还是继续做生意吧…… 对日本人来讲，事情非常尴尬——你要是想继续从美国人手里拿到物资，就必须按照美国人的意思，控制战争规模；而你想控制战争规模又控制不下来，TG在敌后遍地开花，老蒋死活就不投降，你占领的地方地大物博可就是没有好用的石油，重工业基础又弱到不行。所以日本人思前想后，最后还是向东南亚伸出了魔爪。 1939年2月，日本占领海南岛；3月，日本人又搞定了距马尼拉700英里的南沙群岛；6月，派兵封锁天津英租界，7月强迫英国人跟自己一起建设“东亚新秩序”，正在欧洲被希特勒搞得焦头烂额的英国人几乎没怎么犹豫，就在7月24号跟日本人签订了“有田——克莱琪协定”，承认了日本在中国有“特殊需要”。 这下美国人终于坐不住了，7月26号美国政府正式通知日本没，咱们那个美日商约即将在六个月后废止——半年时间，你自己想想清楚，到底还要不要铁和石油了。日本人终于发现这自己扛不住啊！赶紧还是跟美国人谈谈吧，于是9月25日，海军稳健派、熟悉米英鬼畜内部动向之大将野村吉三郎任专职外相，开始跟美国进行谈判，美国人说这事好办，你们开放长江下游、尊重我们在华权利，有钱大伙一起赚嘛！只要你们肯把中国的利益让出来一点，咱们这个商约还是可以再签的。 未果。 这期间的态势十分有趣，日本人在诺门坎吃了大败仗，彻底打消了北进的念头；敌后大规模扫荡、扫荡、再扫荡，八路就是扫不干净；正面战场进入相持状态，长沙会战、随枣会战都没能达成预定的战略目标，日本国内的经济开始遭不住了。 美国人此时反而比较克制，由于罗斯福担心“再进一步就会激怒日本”，所以1940年1月日美商约失效之后两国的贸易竟然还在诡异的继续着，然而谁也不知道这样的日子会持续多久。 对日本来说，他们必须做出选择了。 1940年3月，日本拟定了军需物资自给自足计划，将更多的精力投入到了东南亚 日本政府深切关怀足以改变荷属东印度群岛现状的任何事态——1940年4月15日,外相有田八郎讲话,太平洋战争史·第二卷：21 日本人在东南亚的脚步越来越快，而美国人则在抓紧时间，卖出自己的最后一份石油。所以一方面是日趋紧张的局势，而另一方面则是不断攀升的石油贸易，美孚石油在7月18日向国务院报告，说日本人提出要买下他们的全部产量！美国政府内部已经吵到不可开交，罗斯福接到的报告说假如我们再不限制日本人购买航空汽油，我们自己军队就可能出现6到9个月的汽油供应不足！ 在巨大的压力面前罗斯福终于决定对日本进行禁运，经过漫长的扯皮与大撕逼之后，政府官员们最终达成了一致，在7月26日宣布对航空发动机燃料及润滑油和第一号高熔度的废钢铁实行出口管制。先总统 蒋公激动地浑身颤抖，跟美国大使表示艾玛你们太够意思了！ 总统和国务卿的伟大而辉煌的举动，减轻了中国自卷入冲突以来面临的极严峻的危机。——先总统 蒋公 在这个后来被无数人称颂的禁运限制里，国务院表示辛烷值87以上的航空汽油都必须禁运！ &gt; 日本人：解释解释，什么叫“辛烷值87以上的航空汽油都必须禁运”？ &gt; 国务院：难道你不懂什么叫禁运？ &gt; 日本人：我要你解释解释，什么叫他妈的“辛烷值87以上的航空汽油都必须禁运”？ &gt; 石油公司：87号以上禁运的意思，就是他妈的87号以下不！禁！运！，还有，往86号航空汽油里加铅可以他妈的提高辛烷值！你懂了没有！？ &gt; 日本人：哦大哥，原来这就是他妈的禁运啊！小弟明白了！ 于是1940年7月到12月，日本从美国进口的86号航空汽油同比增加了550%。 此时日本的经济已经开始在崩溃的边缘上晃悠了，国家总动员法的条款几乎已经全都实施了，结果40年日本西部和朝鲜还遭遇了旱灾，粮食收成不好，好多人连吃大米都成了问题。关键是此时日本的外汇储备也接近枯竭，再这么拖下去用不了多久你想买都买不成了！最后高层达成一致，再不对东南亚下手咱们就得先完蛋了。于是1941年7月2日，御前会议最终制定了《适应形势变化的帝国国策纲要》，表示就算是跟米英鬼畜开战，咱们也得南进！7月24日，日本出兵印度支那南部。 然后罗斯福炸了：老子不禁运你石油就是为了不让你打那边，你自己心里没点数么？于是26号冻结了日本在美的全部资产；英国人表示弟儿你说的对，我们也禁运，然后切断了日本在婆罗洲的石油供给。27号荷兰人跟进，冻结日本资产。这下子事情再也没有回转的余地了。 日本人的精神一下子紧张了起来（……为什么才紧张！？），军方此前一直认为我把印度支那南部这么一占，你们这些米英鬼畜还不得乖乖坐下来跟我和谈么？咱们这和平近在眼前啊！ ……以此确保东亚的战略要地。由此或可使英美荷死心不再压迫日本,并给重庆政府以打击,以找到解决日中战争的突破口,进而或许有助于打开日荷谈判。所以只有尽快抓住时机实行“战略上先发制人之措施”,才能避免同英美作战,此即不战而胜之上策——信天清三郎,日本外交史·下册：668 1941年11月，两艘日本油轮自洛杉矶附近海域空载而归。大怒的日本人……切断了英美使馆的取暖油供应。（……我是一直没搞懂日本人的脑回路）而在此之前，8月份美国人已经提出了自己的条件：日本从中国撤军、各国在中国机会均等以及日本改变三国同盟，这个条件被日本人毫不犹豫地拒绝了。9月6日，日本御前会议批准了《帝国国策施行要点》，指出10月上旬外交依然没有进展，则准备开战。 日本人最开始的计划是咱们先赶紧在东南亚占地盘，然后建立个防御圈——考虑到东南亚还有个美属菲律宾，那美国人妥妥是要来跟咱们打的，到时候咱们舰队决战，拼个你死我活！ 然后联合舰队的指挥官山本五十六对此表示了不同意见——美国人啥工业能力？你什么工业能力？心里没点数么？既然已经料到要打，那为什么不趁着美国人还没有完全动员的时候直接先下手为强？要知道，海军要想重建，那难度可比陆军难多了。咱们一鼓作气消灭美国的太平洋海上力量，然后趁着美国人重建海军无暇的关口逼他就范，承认咱们大日本帝国在亚洲的霸权那是十分合理的！ 11月20日，日本向美国提出最后一个谈判方案，日本人表示这绝对是自己最后的底线了 1.日本政府和美国政府都保证,除了目前己驻有日军的法属印度支那以外,不向东南亚和南太平洋地区的任何地方进行任何武装进军。 2.一俟日本和中国之间恢复和平,或在太平洋地区建立了公正的和平,日本政府保证撤走目前驻扎在法属印度支那的军队。同时,日本政府宣布在本协议(以后将包含在最后协议中)订立时,准备把现驻法属印支南部的军队移驻该地区北部. 3.日美两国政府将进行合作,以保证两国在荷属东印度群岛取得所需要的货物和商品。 4.日本政府和美国相互保证把通商关系恢复到日方资金被冻结前的状态.美国政府将按日本所需的数量供应石油。 5.美国政府保证不采取任何措施和行动,不利于日本和中国之间为谋求全面和平所作的努力。——United States Department of State.Papers relating to the foreign relations of the United States, Japan, 1931–1941, Volume II 美国人对此表示难以置信，并回复了一份由国务卿起草的备忘录，基本上重申了自己在8月份提出的要求。美国人所不知道的，是在自己做出这个回复以前，一支规模空前的舰队已经在单冠湾集结完毕了，那上面的飞行员此前曾反复地练习过如何低空投放鱼雷和炸弹。罗斯福此时还在犹豫要不要向日本示好，以挽回两国之间的关系，11月22日美国国务院远东司甚至接到命令，起草一份新的草案，有限度地恢复对日本的石油、食品及药物供应。然而由于中国及英国的强烈反对，这份草案最终也没能实施。 1941年12月1日，日本御前会议做出了决定：与美国开战。 1941年12月7日，珍珠港事件爆发，美国太平洋舰队遭到重创。 1942年，日本军队逼近东印度群岛的巴厘巴板炼油厂，1943年第一季度，日本的石油危机大大缓和。 石油问题已经基本得到解决——东条英机·1943 以上。 后记：这篇文章的作者设置了禁止转载，不过我这博客也没什么人看，我放在这里也是为了备份，也许将来某天知乎挂了或者作者决定退出知乎了删除了这篇问答？另外，作者的这篇文章里面还是有一些戏谑口吻的地方，我打算围绕着作者写的主干，做一做考据，让这篇文章能够成为之后“键政”的有力资料。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
        <tag>政治</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[996 License 1.0]]></title>
    <url>%2Fposts%2F12613%2F</url>
    <content type="text"><![CDATA[Copyright (c) 996 License Version 1.0 (Draft) Permission is hereby granted to any individual or legal entity obtaining a copy of this licensed work (including the source code, documentation and/or related items, hereinafter collectively referred to as the &quot;licensed work&quot;), free of charge, to deal with the licensed work for any purpose, including without limitation, the rights to use, reproduce, modify, prepare derivative works of, publish, distribute and sublicense the licensed work, subject to the following conditions: The individual or the legal entity must conspicuously display, without modification, this License on each redistributed or derivative copy of the Licensed Work. The individual or the legal entity must strictly comply with all applicable laws, regulations, rules and standards of the jurisdiction relating to labor and employment where the individual is physically located or where the individual was born or naturalized; or where the legal entity is registered or is operating (whichever is stricter). In case that the jurisdiction has no such laws, regulations, rules and standards or its laws, regulations, rules and standards are unenforceable, the individual or the legal entity are required to comply with Core International Labor Standards. The individual or the legal entity shall not induce or force its employee(s), whether full-time or part-time, or its independent contractor(s), in any methods, to agree in oral or written form, to directly or indirectly restrict, weaken or relinquish his or her rights or remedies under such laws, regulations, rules and standards relating to labor and employment as mentioned above, no matter whether such written or oral agreement are enforceable under the laws of the said jurisdiction, nor shall such individual or the legal entity limit, in any methods, the rights of its employee(s) or independent contractor(s) from reporting or complaining to the copyright holder or relevant authorities monitoring the compliance of the license about its violation(s) of the said license. THE LICENSED WORK IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN ANY WAY CONNECTION WITH THE LICENSED WORK OR THE USE OR OTHER DEALINGS IN THE LICENSED WORK. 版权所有（c） 反996许可证版本1.0 在符合下列条件的情况下，特此免费向任何得到本授权作品的副本（包括源代码、文件和/或相关内容，以下统称为“授权作品”）的个人和法人实体授权：被授权个人或法人实体有权以任何目的处置授权作品，包括但不限于使用、复制，修改，衍生利用、散布，发布和再许可： 个人或法人实体必须在许可作品的每个再散布或衍生副本上包含以上版权声明和本许可证，不得自行修改。 个人或法人实体必须严格遵守与个人实际所在地或个人出生地或归化地、或法人实体注册地或经营地（以较严格者为准）的司法管辖区所有适用的与劳动和就业相关法律、法规、规则和标准。如果该司法管辖区没有此类法律、法规、规章和标准或其法律、法规、规章和标准不可执行，则个人或法人实体必须遵守国际劳工标准的核心公约。 个人或法人不得以任何方式诱导或强迫其全职或兼职员工或其独立承包人以口头或书面形式同意直接或间接限制、削弱或放弃其所拥有的，受相关与劳动和就业有关的法律、法规、规则和标准保护的权利或补救措施，无论该等书面或口头协议是否被该司法管辖区的法律所承认，该等个人或法人实体也不得以任何方法限制其雇员或独立承包人向版权持有人或监督许可证合规情况的有关当局报告或投诉上述违反许可证的行为的权利。 该授权作品是&quot;按原样&quot;提供，不做任何明示或暗示的保证，包括但不限于对适销性、特定用途适用性和非侵权性的保证。在任何情况下，无论是在合同诉讼、侵权诉讼或其他诉讼中，版权持有人均不承担因本软件或本软件的使用或其他交易而产生、引起或与之相关的任何索赔、损害或其他责任。 https://link.zhihu.com/?target=https%3A//github.com/kattgu7/996-License-Draft/]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OC和Swift混编Frameowork优雅指南]]></title>
    <url>%2Fposts%2F56606%2F</url>
    <content type="text"><![CDATA[本文主要参考了优雅地开发Swift和Object C混编的Framework。不过实际发现，完全按照文章里面”优雅的解决方案“里面的说法操作，还是没法成功。我这里根据实际情况作出了调整。 参考的文章中在“优雅的解决方案”这个section之前的内容都是好用的，你可以用用来创建一个兼容OC和Swift的Cooca Touch Framework。 这里说的“优雅”，指的是控制OC部分接口保留的问题（详情可以参考原文部分） 原文里面只说了具体的操作步骤，没有高屋建瓴地说出这种方法的实际思路：事实上，采用module.modulemap的方法是将OC部分打包成一个可以使用Swfit语句进行导入(import)的模块。以这个视角，我们再来梳理一下操作步骤： 新建一个module.modulemap文件 文件里的内容如下： 12345module OCSource [system] &#123; //由于module.modulemap和OCSource.h是在同一个文件夹的，如果不是同一个，路径要写全 header &quot;OCSource.h&quot; export *&#125; 有一个容易犯错的问题是将这里的模块名字, OCSource命名为了Cocoa Touch Framework的名字。这样会导致编译出错，错误信息会提示你Module名字重复定义。这里的名字要区别的Framework的名字，具体是什么可以自己自由选择。不过推荐和头文件的名字一致 后一步操作是把module.modulemap的路径添加到Build Settings的Import Paths中，这是为了让我们在Swift里面import这个module的时候能够找到目标. Import Paths in Build Settings 那么，这里的$(SRCROOT)/MixFramework其实就是指的module.modulemap的路径。 将OCSouce.h文件的权限改为project Header Visibility Settings 这可以让OCSource.h不再对外可见。 然后，删除MixFramework.h(umbrella header)中#import 的OC header。 原文的内容到此结束，但是其实还是不够的。这时候如果编译，会发现你在Framework内部的Swift使用OCSource的地方都会报错说OCSource不存在。因为将OCSource.h从umbrella header中删除之后Swift就无法看到这个文件了。然而，通过module.modulemap文件我们将OCSource.h及相关的OC文件打包成了了一个Swift模块，因此我们可以在Swift代码中import进来： 1import OCSource 在报错的Swift文件中添加这个导入，就可以解决这个问题了.]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>ios</tag>
        <tag>swift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Universal(Fat) Framework for Swift Projects]]></title>
    <url>%2Fposts%2F28461%2F</url>
    <content type="text"><![CDATA[Cocoa Touch Framework 最近在给朋友做一个项目，要求将涉及到的算法内容整理成一个单独的framework，这样可以隐藏算法细节，方便交付。这个需求可以很容易地通过Cocoa Touch Framework实现。不过在交付的时候存在一个头疼的问题：默认情况下，Xcode在编译Cocoa Touch Framework时只会编译出支持模拟器或者真机的Framework，而无法编译出同时支持模拟器和真机的Framework，即Universal(Fat) Framework。这一需求还需要进一步地利用一些系统脚本来实现。 这里假设你已经有了一个能够正常工作，编译的包含Cocoa Touch Framework的工程。我这里实现时使用的是Xcode10.2。 事实上我在调研中发现了很多不同的实现编译Universal Framework的教程，但是他们并不总是有用，我这里只遴选了我自己测试通过没有问题的思路。这一思路通过Archive过程来打包输出framework 首先从Xcode左上角选择Cocoa Touch Framework的默认scheme，然后点击Edit Scheme Edit Scheme 在Archive的post-action中添加一个运行脚本(New Run Script Action) New Run Script Action 脚本内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748exec &gt; /tmp/$&#123;PROJECT_NAME&#125;_archive.log 2&gt;&amp;1UNIVERSAL_OUTPUTFOLDER=$&#123;BUILD_DIR&#125;/$&#123;CONFIGURATION&#125;-universalif [ "true" == $&#123;ALREADYINVOKED:-false&#125; ]thenecho "RECURSION: Detected, stopping"elseexport ALREADYINVOKED="true"# make sure the output directory existsmkdir -p "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;"echo "Building for iPhoneSimulator"xcodebuild -workspace "$&#123;WORKSPACE_PATH&#125;" -scheme "$&#123;TARGET_NAME&#125;" -configuration $&#123;CONFIGURATION&#125; -sdk iphonesimulator -destination 'platform=iOS Simulator,name=iPhone 6' ONLY_ACTIVE_ARCH=NO ARCHS='i386 x86_64' BUILD_DIR="$&#123;BUILD_DIR&#125;" BUILD_ROOT="$&#123;BUILD_ROOT&#125;" ENABLE_BITCODE=YES OTHER_CFLAGS="-fembed-bitcode" BITCODE_GENERATION_MODE=bitcode clean build# Step 1. Copy the framework structure (from iphoneos build) to the universal folderecho "Copying to output folder"# 这行是在我参考的脚本的基础上添加进去的。脚本在运行过程中有一个问题：在试图将# archive过程中生成的device framework拷贝进来时，总是拷贝的framework文件夹# 的内容，而非整个文件夹，所以我们这里手动创建这个文件夹mkdir -p "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;/$&#123;FULL_PRODUCT_NAME&#125;"cp -R "$&#123;ARCHIVE_PRODUCTS_PATH&#125;$&#123;INSTALL_PATH&#125;/$&#123;FULL_PRODUCT_NAME&#125;" "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;/$&#123;FULL_PRODUCT_NAME&#125;"# Step 2. Copy Swift modules from iphonesimulator build (if it exists) to the copied framework directorySIMULATOR_SWIFT_MODULES_DIR="$&#123;BUILD_DIR&#125;/$&#123;CONFIGURATION&#125;-iphonesimulator/$&#123;TARGET_NAME&#125;.framework/Modules/$&#123;TARGET_NAME&#125;.swiftmodule/."echo "SIMULATOR_SWIFT_MODULES_DIR: $&#123;SIMULATOR_SWIFT_MODULES_DIR&#125;"if [ -d "$&#123;SIMULATOR_SWIFT_MODULES_DIR&#125;" ]; thencp -R "$&#123;SIMULATOR_SWIFT_MODULES_DIR&#125;" "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;/$&#123;TARGET_NAME&#125;.framework/Modules/$&#123;TARGET_NAME&#125;.swiftmodule"fi# Step 3. Create universal binary file using lipo and place the combined executable in the copied framework directoryecho "Combining executables"lipo -create -output "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;/$&#123;FULL_PRODUCT_NAME&#125;/$&#123;EXECUTABLE_PATH&#125;" "$&#123;BUILD_DIR&#125;/$&#123;CONFIGURATION&#125;-iphonesimulator/$&#123;EXECUTABLE_PATH&#125;" "$&#123;ARCHIVE_PRODUCTS_PATH&#125;$&#123;INSTALL_PATH&#125;/$&#123;EXECUTABLE_PATH&#125;"# Step 4. Create universal binaries for embedded frameworks#for SUB_FRAMEWORK in $( ls "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;/$&#123;TARGET_NAME&#125;.framework/Frameworks" ); do#BINARY_NAME="$&#123;SUB_FRAMEWORK%.*&#125;"#lipo -create -output "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;/$&#123;TARGET_NAME&#125;.framework/Frameworks/$&#123;SUB_FRAMEWORK&#125;/$&#123;BINARY_NAME&#125;" "$&#123;BUILD_DIR&#125;/$&#123;CONFIGURATION&#125;-iphonesimulator/$&#123;SUB_FRAMEWORK&#125;/$&#123;BINARY_NAME&#125;" "$&#123;ARCHIVE_PRODUCTS_PATH&#125;$&#123;INSTALL_PATH&#125;/$&#123;TARGET_NAME&#125;.framework/Frameworks/$&#123;SUB_FRAMEWORK&#125;/$&#123;BINARY_NAME&#125;"#done# Step 5. Convenience step to copy the framework to the project's directoryecho "Copying to project dir"yes | cp -Rf "$&#123;UNIVERSAL_OUTPUTFOLDER&#125;/$&#123;FULL_PRODUCT_NAME&#125;" "$&#123;PROJECT_DIR&#125;"open "$&#123;PROJECT_DIR&#125;"fi 上述脚本的内容主要来自于export-fat-swift-dynamic-framework，我在这里根据实际情况进行了更改 此时执行archive操作(Product-&gt;Archive)完成后会自动弹出Finder窗口显示新生成的framework的位置（应当就是位于项目根目录下）。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>ios</tag>
        <tag>swift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时局图：论扛着红旗反红旗]]></title>
    <url>%2Fposts%2F8446%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
        <tag>政治</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS:图像截取部分(Image Cropping)]]></title>
    <url>%2Fposts%2F2543%2F</url>
    <content type="text"><![CDATA[Cover 这里我们讨论的图像截取部分是指从一个完整的大图中截取一小部分出来。当然，使用js实现。 这边文章基本整理自Cropping images with Javascript， 添加了一些我的评论 例如，我们要从这样的大图中： 大图 截取出 小图 使用H5中的canvas可以简单地解决这个问题。 1. 载入原图像 1234567891011121314var loadTimer;var imgObject = new Image();imgObject.src = 'images/fozzie.jpg';imgObject.onLoad = onImgLoaded();function onImgLoaded() &#123; if (loadTimer != null) clearTimeout(loadTimer); if (!imgObject.complete) &#123; loadTimer = setTimeout(function() &#123; onImgLoaded(); &#125;, 3); &#125; else &#123; onPreloadComplete(); &#125;&#125; 注意这里我们为了演示是读取的图片文件内容，实际上除了图像文件，这里的“图像”还可以是其他形式，例如video元素，别的canvas等。 2. 当图片完成载入以后，重新绘制你要截取的那一部分 123456function onPreloadComplete()&#123; //call the methods that will create a 64-bit version of thumbnail here. var newImg = getImagePortion(imgObject, 120, 150, 150, 80, 2); //place image in appropriate div document.getElementById("images").innerHTML = "&lt;img alt="" src=""+newImg+"" /&gt;";&#125; 这个onPreloadComplete函数会在图像载入完成以后调用。在这个函数中我们会调用实际完成图片截取的函数getImagePortion 3. 图像截取 123456789101112131415161718getImagePortion(imgObj, newWidth, newHeight, startX, startY, ratio)&#123; /* the parameters: - the image element - the new width - the new height - the x point we start taking pixels - the y point we start taking pixels - the ratio */ //set up canvas for thumbnail var tnCanvas = document.createElement('canvas'); var tnCanvasContext = canvas.getContext('2d'); tnCanvas.width = newWidth; tnCanvas.height = newHeight; /* use the sourceCanvas to duplicate the entire image. This step was crucial for iOS4 and under devices. Follow the link at the end of this post to see what happens when you don’t do this */ var bufferCanvas = document.createElement('canvas'); var bufferContext = bufferCanvas.getContext('2d'); bufferCanvas.width = imgObj.width; bufferCanvas.height = imgObj.height; bufferContext.drawImage(imgObj, 0, 0); /* now we use the drawImage method to take the pixels from our bufferCanvas and draw them into our thumbnail canvas */ tnCanvasContext.drawImage(bufferCanvas, startX,startY,newWidth * ratio, newHeight * ratio,0,0,newWidth,newHeight); return tnCanvas.toDataURL();&#125; 上面的函数时原作者给出的方法，他先将图像完整地画到一个canvas(bufferCanvas)上，再将这个canvas对应的目标区域画到tnCanvas上，根据注释来看，似乎是出于性能或者适配方面的考虑。不过就我在开发桌面端网页时，可以直接将imgObj画到tnCanvas上。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】IOS的一些设计规范]]></title>
    <url>%2Fposts%2F23804%2F</url>
    <content type="text"><![CDATA[转载自BIGD团队。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>转载</tag>
        <tag>ios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac: 将APP打包成dmg]]></title>
    <url>%2Fposts%2F62609%2F</url>
    <content type="text"><![CDATA[创建一个新的文件夹，将APP放到这个新文件夹中 打开Disk Utility &gt; File &gt; New Image &gt; Image from Folder.（中文的话，是磁盘工具 &gt; 文件 &gt; 新建映像 &gt; 来自文件夹的映像...） 在弹出的窗口中，选择在第一步中新建的文件夹 选择输出dmg文件的存储位置，然后点击保存按钮]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>MacOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】在小米电视和小米盒子上看YOUTUBE]]></title>
    <url>%2Fposts%2F5827%2F</url>
    <content type="text"><![CDATA[使用小米电视和小米盒子看YouTube上的视频，是很多中国电视用户很想做的事情，下面，我就介绍一种很简单的方法，不用ROOT小米电视或小米盒子，不用重装系统，几分钟的设置就可以在小米电视和小米盒子上看YouTube的方法。 首先需要下载两个APK应用，第一个是SmartYouTubeTV，点击这里下载最新版SmartYouTubeTV，将其复制到U盘。第二个是Shadowsocks，访问apkmirror网站，搜索Shadowsocks，找到最新版后，下载universal的apk到U盘即可。 之后，打开小米电视或小米盒子，在“设置-账户与安全”里，选择“允许安装未知来源的应用”。插入U盘，将上述两个apk文件安装到电视上。 最后，在Shadowsocks上设置好服务器地址，打开SmartYouTubeTV，选择第一个，然后可以选择登陆Google账号，登陆的时候，会让用户在手机上访问 youtube.com/activate 来登陆激活，登陆好了后，电视即可和电脑浏览器的YouTube同步了。 Smart YouTube TV里登陆Google账号后，你会发现，YouTube里的订阅、上传、历史什么的功能全部可以正常使用了，完美支持小米遥控器控制，观看视频体验极佳，完全不亚于官方的应用。 当然，用户也可以选择安装官方的YouTube应用，但必须安装Google框架等一堆东西，使用体验可能还未必好。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>转载</tag>
        <tag>翻墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在普通网络环境下上北邮人]]></title>
    <url>%2Fposts%2F54288%2F</url>
    <content type="text"><![CDATA[在学校里看剧、电影，下载破解游戏基本都靠北邮人。一方面资源比较全，另一方面是走IPv6，不需要走计费的校园网IPv4流量。不过由于北邮人只支持IPv6，而国内IPv6基本只有校园网有。问题来了，怎么在校外的纯IPv4环境下使用IPv6 Only的北邮人呢？ 从IPv4到IPv6 这是最重要的一步。你首先需要一个支持IPv6的VPS。国内目前支持IPv6的好像只有阿里云？，即便支持，国内的IPv6 VPS又贵又难用（需要申请）。因此最好的方案是采用海外的VPS。听起来用海外的VPS会很慢？其实海外的VPS主要是延时高，其实速度还是挺快的，而且P2P传输业务受到延时的影响挺小的，实测利用我的VPS可以达到5MB/s的P2P下载速度（在服务器上看上下行都是5MB/s，基本跑满了100M的带宽）。我用的VPS是Digital Ocean的旧金山节点。价格是$5一个月。平均下来每天一块钱吧。注意创建Droplet的时候要自己勾选IPv6（添加IPv6是免费的）。 选择IPv6 在服务器上我部署了Shadowsocks服务。SS服务器可以直接无痛支持IPv4到IPv6的转换。关于如何部署Shadowsocks，这方面的教程文章网上汗牛充栋，我这里就不提供了。 设置 首先需要将北邮人的网址bt.byr.cn添加到Shadowsocks客户端的代理列表。 Shadowsocks选项 点击Shadowsocks小飞机，选择“编辑PAC用户自定规则”。在弹出的框中输入||bt.byr.cn： 编辑PAC用户自定规则 然后你就能在IPv4网络环境下打开北邮人的网页啦。 接下来是设置下载客户端uTorrent的网络设置。打开uTorrent的设置(Preferences)，进入到Network。进行如下设置： uTorrent设置 注意：上面的Socks5设置中，端口会与你的Shadowsocks设置有关。如果你没有动过相关设置的话，应该就是1086端口。 查看你的Shadowsocks客户端Sock5代理端口设置的方式是单击Shadowsocks小飞机，选择偏好设置，在弹出的窗口中点击“高级”，其中“本地Socks5监听端口”即为应该填写到uTorrent设置中的代理端口。 大功告成！_(:з」∠)_]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SimpleOpenNI在Processing导出应用中的库引用问题]]></title>
    <url>%2Fposts%2F65501%2F</url>
    <content type="text"><![CDATA[在Processing中使用SimpleOpenNI时，如果尝试将本来能够正常运行的pde文件导出成应用，那么在运行时会出现java.lang.UnsatisfiedLinkError这个错误。详细信息如下： 1234567891011121314Can&apos;t load SimpleOpenNI library (libSimpleOpenNI.jnilib) : java.lang.UnsatisfiedLinkError: Can&apos;t load library: /SimpleOpenNI/library/libSimpleOpenNI.jnilibVerify if you installed SimpleOpenNI correctly.http://code.google.com/p/simple-openni/wiki/Installationjava.lang.UnsatisfiedLinkError: SimpleOpenNI.SimpleOpenNIJNI.swig_module_init()V at SimpleOpenNI.SimpleOpenNIJNI.swig_module_init(Native Method) at SimpleOpenNI.SimpleOpenNIJNI.&lt;clinit&gt;(SimpleOpenNIJNI.java:290) at SimpleOpenNI.ContextWrapper.&lt;init&gt;(ContextWrapper.java:54) at SimpleOpenNI.SimpleOpenNI.&lt;init&gt;(SimpleOpenNI.java:253) at Sketch.settings(Sketch.java:28) at processing.core.PApplet.handleSettings(PApplet.java:954) at processing.core.PApplet.runSketch(PApplet.java:10786) at processing.core.PApplet.main(PApplet.java:10511) at Main.main(Main.java:7) 根据错误信息，是在读取libSimpleOpenNI.jnilib这个库文件时失败导致的。奇怪的是，程序尝试读取的路径是：/SimpleOpenNI/library/libSimpleOpenNI.jnilib。这是一个很奇怪的绝对路径。也有人尝试直接将库文件复制到这个全局路径的位置，可以让程序运行起来。可是这种方法也太不优雅了。 为什么会出现这种现象？ 通过IntelliJ可以打开SimpleOpenNI.jar查看代码细节。可以看到SimpleOpenNI.class中确定载入库文件路径的方式如下： 1234567891011121314151617181920212223242526272829303132static &#123; String var0 = System.getProperty("os.name").toLowerCase(); String var1 = "SimpleOpenNI"; String var2 = System.getProperty("os.arch").toLowerCase(); if (var0.indexOf("win") &gt;= 0) &#123; // ... &#125; else if (var0.indexOf("nix") &lt; 0 &amp;&amp; var0.indexOf("linux") &lt; 0) &#123; if (var0.indexOf("mac") &gt;= 0) &#123; var1 = "lib" + var1 + ".jnilib"; nativLibPath = getLibraryPathLinux() + "/SimpleOpenNI/library/"; nativDepLibPath = nativLibPath + "osx/"; &#125; &#125; else &#123; nativLibPath = "/SimpleOpenNI/library/linux"; if (var2.indexOf("86") &gt;= 0) &#123; var1 = var1 + "32"; &#125; else if (var2.indexOf("64") &gt;= 0) &#123; var1 = "lib" + var1 + "64.so"; nativLibPath = getLibraryPathLinux() + "/SimpleOpenNI/library/"; nativDepLibPath = nativLibPath + "linux64/"; &#125; &#125; try &#123; System.load(nativLibPath + var1); &#125; catch (UnsatisfiedLinkError var5) &#123; System.out.println("Can't load SimpleOpenNI library (" + var1 + ") : " + var5); System.out.println("Verify if you installed SimpleOpenNI correctly.\nhttp://code.google.com/p/simple-openni/wiki/Installation"); &#125; _initFlag = false; &#125; 注意到在生成库文件路径时，/SimpleOpenNI/library/libSimpleOpenNI.jnilib，前面应该会添加getLibraryPathLinux()的结果。 123456789101112public static String getLibraryPathLinux() &#123; URL var0 = SimpleOpenNI.class.getResource("SimpleOpenNI.class"); if (var0 != null) &#123; String var1 = var0.toString().replace("%20", " "); int var2 = var1.indexOf(47); boolean var3 = true; int var4 = var1.indexOf("/SimpleOpenNI/library"); return -1 &lt; var2 &amp;&amp; -1 &lt; var4 ? var1.substring(var2, var4) : ""; &#125; else &#123; return ""; &#125; &#125; 我尝试了在不同环境下,SimpleOpenNI.class.getResource(&quot;SimpleOpenNI.class&quot;)下运行的结果。发现： 在pde运行时，获取到的是独立的SimpleOpenNI.jar下的路径，例如：/Users/lena/Documents/Processing/libraries/SimpleOpenNI/library/SimpleOpenNI.jar!/SimpleOpenNI/SimpleOpenNI.class 在导出应用中运行时，获取到的是打包后应用内的，例如.../MySketch/application.macosx/MySketch.app/Contents/Java/SimpleOpenNI.jar!/SimpleOpenNI/SimpleOpenNI.class 在函数getLibraryPathLinux中，程序会定位/SimpleOpenNI/library这个字符串，然后取出这个子字符串前的内容构成的路径。上述第二种情形内，SimpleOpenNI.jar被打包到应用内后，不在处于/SimpleOpenNI/library这个前缀目录下，所以导致定位失败。 如何解决这个问题。 在无法直接修改SimpleOpenNI的源代码的情况下，要修复这个问题，就要想办法把SimpleOpenNI.jar放到SimpleOpenNI/library目录下。我使用的macOS系统，下面的方法都是在Mac下测试。不过基本思路可以迁移到Windows上。 在生成的App上右键选择显示包内容。可以查看其内部结构： 123456789101112131415161718192021222324252627282930.├── Info.plist├── Java│ ├── Sketch.jar│ ├── NiTE2│ ├── SimpleOpenNI.jar│ ├── SimpleOpenNI32.dll│ ├── SimpleOpenNI64.dll│ ├── core.jar│ ├── data│ ├── gluegen-rt-natives-macosx-universal.jar│ ├── gluegen-rt.jar│ ├── javamp3-1.0.3.jar│ ├── jogl-all-natives-macosx-universal.jar│ ├── jogl-all.jar│ ├── jsyn-20171016.jar│ ├── libSimpleOpenNI.jnilib│ ├── libSimpleOpenNI64.so│ ├── osx│ ├── sound.jar│ ├── win32│ └── win64├── MacOS│ └── Sketch├── PkgInfo├── PlugIns│ └── jdk1.8.0_181.jdk└── Resources ├── en.lproj └── sketch.icns 可以看到SimpleOpenNI.jar位于Java目录下。我尝试过直接在此处创建目录SimpleOpenNI/library并把SimpleOpenNI.jar放进去。但是运行提示无法找到SimpleOpenNI.jar。这需要在APP运行时进一步指定CLASSPATH。有一种方法是直接在Info.plist文件里面添加-Djava.class.path运行属性，或者添加ClASSPATH环境变量，但是这种方法会要求你手动填写所有需要使用的jar依赖，甚至是包括processing的jar文件。这对于后续维护和修改很不利。所以这里我采取了另一种取巧的办法。 进入Contents/MacOS目录，删除原来的Sketch文件(你看到的应该是和你的Processing程序同名的文件，我这里用Sketch来代替)。新建一个同名的空白的文本文件，然后在文件中添加如下内容： 1234567891011121314151617181920212223242526#!/bin/bashcd "$(dirname $&#123;BASH_SOURCE&#125;)"cd ../..APP_ROOT=$(pwd)cd Contents/JavaJAR_LIBS=$(ls *.jar | tr "\n" ":")# 添加SimpleOpenNI.jarJAR_LIBS=$&#123;JAR_LIBS&#125;./SimpleOpenNI/library/SimpleOpenNI.jarAPP_NAME=$(basename "$&#123;BASH_SOURCE&#125;")# 注意：如果你内嵌的jdk的版本不同，要把jdk1.8.0_181.jdk替换成对应的版本# 如果你没有在app内部内嵌jdk，这里修改成JAVA_BIN=java，使用系统全局的java即可JAVA_BIN=$&#123;APP_ROOT&#125;/Contents/PlugIns/jdk1.8.0_181.jdk/Contents/Home/jre/bin/java$&#123;JAVA_BIN&#125; \-Djna.nosys=true \-Djava.ext.dirs=$APP_ROOT/Contents/PlugIns/jdk1.8.0_181.jdk/Contents/Home/jre/lib/ext \-Xdock:icon=$APP_ROOT/Contents/Resources/sketch.icns \-Djava.library.path=$APP_ROOT/Contents/Java \-Dapple.laf.useScreenMenuBar=true \-Dcom.apple.macos.use-file-dialog-packages=true \-Dcom.apple.macos.useScreenMenuBar=true \-Dcom.apple.mrj.application.apple.menu.about.name=$&#123;APP_NAME&#125; \-classpath $&#123;JAR_LIBS&#125; $&#123;APP_NAME&#125; 为这个文件添加可执行权限 1chmod +x ./Sketch 将~/Documents/Processing/libraries/SimpleOpenNI整个文件夹拷贝进导出APP的Contents/Java目录下。然后就可以运行了。]]></content>
      <categories>
        <category>processing</category>
      </categories>
      <tags>
        <tag>processing</tag>
        <tag>debug</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shadowsocks中继:从IPv4到IPv6]]></title>
    <url>%2Fposts%2F6289%2F</url>
    <content type="text"><![CDATA[最近墙又双叒叕加高了。在春节前就发现自己的VPS无法连接，后来发现还好只是端口被封禁，换成其他的端口就能使用了。不过这才撑了半个月新的端口访问又不太稳定了。如果再换端口，或许也可以。但是不是长久之计。不过我的VPS是支持IPv6的，一般来说，墙对于IPv6流量的拦截比较弱。或许可以想办法先把自己的流量转换成IPv6然后再出去。 我也设想过要不要给代理添加混淆的功能，处于以下几方面的考虑，还是选择了流量转换的方案： 1. 手机端部分ss应用不支持混淆； 2. 未来混淆还是可能被针对性的拦截。但是IPv6则不会。GFW拦截还是拦截大鱼不拦截小鱼的。国内目前IPv6的使用范围仍然非常小，而且基本只限于教育网。因此IPv6在未来的很长一段时间内不会成为GFW的针对目标 我们这里使用HAProxiy来完成这一功能。 安装HAProxy 1234wget http://www.haproxy.org/download/1.7/src/haproxy-1.7.2.tar.gztar -xzf haproxy-1.7.2.tar.gzmake TARGET=linux2826 USE_GETADDRINFO=1sudo make install 注意，在倒数第二行的make命令中，TARGET需要根据你的内核版本来选择。USE_GETADDRINFO的作用是使得HAProxy可以对域名采用DNS查询来获取IP。使用包管理器安装的HAProxy是不带这个功能的。 设置 123456789101112131415161718global ulimit-n 51200 daemon # run as daemondefaults log global mode tcp option dontlognull timeout connect 1000 timeout client 150000 timeout server 150000frontend ss-in bind *:port # 跳板机监听端口 default_backend ss-outbackend ss-out server server1 vps_host:vps_ss_port maxconn 20480 设置文件位于/etc/haproxy/haproxy.cfg。在完成设置后，使用sudo haproxy -f /etc/haproxy/haproxy.cfg来运行。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】无人机击落客机只是时间问题]]></title>
    <url>%2Fposts%2F48586%2F</url>
    <content type="text"><![CDATA[我是一个无人机集群技术的研究者，从最近开始我打算集中整理发布一些无人机，尤其是无人机集群技术的新进展以及评论文章。 原文链接：It’s only a matter of time before a drone takes down a passenger plane Cover 2018年12月，英国第二大机场盖特威克机场，发现有一架无人机飞过机场，不得不关闭一天，几十万旅客受到影响。目前还不知道这架无人机是谁操作，为什么要飞入机场。 这个事件表明，无人机对商业航空已经构成威胁。更严重的是，&quot;反无人机&quot;技术起不了多大作用。无人机已经变得太便宜，太强大，客机将不可避免地受到影响。无论是开枪、无线电干扰、或者其他措施，都无法可靠地保护客机。这可能听起来危言耸听，但我们对无人机真的缺乏办法。 现在，消费者可以买到的最便宜无人机，只需要25美元。这些产品接受遥控器的无线信号，相对容易防范，只要干扰它们的无线电信号，就可以了。稍微昂贵的无人机有 GPS 芯片，这种无人机可以编程设置一个&quot;地理围栏&quot;，防止它们飞入指定的地理坐标范围内。 但是，上面的这些措施，只能防住普通消费者从正规渠道买到的无人机。对于具有中等技术水平的人来说，制造一架无人机很容易，自制无人机也不需要 GPS 芯片。它们也不一定需要与操作员通信，才能保持飞行，这使得无线电干扰无效。而且，强度太大的干扰信号，反而可能会影响到本来要保护的客机。 可以肯定的是，一架无人机攻击一架客机，成功机会不大。这是因为在起飞和着陆时（最容易遭遇无人机的阶段），客机的移动速度非常快，通常在每小时150到200英里之间，很少有无人机能够以50~70英里/小时的速度飞行，所以客机应该可以避开无人机。此外，飞机的设计可以承受鸟撞，如果一架无人机意外撞到客机，客机可能只会受到轻微损坏，很可能还是能够安全降落。 但是，如果无人机成群飞行，事情就会发生变化。虽然单个无人机很难攻击飞机，但是在客机的飞行路径上放置30架无人机，就可能会发生变化。考虑到无人机的价格，多架无人机群体攻击是很容易的。如果通过编程，找出客机的引擎（通过红外传感或通过图像），然后无人机携带少量爆炸物，撞击可能会致命。 总之，对于那些蓄意攻击客机的半自动或全自动无人机集群，根本就没有好的技术对策。]]></content>
      <categories>
        <category>科技新闻</category>
      </categories>
      <tags>
        <tag>无人机</tag>
        <tag>科技新闻</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netlink:用户空间与内核空间交互]]></title>
    <url>%2Fposts%2F2348%2F</url>
    <content type="text"><![CDATA[Reference 1 什么是Netlink Netlink is a socket family that supplies a messaging facility based on the ++BSD socket interface++ to send and retrieve kernel-space information from user-space. Netlink is portable, highly extensible and it supports ++event-based notifications++. 从这段描述来看Netlink可以提供类似socket接口，这意味着我们能够传输比较大量的，结构化的数据。另外，Netlink还提供了基于时间通知的功能，也适合我们时刻监控系统动态。 Netlink是一种面向数据表(datagram-oriented)的连通用户空间和内核空间的__++消息系统++__。同时，Netlink也可以用于进程间通信(InterProcess Communication, IPC)。我们这里只关注前者。Netlink构筑与通用的BSD scoket基础设施之上，因此支持使用socket(), bind(), sendmsg(), recvmsg()和其他通常的socket polling操作。 一般的BSD socket使用的是固定格式的数据结构(如AF_INET或者AF_RAW)。Netlink则提供更加可扩展的数据格式。 2 Netlink的典型应用场景 当前Netlink主要应用场景是网络相关应用，包括： advanced routing IPsec key management tools firewall state synchronization uesr-space packet enqueuing border gateway routing protocols wireless mesh routing protocols 这个应用场景与我们的需要时契合的 3 Netlink总线 Netlink允许最多32条内核空间总线。一般来说每个总线都关联到一个内核子系统中（多个子系统也可以共享一个总线）。总线共享的例子包括： nfnetlink：所有防火墙相关子系统共享 rtnetlink：网络设备管理，路由和队列管理 关于Netlink总线，我发现了一个内核的patch，其中提到，&quot;This patchset aims to improve this situation by add ing a new NETLINK_DESC bus with two commands...&quot; 4 Netlink通信类型 Netlink支持两种通信类型： Unicast：一对一通信，即一个内核子系统对应一个用户空间程序。这种通信模式一般用来发送命令，或者获取命令执行的结果。 Multicast：一对多通信。通常的场景是一个内核态模块向多个用户态监听者发送消息。这种监听者被划分为多个不同的组。一条Netlink总线可以提供多个组，用户空间可以订阅到一个或者多个组来获取对应的信息。最多可以创建 个组。 Example scenario of unicast and multicast Netlink sockets 上图给出了Unicast和Multicast的图示。注意这里unicast是同步的，multicast是异步的。 5 Netlink消息格式 一般来说，Netlink消息对齐到32bit，其内部数据是host-byte order. 一个Netlink消息总由一段16bytes的header组成，header的格式为struct nlmsghdr（定义在&lt;include/linux/netlink.h&gt;中） Layout of a Netlink message header header包含如下字段： 消息长度（32bits, 包含header的长度） 消息类型（16bits）。消息类型的划分有两大类别：数据消息和控制消息。其中数据消息的类型取决于内核模块所允许的取值。控制消息类型则对所有Netlink子系统是一致的。控制消息的类型目前一共有四种。 NLMSG_NOOP: 不对对应任何实质操作，只用来检测Netlink总线是否可用 NLMSG_ERROR：该消息包含了错误信息 NLMSG_DONE：this is the trailing message that is part of a multi-part message. A multi-part message is composed of a set of messages all with the NLM_F_MULTI flag set. NLMSG_OVERRUN：没有使用 消息标识(16bits)。一些例子如下： NLM_F_REQUEST: 如果这个标识被设置了，表明这个消息代表了一个请求。从用户空间发往内核空间的请求必须要设置这个标识，否则内核子系统必须要回复一个invalid argument(EINVAL)的错误信息。 NLM_F_CREATE: 用户空间想要发布一个命令，或者创建一个新的配置。 NLM_F_EXCL: 通常和NLM_F_CREATE一起使用，用来出发配置已经存在的错误信息。 NLM_F_REPLACE: 用户空间想要替换现有配置。 NLM_F_APPEND: 想现有配置添加配置。这种操作一般针对的是有序的数据，如路由表。 NLM_F_DUMP: 用户应用想要和内核应用进行全面重新同步。这中消息的结果是一系列的multipart message。 NLM_F_MULTI: this is a multi-part message. A Netlink subsystem replies with a multi-part message if it has previously received a request from user-space with the NLM F DUMP flag set. NLM_F_ACK: 设置了这个标识后，内核会返回一个确认信息表明一个请求已经执行。如果这个flag没有返回，那么错误信息会作为sendmsg()函数的返回值同步返回。 NLM_F_ECHO: if this flag is set, the user-space application wants to get a report back via unicast of the request that it has send. 注意通过这种方式获取信息后，这个程序不会再通过事件通知系统获取同样的信息。 Sequence Number (32bits): The sequence number is used as a tracking cookie since the kernel does not change the sequence number value at all 可以和NLM_F_ACK一起使用，用户空间用来确认一个请求被正确地发出了。 Netlink uses the same sequence number in the messages that are sent as reply to a given request For event-based notifications from kernel-space, this is always zero. Port-ID (32bits): 包含了Netlink分配的一个数字ID。Netlink使用不同的port ID来确定同一个用户态进程打开的不同socket通道。第一个socket的默认port ID是这个进程的PID(Process ID)。在下面这些场景下，port ID为0： 消息来自内核空间 消息发送自用户空间，我们希望Netlink能够自动根据socket通道的port ID自动设置消息的port ID 以上是通用Netlink header格式。一些内核子系统会进一步定义自己的header格式，这样不同的子系统可以共享同一个Netlink socket总线。这种情形成为GetNetlink。 6 Netlink负载 6.1 Type-Length-Value(TLV)格式 An example of a hypothetical Netlink payload in TLV format Netlink的消息格式由TLV格式的属性组成。TLV属性分为Length, Type和Payload三部分。这种格式具有很强的可扩展性。在内核中，TLV属性的header定义如下: 12345678910111213/* * &lt;------- NLA_HDRLEN ------&gt; &lt;-- NLA_ALIGN(payload)--&gt; * +---------------------+- - -+- - - - - - - - - -+- - -+ * | Header | Pad | Payload | Pad | * | (struct nlattr) | ing | | ing | * +---------------------+- - -+- - - - - - - - - -+- - -+ * &lt;-------------- nlattr-&gt;nla_len --------------&gt; */struct nlattr &#123; __u16 nla_len; __u16 nla_type;&#125;; nla_type：属性的取值很大程度上取决于内核空间子系统定义。不过Netlink预先定了两个重要的比特位： NLA_F_NETSTED: 是否是嵌套属性。即在payload部分，以TLV的格式存储了更多的属性。 NLA_F_NET_BYTEORDER: payload内容的字节顺序（是否是network byte order(1)) nla_len: 注意，尽管payload部分会按照32bit进行对齐，这里的长度内容是不包含对齐补全的bit的。另外，这里的长度值包含了header。 7 Netlink错误消息 Layout of a Netlink error message Netlink提供了一种包含了Netlink error header的消息类型，其格式如上图所示。这个header定义为struct nlmsgerr (&lt;include/linux/netlink.h&gt;) 12345678910111213struct nlmsgerr &#123; int error; struct nlmsghdr msg; /* * followed by the message contents unless NETLINK_CAP_ACK was set * or the ACK indicates success (error == 0) * message length is aligned with NLMSG_ALIGN() */ /* * followed by TLVs defined in enum nlmsgerr_attrs * if NETLINK_EXT_ACK was set */&#125;; error: 错误类型。定义在error.h中，可以用perror()解析。 Netlink消息，为触发此错误的消息内容。 &gt; With regards to message integrity, the kernel subsystems that support Netlink usually report invalid argument (EINVAL) via recvmsg() if user-space sends a malformed message 8 GeNetlink 前文我们提到过GetNetlink了。这一技术是为了缓解Netlink总线数量过少的问题。GeNetlink allows to register up to 65520 families that share a single Netlink bus. Each family is intended to be equivalent to a virtual bus。其中，每个family通过一个唯一的string name and ID number来注册。其中string name作为主键，而ID number在不同的系统中可能不同。 9 Netlink开发 Netlink开发涉及到内核空间和用户空间双边的开发。Linux提供了很多帮助函数来见过Netlink开发中重复性的解析，验证，消息构建的操作。 9.1 用户空间开发 从用户空间这一侧来看，Netlink sockets实现在通用的BSD socket接口之上。因此，在用户空间开发Netlink和开发TCP/IP socket应用是很类似的。不过，同其他典型的BSD socket应用相比，Netlink存在以下的不同之处： Netlink sockets do not hide protocol details to user-space as other protocols to. 即，Netlink会直接处理原始数据本身，用户空间的开发也要直接处理原始数据格式的负载。 Errors that comes from Netlink and kernel subsystems are not returned by recvmsg() as an integer. Instead, errors are encapsulated in the Netlink error message. 唯一的例外是No buffer space error (ENOBUFS)，这个错误是表明无法将Netlink消息放入队列。标准的通用socket错误，同样也是从recvmsg()中以integer形式返回。 涉及用户空间的Netlink开发的有两个库：libnl和libmnl。这些库都是用C开发，用来简化Netlink开发。Netlink用户空间的进一步开发可以参考这两个库的例子和教程。 原始API的文档：https://www.systutorials.com/docs/linux/man/7-netlink/ 9.1.1 打开socket 下面来阐述一下用户空间的Netlink开发的重要事项。前面提到Netlink使用了BSD socket的接口。一般而言，创建socket的接口长这样子（socket接口）： 1int socket (int family, int type, int protocol); 第一个参数family是socket的大类。在开发TCP/IP应用的时候，这里总是AF_INET。而在Netlink中，这里总是设置为AF_NETLINK。 type可以选择SOCK_RAW或者SOCK_DGRAM。不过Netlink并不会区分这两者。 protocol为Netlink场景下定义的具体协议类型，现有的主要协议包括： 123456789101112131415161718192021222324#define NETLINK_ROUTE 0 /* Routing/device hook */#define NETLINK_UNUSED 1 /* Unused number */#define NETLINK_USERSOCK 2 /* Reserved for user mode socket protocols */#define NETLINK_FIREWALL 3 /* Unused number, formerly ip_queue */#define NETLINK_SOCK_DIAG 4 /* socket monitoring */#define NETLINK_NFLOG 5 /* netfilter/iptables ULOG */#define NETLINK_XFRM 6 /* ipsec */#define NETLINK_SELINUX 7 /* SELinux event notifications */#define NETLINK_ISCSI 8 /* Open-iSCSI */#define NETLINK_AUDIT 9 /* auditing */#define NETLINK_FIB_LOOKUP 10 #define NETLINK_CONNECTOR 11#define NETLINK_NETFILTER 12 /* netfilter subsystem */#define NETLINK_IP6_FW 13#define NETLINK_DNRTMSG 14 /* DECnet routing messages */#define NETLINK_KOBJECT_UEVENT 15 /* Kernel messages to userspace */#define NETLINK_GENERIC 16/* leave room for NETLINK_DM (DM Events) */#define NETLINK_SCSITRANSPORT 18 /* SCSI Transports */#define NETLINK_ECRYPTFS 19#define NETLINK_RDMA 20#define NETLINK_CRYPTO 21 /* Crypto layer */#define NETLINK_INET_DIAG NETLINK_SOCK_DIAG 我们可以直接使用NETLINK_USERSOCK供自己使用，或者自己定义一个新的量。 这里的protocol应当对应的是1.1.3中提到的总线。推理过程如下： 1. https://lwn.net/Articles/746776/ 这个链接中提叫的patch描述中称：This patch set aims to improve this situation by adding a new NETLINK_DESC bus with two commands 2. 在参考文献中谈论Netlink总线时，聚到了rtnetlink这个例子。根据rtnetlink的man page， #include &lt;asm/types.h&gt; #include &lt;linux/netlink.h&gt; #include &lt;linux/rtnetlink.h&gt; #include &lt;sys/socket.h&gt; rtnetlink_socket = socket(AF_NETLINK, int socket_type, NETLINK_ROUTE); 9.1.2 绑定socket地址 在打开了一个socket之后，我们需要为socket绑定一个本地地址。Netlink的地址格式如下： 1234567struct sockaddr_nl&#123; sa_family_t nl_family; /* AF_NETLINK */ unsigned short nl_pad; /* zero */ __u32 nl_pid; /* process pid */ __u32 ; /* mcast groups mask */&#125; nladdr; 这里的nl_pid可以通过getpid()这个函数来获取当前进程的pid来进行赋值 如果要在一个进程的多个线程中打开多个socket，可以用如下公式生成nl_pid： 1pthread_self() &lt;&lt; 16 | getpid(); struct socketadd_nl中的nl_groups为bit mask，代表了广播分组。当设置为0时代表单播消息。 确定地址后可以将其绑定到socket 12// fd为socket()返回的句柄bind(fd, (struct sockaddr*)&amp;nladdr, sizeof(nladdr)); 9.1.3 发送Netlink消息 为了发送Netlink消息，我们还需要创建一个struct socketaddr_nl作为发送的目的地址。如果消息是发送给内核的，那么nl_pid和nl_groups都要设置为0。如果这个消息是一个多播消息，那么需要设置nl_groups的对应比特。设置好目的地址之后，我们可以开始组装sentmsg()API需要的消息格式 123struct msghdr msg;msg.msg_name = (void *)&amp;(nladdr);msg.msg_namelen = sizeof(nladdr); 上面是socket的通用header，我们还需要设置Netlink自己的Message header这里struct nlmsghdr定义为： 12345678struct nlmsghdr&#123; __u32 nlmsg_len; /* Length of message */ __u16 nlmsg_type; /* Message type*/ __u16 nlmsg_flags; /* Additional flags */ __u32 nlmsg_seq; /* Sequence number */ __u32 nlmsg_pid; /* Sending process PID */&#125;; 在1.5中我们队各个字段的含义有了详细的介绍。按照对应的含义进行设置。 Netlink的消息由Netlink header和payload组成。因此我们需要一次性创建包含header和payload的内存块。 12345struct nlmsghdr *nlh = (struct nlmsghdr *)malloc(NLMSG_SPACE(MAX_PAYLOAD)); memset(nlh, 0, NLMSG_SPACE(MAX_PAYLOAD));nlh-&gt;nlmsg_len = NLMSG_SPACE(MAX_PAYLOAD);nlh-&gt;nlmsg_pid = getpid();nlh-&gt;nlmsg_flags = 0; 此处使用的NLMSG_SPACE宏定义是Netlink提供的工具，其定义如下： 12#define NLMSG_LENGTH(len) ((len) + NLMSG_HDRLEN)#define NLMSG_SPACE(len) NLMSG_ALIGN(NLMSG_LENGTH(len)) 这个宏做了两件事： 在长度上加上header的长度 将Payload进行32bit对齐 设置好负载内容后（负载数据段可以通过NLMSG_DATA(nlh)来获取），就可以发送了： 123456789struct iovec iov;iov.iov_base = (void *)nlh;iov.iov_len = nlh-&gt;nlmsg_len;msg.msg_iov = &amp;iov;msg.msg_iovlen = 1;sendmsg(fd, &amp;msg, 0); 9.1.3 接收Netlink消息 接收过程是类似的。接收程序需要提前分配一个足够的buffer来接收Netlink消息： 123456789101112struct sockaddr_nl nladdr;struct msghdr msg;struct iovec iov;iov.iov_base = (void *)nlh;iov.iov_len = MAX_NL_MSG_LEN;msg.msg_name = (void *)&amp;(nladdr);msg.msg_namelen = sizeof(nladdr);msg.msg_iov = &amp;iov;msg.msg_iovlen = 1;recvmsg(fd, &amp;msg, 0); 9.2 内核空间开发 9.2.1 创建新的Netlink协议类型 除非要复用内核既有Netlink协议类型，不然最好定义一个自己用的总线类型 1#define NETLINK_TEST 31 这个定义可以加在netlink.h中，或者放在模块的头文件里。 9.2.2 创建socket 在用户态，我们通过socket()接口来创建socket，而在内核中，我们使用如下的API： 12struct sock *netlink_kernel_create(struct net *net, int unit, struct netlink_kernel_cfg *cfg); net一般固定为全局变量init_net unit即为协议类型，我们在这里填上NETLINK_TEST cfg为Netlink的内核设置 123456789struct netlink_kernel_cfg &#123; unsigned int groups; unsigned int flags; void (*input)(struct sk_buff *skb); struct mutex *cb_mutex; int (*bind)(struct net *net, int group); void (*unbind)(struct net *net, int group); bool (*compare)(struct net *net, struct sock *sk);&#125;; 其中input是必须要设置的，是socket在接收到一个消息后的回调函数。回调函数的一个例子如下： 1234567891011121314151617181920212223242526272829303132333435static void hello_nl_recv_msg(struct sk_buff *skb)&#123; struct nlmsghdr *nlh; int pid; struct sk_buff *skb_out; int msg_size; char *msg = "Hello from kernel"; int res; printk(KERN_INFO "Entering: %s\n", __FUNCTION__); msg_size = strlen(msg); nlh = (struct nlmsghdr *)skb-&gt;data; printk(KERN_INFO "Netlink received msg payload:%s\n", (char *)nlmsg_data(nlh)); pid = nlh-&gt;nlmsg_pid; /*pid of sending process */ skb_out = nlmsg_new(msg_size, 0); if (!skb_out) &#123; printk(KERN_ERR "Failed to allocate new skb\n"); return; &#125; nlh = nlmsg_put(skb_out, 0, 0, NLMSG_DONE, msg_size, 0); NETLINK_CB(skb_out).dst_group = 0; /* not in mcast group */ strncpy(nlmsg_data(nlh), msg, msg_size); res = nlmsg_unicast(nl_sk, skb_out, pid); if (res &lt; 0) printk(KERN_INFO "Error while sending bak to user\n");&#125; 9.2.3 从内核向用户态程序发送消息 正如在用户空间的发送流程那样，发送消息需要先设置一个socket接收地址。设置接收地址需要通过NETLIN_CB宏访问skb从control buffer中存储的netlink参数（struct netlink_skb_parms）。 123456789struct netlink_skb_parms &#123; struct scm_creds creds; /* Skb credentials */ __u32 portid; __u32 dst_group; __u32 flags; struct sock *sk; bool nsid_is_set; int nsid;&#125;; 其中重要的参数时dst_group和flags。 如果要发送的数据包是单播数据包，发送方式为： 12NETLINK_CB(skb_out).dst_group = 0; /* not in mcast group */res = nlmsg_unicast(nl_sk, skb_out, pid); 这里的目标pid可以通过接收到的消息nlh-&gt;nlmsg_pid获取 如果要发送的数据包是多播： 1res = nlmsg_multicast(nl_sk, skbout, own_pid, group, flags); 此处的own_pid是传输自己的pid来纺织消息传递给自己。因此内核态在这里填写0 NETLNK_CB(skb_out).dst_group会在发送函数内设置。 10 Further Reading Kernel Korner - Why and How to Use Netlink Socket https://gist.github.com/arunk-s/c897bb9d75a6c98733d6]]></content>
      <categories>
        <category>kernel</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用iptables和route来建立起Linux的网关设置]]></title>
    <url>%2Fposts%2F32824%2F</url>
    <content type="text"><![CDATA[本文翻译自：Setting Up Gateway Using iptables and route on Linux。 网络资源的分享是非常重要的，而建立起一个网关来进行网络分享是一个比较好的解决方案。在Linux系统中创建和设置网关非常简单，成本低廉，而且性能可靠。 1 Linux网络设置 假定我们要处理的Linux有如下的配置： NIC1: eth0, ip: 192.168.0.1，连接到局域网(LAN) NIC2: eth1, ip: 1.2.3.4, 连接到公网 网络拓扑图 现在我们希望将分享这台机器的网络连接给LAN网络上的其他电脑(ip: 192.168.0.0/16) 2 设置网关 下面提到的所有操作都需要root权限来执行。 2.1 操作IP路由表 1234ip route add 192.168.0.0/16 dev eth0# or# route add -net 192.168.0.0/16 dev eth0 2.2 启用Linux IP 转发(IP Forwarding) 1234sysctl -w net.ipv4.ip.forward=1# or# echo 1 &gt; /proc/sys/net/ipv4/ip_forward 你也可以直接编辑/etc/sysctl.conf来持久化这一设置： 1net.ipv4.ip_forward = 1 2.3 通过iptables设置源地址映射(SNAT) 将（其他电脑发送的）包的源地址修改为网关的源地址。iptables会自动将响应包的目的地址替换成正确的IP地址。 1iptables -t nat -A POSTROUTING ! -d 192.168.0.0/16 -o eth1 -j SNAT --to-source 1.2.3.4 除了使用SNAT，也可以使用MASQUERADE: 1iptables -t nat -A POSTROUTING ! -d 192.168.0.0/16 -o eth1 -j MASQUERADE 注意，对于静态IP而言，SNAT的方式要更好一些。根据iptables man page: This target is only valid in the nat table, in the POSTROUTING chain. It should only be used with dynamically assigned IP (dialup) connections: if you have a static IP address, you should use the SNAT target. Masquerading is equivalent to specifying a mapping to the IP address of the interface the packet is going out, but also has the effect that connections are forgotten when the interface goes down. This is the correct behavior when the next dialup is unlikely to have the same interface address (and hence any established connections are lost anyway). 你还需要确保其他iptables不会阻拦对应的连接。如果你有这方面的问题，可以尝试： 123iptables -Fiptables -t nat -Fiptables -t nat -A POSTROUTING ! -d 192.168.0.0/16 -o eth1 -j SNAT --to-source 1.2.3.4 上面的代码可以允许所有的接入连接。不过这会存在一些安全性问题。 3 客户端配置 客户端配置主要是把网关设置成192.168.0.1。例如如下命令 1234ip route add default via 192.168.0.1 dev eth0# or# route add default gw 192.168.0.1 eth0]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>linux</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派上搭建视频流服务的方法尝试]]></title>
    <url>%2Fposts%2F28769%2F</url>
    <content type="text"><![CDATA[最近实验需要在树莓派上搭建一个简单的视频服务，而且，希望画质一定的情况下，消耗的带宽越少越好。关于带宽的问题，其实开始并没有考虑太多，但是在尝试用uv4l工具创建mpeg流的时候发现，尽管分辨率很低（720p）不到，需要的数据率却达到了大约5MB/s。我们待测试的通信层不具备这样高的传输传输能力。因此需要想办法把数据率降下来。综上，我们需要产生一个编码后的视频流，如H264。 幸运的是我发现了h264-live-player这个项目。这个项目是基于Node.js的工程，利用Websocket传输H264编码数据，在客户端用Broadway解码，而服务端的H264流通过raspivid产生。 在接下来的部分，我先简要介绍一下Raspivid的使用，然后介绍一下h264-live-player的情况。如果只是想上手使用，可以直接拉到最后。 Raspivid raspivid是一个在树莓派上用于捕捉视频数据的命令行工具。在h264-live-player中，lib/raspivid.js文件调用了这个命令来产生H264的视频流。在这个文件中使用的命令是： 1raspivid -t 0 -o - -w WIDTH -h HEIGHT -fps FPS 其中，-t 0表示捕捉的时间不限。-o -表示将H264流输出到stdout。后面的-w, -h, -fps则分别是制定画面的宽高还有帧率。在raspivid命令产生H264流后，h264-live-player会通过一系列的回调函数通过Websocket将H264数据发送给前端。 h264-live-player 关键代码解析。 注意，原作者的工程里面存在一些问题，其中重点是客户端刷新后视频流解析会出现异常。我在我的fork中修复了这些问题，还做了一些其他的改进。因此这里的介绍都以我的fork中的代码为准。 后端 首先还是要看lib/raspivid.js这个文件。RpiServer这个类继承于Server，Server中预留了get_feed给子类实现，器作用是产生视频流。 12345678910111213141516get_feed() &#123; if (this.streamer !== undefined) &#123; this.streamer.kill(); &#125; var msk = "raspivid -t 0 -o - -w %d -h %d -fps %d"; var cmd = util.format(msk, this.options.width, this.options.height, this.options.fps); console.log(cmd); var streamer = spawn('raspivid', ['-t', '0', '-o', '-', '-w', this.options.width, '-h', this.options.height, '-fps', this.options.fps, '-pf', 'baseline']); streamer.on("exit", function(code)&#123; if (code) &#123; console.log("Failure", code); &#125; &#125;); this.streamer = streamer; return streamer.stdout;&#125; 这个函数返回的是raspivid子进程的stdout流，也即H264流。 然后我们来看lib/_server.js文件中_Server的定义。注意start_feed这个函数： 12345678910start_feed() &#123; if (this.readStream) &#123; this.readStream.end(); &#125; var readStream = this.get_feed(); this.readStream = readStream; readStream = readStream.pipe(new Splitter(NALseparator)); readStream.on("data", this.broadcast);&#125; 这个函数在客户端发起播放流的请求后调用。这里Server调用子类实现的get_feed函数获取视频流，然后视频流上注册data事件的回调函数。 这里需要解释一下readStream = readStream.pipe(new Splitter(NALseparator));这行代码。这里我们为视频流增加了一个Splitter，生成Splitter的参数为一个Buffer。 1const NALseparator = new Buffer([0,0,0,1]);//NAL break 在H264规范中，帧中间的会插入00 00 00 01作为帧间隔标识。这里插入的Splitter的作用是，在每次遇到NALseperator形式的字符流时，将之前收到的数据作为一个chunk，调用data事件的回调函数。 再来看看broadcast函数。在视频流收到一定的函数时会调用这个函数： 12345678910111213141516broadcast(data) &#123; this.wss.clients.forEach(function(socket) &#123; if (socket.readyState !== WebSocket.OPEN) &#123; return; &#125; if(socket.buzy) return; socket.buzy = true; socket.buzy = false; socket.send(Buffer.concat([NALseparator, data]), &#123; binary: true&#125;, function ack(error) &#123; socket.buzy = false; &#125;); &#125;);&#125; 这里的代码非常简单，核心就是通过socket.send将数据发送给客户端。注意这里的数据的内容是Buffer.concat([NALseperator, data])。这是因为Splitter会截断分隔符。 前端 前端的代码集中在vendor/wsavc/index.js中。重点是下面这段代码： 12345678910111213141516171819202122232425262728293031323334var framesList = [];this.ws.onmessage = (evt) =&gt; &#123; if(typeof evt.data == "string") return this.cmd(JSON.parse(evt.data)); this.pktnum++; var frame = new Uint8Array(evt.data); //log("[Pkt " + this.pktnum + " (" + evt.data.byteLength + " bytes)]"); //this.decode(frame); framesList.push(frame);&#125;;var shiftFrame = function() &#123; if(!running) return; if(framesList.length &gt; 10) &#123; log("Dropping frames", framesList.length); framesList = []; &#125; var frame = framesList.shift(); if(frame) &#123; this.decode(frame); &#125; requestAnimationFrame(shiftFrame);&#125;.bind(this);shiftFrame(); 在接收到服务器发送的数据时，数据会被转换成Uint8Array，然后压入到一个队列中。而在shiftFrame这个函数会周期性的调用，从队列中取出数据进行解码。解码后会触发Broadway解码器的onPictureDecoded回调，在这个回调中canvas中的图像会被更新。 h264-live-player的部署和使用 安装Node.js到树莓派 SSH登录到树莓派，然后运行 12345sudo apt-get updatesudo apt-get dist-upgradecurl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -sudo apt-get install -y nodejs 使用下面的命令来验证安装成功： 1234$ node -vv8.14.1$ npm -v # npm是Node.js的包管理器6.4.1 安装h264-live-player 123456# 下载仓库git clone git@gitlab.vlionthu.com:tdma-uav/raspberry-pi-video-stream.git playercd player# 安装依赖npm install 运行 12cd playernode server_rpi.js 上面的运行方法会在terminal中启动服务脚本。如果要这个程序常驻后台，可以尝试使用pm2 12345678910sudo npm install -g pm2 # 安装pm2，这里的-g表示安装到全局环境下cd player # cd to player folder# 启动pm2 start ./server-rpi.js \ -i 1 \ --name "video-stream" \ -o "/home/pi/player/stdout.log" \ -e "/home/pi/player/stderr.log" 在网页端访问摄像头 1http://rasp_ip:8080 可以通过添加/?r的query参数来上下翻转画面。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>折腾</tag>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dependency Injection in Node.js | 2016]]></title>
    <url>%2Fposts%2F30333%2F</url>
    <content type="text"><![CDATA[在上一篇文章中我们初步讨论的Dependency Injection的一些理念。在这篇文章中，我翻译了awilix模块的作者Jeff Hansen的文章：Dependency Injection in Node.js - 2016 edition。原文包含三个部分，我在这里直接整理成为一篇完整的文章。 在翻译中我以传到核心思想为主，故不会太拘泥于一些细节问题。对于一些插科打诨的话，如果不是特别有意思的话，也许不会翻译。 作者Jeff Hansen Part I 在2015年，RisingStack写了一篇关于Dependency Injection(缩写为DI)的文章，解释了什么是DI，以及如何手动实现。如果你还没有阅读这篇文章，我强烈建议你先阅读以下那篇文章。这样你对于本文的一些概念会有更加清晰的理解。 这里提到的RisingStack的文章的中文版可以在我的博客里找到: Node.js | Dependency Injection。 在这一系列文章中，我会扩展一下手动实现的DI，为什么这种做法是糟糕的，以及我们如何最终能够让DI的现实变得优雅 -- 甚至比require/imports方式要更好。我将要证明Node中使用DI可以不像之前的做法那样沉闷。这都要归功于在ES6中引入的新特性：Proxies（直译就是代理）。 我100%肯定作为一个Node的开发者，你会见过某种形式的DI。借鉴一下RisingStack文章中的例子: 1234567var express = require('express')var app = express()var session = require('express-session')app.use(session(&#123; store: require('connect-session-knex')&#125;)) session needs a store! - 这种存储的具体实现方式是多样的 ：redis，MySQL。Express本身并不关心背后的实现。我们来看下面的这个例子 -- 非DI实现： 1234567import db from '../mydatabase'export default &#123; getToDos: () =&gt; &#123; return db.query('select * from todos') &#125;&#125; 在这个例子中我们直接导入了db模块，因此这个文件就依赖于db模块在磁盘上的具体存储位置，以及依赖于特定的是方式。在大多数场景下这并不算一个大问题。不过这种方式让测试变得更加困难 -- 不至于无法进行测试，但是无论如何都变得更加地困难了。另外，这个模块还假定db模块已经准备好了（例如：数据库连接已经建立起来了）。 如果我们进一步将上面的代码转化成为对于测试友好的DI实现方式： 1234567export default function makeTodosService (&#123; db &#125;) &#123; return &#123; getTodos: () =&gt; &#123; return db.query('select * from todos') &#125; &#125;&#125; 那么上面两个例子有什么区别呢？在下面的DI实现的例子中我们不是export出一个对象，而是export出一个生成这种对象的函数。这个函数同时阐明了为了创建此种对象所需要的依赖。 如果你熟悉在其他语言中的DI实现，如Java, C#，还有PHP。下面这个使用ES6的类实现的例子可能更受你喜欢一些： 12345678export default class TodosService &#123; constructor(&#123; db &#125;) &#123; this.db = db &#125; getTodos() &#123; return this.db.query('select * from todos') &#125;&#125; 不过从个人角度我还是更喜欢函数的方法：不用担心this的上下文的问题。 测试上面这个基于DI的例子非常简单 -- 你不再需要担心对require进行修修补补来替代数据库模块从而连接到测试数据库。 12345678910111213describe('Todo Service', function () &#123; beforeEach(() &#123; subject = makeTodosService(&#123; db: testDatabaseSomehow &#125;) &#125;) it('work', async function() &#123; const todos = await subject.getTodos( expect(todos.length).to.equal(3) ) &#125;)&#125;) Part II 在这个部分我们来构思一个Todo APP。 在我们开始折腾API框架和其他乱七八糟的部分之前，我们来大致搭建一下项目的骨架 -- the service and data access。为了可读性的考虑我在这里使用了ES7的async-await机制。 然我们来开始我们的Todos Service - 这个模块来负责处理所有的业务逻辑。 我会在下面的代码片段那种使用不同的风格（函数式或者是面向对象的）来证明，这些具体的代码风格并不本质，你可以使用任何你喜欢的方式。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// todosService.jsimport assert from 'assert'// Using object destructring to make it look goodexport function makeTodosService (&#123; // "repository" is a fancy term to describe an object // that is used to retrieve data from a datasource - the actual // data source does not matter. Could be a database, a REST API, // or some IoT things like sensors or what ever todosRepository, // We also want info about the user that is using the service, // so we can restrict access to only their own todos. currentUser&#125;) &#123; assert(todosRepositry, 'opts.todosRepository is required.') assert(currentUser, 'opts.currentUser is required.') return &#123; // Gets todos for the current user getTodos: async(query) =&gt; &#123; const todos = await todosRepository.find(&#123; // can be ALL, INCOMPLETED, COMPLETED filter: query.filter, userId: currentUser.id &#125;) return todos &#125;, createTodo: async (data) =&gt; &#123; const newTodo = await todosRepository.create(&#123; text: data.text, userId: currentUser.id, completed: false &#125;) return newTodo &#125;, updateTodo: async (todoId, data) =&gt; &#123; const todo = await todosRepository.get(todoId) // verify that we are allowed to modify this todo if (todo.userId !== currentUser.id) &#123; throw new Error('Forbidden') &#125; const updatedTodo = await todosRepository.update(todoId, &#123; text: data.text, completed: data.completed &#125;) return updatedTodo &#125;, deleteTodo: async (todoId) =&gt; &#123; const todo = await (todoId) const todo = await todosRepository.get(todoId); if (todo.userId !== currentUser.id) &#123; throw new Error('Forbidden') &#125; await todoRepository.delete(todoId) &#125; &#125;&#125; 代码有点长，但是并没有什么太fancy的东西。我们并没有依赖于外部库（除了自带的assert模块用于输入检验）。不过，我们导出的函数其实有两个依赖： todosRepository -- 给予todos数据库访问的对象（我们并不关心具体的实现细节）。 currentUser -- 正在使用这个服务的用户。注意我们并不知道这个对象从何处生成，也不关心这些细节。 我们继续往下走，给出todos repository的一个不错的实现方式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// todosRepository.js// Let's do an in-memory implementation for now.const _todos = []export default class TodosRepository &#123; // Making all methods async makes them return promises! async find(query) &#123; const filtered = _todos.filter((todo) =&gt; &#123; // Check the user id if (todo.userId !== query.userId) &#123; return false; &#125; // check the filter if (query.filter === "COMPLETED") &#123; return todo.completed === true &#125; if (query.filter === "INCOMPLETED") &#123; return todo.completed === false &#125; return true &#125;) return filtered &#125; async get(id) &#123; const todo = _todos.find(x =&gt; x.id === id) return todo &#125; async create(data) &#123; const newTodo = &#123; id: Date.now(), text: data.text, userId: data.userId, completed: data.completed &#125; _todos.push(newTodo) return newTodo &#125; async update(id, data) &#123; const todo = await this.get(id) Object.assign(todo, data) return todo &#125; async delete(id) &#123; const todo = await this.get(id) _todos.splice(todo, 1) &#125;&#125; 上面的代码只是todos repository的一个in-memory实现。任何时候我们准备好的时候，可以替换成MySQL，Rethink，MongoDB等存储后端，只要具有同形式的API就可以了。Typescript和Flow在这里可以发挥很大的作用。 把系统粘合起来 在我们进入到RESTful API之前，让我们先把上门两个模块在测试中整合起来。下面的方法被称为“穷人式的DI”，不过别担心，在后面我们会展示更加fancy的做法。 1234567891011121314151617181920212223242526272829303132333435363738394041import makeTodosService from './todosService'import TodosRepository from './todosRepository'describe('Todos System', function () &#123; it('works', async function() &#123; // This is how DI is done manually const todosService = makeTodosService(&#123; todosRepository: new TodosRepository(), // Let's fake it til we make it! currentUser: &#123; id: 123, name: 'Jeff' &#125; &#125;) // Todos Service already knows who's creating it! const created = await todosService.create(&#123; text: 'Write Medium article' &#125;) expect(created.userId).to.equal(123, 'user id should match currentUser') const todos = await todosService.getTodos(&#123; filter: 'ALL' &#125;) expect(todos.length).to.equal(1) await todosService.update(todo.id, &#123; completed: true &#125;) const incompleteTodos = await todosService.getTodos(&#123; filter: 'INCOMPETED' &#125;) expect(incompleteTodos.length).to.equal(0) const completedTodos = await todosService.getTodos&#123; filter: 'COMPLETED' &#125; expect(completedTodos.length).to.equal(1) &#125;)&#125;) 看到上面的代码你可能会想：“这里的代码不是已经知道了两个模块了么？”。没错，在一个真实的APP中（下文中我们会提及），还是需要有一个知道所有使用的模块的单一置信源（source of truth）。在我们倒腾DI黑科技的时候，我们把这个部分的代码称为：组合根（The Composition Root，译者按：这个名字放在中文下太绕口了）。这是在应用中将所有的模块胶合在一起的地方。Composition Root可能长这个样子： 12345678910111213141516cosnt currentUser = &#123; id: 123, name: 'Jeff'&#125;const todoRepository = new TodosRepository()const todosService = makeTodosService(&#123; todosRepository, currentUser&#125;)export default &#123; todosService, todosRepository&#125; 看到这个代码，我知道你一定在想：“我现在还不知道这个currentUser具体是指哪个用户呢！我要构建的是一个Web应用，这种方法根本没用！”。你说的对。有两种方法来手动解决这个问题： 为所有需要currentUser的方法手动传递这个参数 -- 这也太坑了。 将实例化过程推迟到你拥有了所有的数据之后（译者按：即在已知了currentUser之后再调用工厂函数初始化todosService）-- 这种方法也不好，你需要在很多的地方重复地进行实例化。 为了进一步解释以下第二点，下面给出一个例子。例子中使用到了Koa Router 123456789101112131415161718192021const router = new KoaRouter()router.get("/todos", async (ctx) =&gt; &#123; const todosService = makeTodosService(&#123; todosRepository: new TodosRepository(), currentUser: ctx.state.user &#125;) ctx.body = await todosService.getTodos(ctdx.request.query) ctx.status = 200&#125;)router.post("/todos". async (ctx) =&gt; &#123; const todosService = makeTodosService(&#123; todosRepository: new TodosRepository(), currentUser: ctx.state.user &#125;) // ...&#125;)// and so on 这还只是涉及到两个模块。想象一下要是需要处理10个模块（这还只是对于小型的应用）。没错，第二种方法也是很糟糕的。 Part III Angular曾经是在JavaScript世界中第一个引入了DI的大型框架。他们的做法是使用函数的字符串表达来提取使用的模块名称。在当时这是唯一的做法。 有一些人尝试将DI功能从Angular中独立出来做成一个独立模块。但是问题是，大多数DI模块要求你的所有代码都要围绕着特定的DI系统来开发，这位违背了DI设计理念的初衷。 DI的作用是减少程序模块之间的耦合程度，提高代码的可维护性。在这种目标下，DI系统的设计应当尽可能减少对于其它业务代码的影响。如果为了使用DI要对业务代码结构进行大范围的改动的话就得不偿失了。 我们希望能够在不改动我们的service和repository模块的情况下使用DI机制。 关于Awilix - The DI container you deservce 如果你不知道DI容器是什么，下面是一个简短的解释。DI容器的功能是将系统中的模块整合起来，从而让开发者不再需要太关注这些DI的实现细节问题。在前面两个Part中我们给出的示例代码：实例化services和repositories，确保service获取repository对象。这些工作都将由DI容器来完成。 Awilix就是这样的一个容器，其实现是基于ES6 Proxies，这一意味着不再需要对函数的参数进行字符串解析。 现在让我们回到开头的todo应用。让我们使用Awilix来将各个模块整合起来。我们将会使用Koa 2来实现Web API。先让我们来安装这些依赖： 1npm install -S koa@next koa-router@next awilix awilix-koa 这里的awilix-koa模块让Awlix和Koa的搭配更加易用。现在让我们从composition root开始 123456789101112131415161718192021// configureContainer.jsimport &#123; createContainer, asClass, asFunction &#125; from 'awilix'import makeTodosService from './todosService'import TodosRepository from './todosRepository'export default function configureContainer () &#123; const container = createContainer() // Ordering does not matter container.register(&#123; // Notice the scoped() at the end - this signals // Awilix that we gonna want a new instance per "scope" todosService: asFunction(makeTodosService).scoped(), // We only want a single instance of this for the apps // lifetime (it does not deal with user context) // so we can reuse it! todosRepository: asClass(TodosRepository).singliton() &#125;) return container&#125; 这看起来已经非常不错了。不过如果你有超过100个服务需要注册，Awilix提供了自动化的工具。 现在让我们来配置Koa应用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// server.jsimport Koa from 'koa'import KoaRouter from 'koa-router'import &#123; asValue &#125; from 'awilix'import &#123; scopePerRequest, makeInvoker &#125; from 'awilix-koa'import configureContainer from './configureContainer'const app = new Koa()const router = new KoaRouter()const container = configureContainer()// This installs a scoped container into our// context - we will use this to register our current userapp.use(scopePerRequest(container))// Let's do that now!app.use((ctx, next) =&gt; &#123; ctx.state.container.register(Value)(&#123; // Imagine some auth middleware somewhere... // This makes currentUser available to all services currentUser: ctx.state.user &#125;) return next()&#125;)// Now our handlers will be able to resolve a todos service// using DI!// P.S: be a good dev and use multiple files. ;)const todosAPI = (&#123; todosService &#125; =&gt; &#123; return &#123; getTodos: async (ctx) =&gt; &#123; const todos = await todosService.getTodos(ctx.request.query) ctx.body = todos ctx.status = 200 &#125;, createTodos: async (ctx) =&gt; &#123; const todo = await todosService.createTodo(ctx.request.body) ctx.body = todo ctx.status = 201 &#125;, updateTodo: async (ctx) =&gt; &#123; const updated = await todosService.updateTodo( ctx.params.id, ctx.request.body ) ctx.body = updated, ctx.status = 200 &#125;, deleteTodo: async (ctx) =&gt; &#123; await todosService.deleteTodo( ctx.params.id, ctx.request.body ) &#125; &#125;&#125;)// Awilix magic will run the above function// every time a request comes in, so we have// a set of scoped services per requestconst api = makeInvoker(todosAPI)router.get('/todos', api('getTodos'))router.post('/todos', api('createTodos'))router.patch('/todos/:id', api('updateTodo'))router.patch('/todos/:id', api('deleteTodo'))app.use(router.routes())app.listen(1337) 上面的代码还只是一个简单的雏形，不过你现在已经有了构建大规模项目的基础。 结论 DI是一个很有用的东西，不过手动去实现DI是一件糟心的事情。这也是Awilix这种DI容器扮演作用的地方。]]></content>
      <categories>
        <category>形而上</category>
      </categories>
      <tags>
        <tag>nodejs</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js | Dependency Injection]]></title>
    <url>%2Fposts%2F61013%2F</url>
    <content type="text"><![CDATA[Dependency Injection这个概念是我之前在实习的时候做Java开发的时候接触的。Dependency Injection可以大大降低模块之间的耦合度，提高系统的可扩展性和鲁棒性，不过这个概念对于新人来说理解起来还是存在比较大的障碍。由于当时实习的时间比较短，对于这个概念我并没有吃透。这次学习Node.js的时候，又在awilix这个库里面遇到了这个概念。以此为契机就来好好学习一些Dependency Injection和其后的设计逻辑与方法。 下面的内容翻译自：Dependency Injection in Node.js。这篇文章浅显地介绍了Dependency Injection的基本理念。选择这篇文章是因为我在阅读awilix模块作者关于Dependency Injection的系列文章中时，作者在开篇提议阅读此文。 不过这篇文章毕竟是2015年的文章，在js的一些语法和模块细节上和今时今日的有些不同，但是并不妨碍我们对于其核心理念的理解。 使用Dependency Injection的理由 解耦 (Decoupling) Dependency Injection使你的模块耦合度降低，从而提升代码的可维护性。 更简单的单元测试 比起需要硬编码的依赖关系，你可以将依赖关系传输进入你要用的模块。在大多数场合下使用这种范式你不必要使用proxyquire这样的模块。 这一段作者写的比较含糊。其实意思是在使用Dependency Injection场景下，我们在独立测试一些单元功能的时候，对于其他模块可以通过注入Mock对象，从而将待测试的模块独立出来进行测试。 更快速的开发 在使用了Dependency Injection的场景下，在接口定义好了以后，开发会更加容易，Merge conflict会更少。 如何在Node.js中使用Dependency Injection 下面我们来看看如何在不适用Dependency Injection的前提下开发应用，然后看看如何进行转化。 不使用Dependency Injection的例子 下面是一段简单的没有使用Dependency Injection的代码： 12345678// team.jsvar User = require('./user');function getTeam(teamId) &#123; return User.find(&#123;teamId: teamId&#125;);&#125;module.exports.getTeam = getTeam; 对应的测试可能是： 1234567891011121314151617// team.spec.jsvar Team = require('./team');var User = require('/user');describe('Team', function() &#123; it('#getTeam', function* () &#123; var users = [&#123;id: 1, id: 2&#125;]; this.sandbox.stub(User, find, function() &#123; return Promise.resolve(users); &#125;) var team = yield team.getTeam(); expect(team).to.eql(users); &#125;)&#125;) 在上面的代码中我们做的是创建了一个名为team.js的模块，该模块可以返回属于一个team的用户列表。为了实现这一功能，我们导入User模块，然后我们再调用其find方法返回用户列表。 看起来不错，是吗？但是当我们需要进行测试时，我们必须要使用sinon的test stubs. 在测试文件中，我们需要引入User模块，为其stub一个find方法。注意，我们在这里要使用sandbox功能，这样我们不需在测试完成后回复find的原函数。 注意：如果原始对象使用了Object.freeze，那么stubs将不会起作用。 使用Dependency Injection的例子 123456789101112// team.jsfunction Team(options) &#123; this.options = options;&#125;Team.prototype.getTeam = function(teamId) &#123; return this.options.User.find(&#123;teamId: teamId&#125;);&#125;function create(options) &#123; return new Team(options);&#125; 你可以使用下面的这个文件来进行测试 12345678910111213141516171819202122// team.spec.jsvar Team =- require('./team');describe('Team', function() &#123; it('#getTeam', function* () &#123; var users = [&#123;id: 1, id: 2&#125;]; var fakeUser = &#123; find: function() &#123; return Promise.resolve(users); &#125; &#125; var team = Team.create(&#123; User: fakeUser &#125;) var team = yield team.getTeam(); expect(team).to.eql(users); &#125;);&#125;); 那么，使用了Dependency Injection的版本同之前的版本有什么区别呢？首先你可能注意到的是这里使用了工厂模式：我们使用这种设计模式来将options/dependencies inject到新创建的对象中 - 这里是我们注入User模块的方法。 在测试文件中我们还需要创建一个fake model来代表User模块，然后将这个伪造的模块传递给工厂函数。很简单，不是吗？ Dependency Injection in Real Projects 你可以在非常多的开源项目中发现Dependency Injection的例子。例如，你在日常工作中常常用到的Express/Koa的大部分中间件都使用了这种技术。 Express Middlewares 1234567var express = require('express');var app = express();var session = require('express-session');app.use(session(&#123; store: require('connect-session-knex');&#125;)) 上面的代码片段使用了基于工厂模式的Dependency Injection：对应session中间件我们传递了一个connect-session-knex模块。这个模块需要实现session模块调用需要的借口。 在这个例子中，connect-session-knex模块需要实现下面的方法： store.destroy(sid, callback) store.get(sid, callback) store.set(sid, session, callback) Hapi plugins Dependency Injection的概念还可以在Hapi中找到。下面的例子中，handlebars模块被作为view engine注入给Hapi使用: 1234567server.views(&#123; engines: &#123; html: require('handlebars`) &#125;, relativeTo: __dirname, path: 'templates'&#125;)]]></content>
      <categories>
        <category>形而上</category>
      </categories>
      <tags>
        <tag>nodejs</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab|安装-迁移-删除]]></title>
    <url>%2Fposts%2F25664%2F</url>
    <content type="text"><![CDATA[1 安装 1.1 Omnibus package installation 这是Gitlab官网推荐的安装方式。官网文档链接位于Gitlab Installation。不过，现在直接去官网默认给出的是企业版，即gitlab-ee的安装方式（付费的），而个人版其实用gitlab-ce就够了。gitlab-ce安装方式如下 1.1.1 安装并配置依赖 1sudo apt-get install -y curl openssh-server ca-certificates 然后安装Postfix来启动邮件提醒功能。（如果你使用了第三方的邮件服务，可以跳过这一步并且参照配置外部SMTP服务器）。 1sudo apt-get install -y postfix 在接下来的配置过程中，选择'Internet Site'选项。使用你的服务器的域名来作为'mail name'。如果还有后续的选项，输入Enter直至安装完成。 1.1.2 安装Gitlab-EE 添加Gitlab Package仓库： 1curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash 注意这里安装的是CE版本，故是gitlab-ce，企业版对应的是gitlab-ee 接下来安装Gitlab： 1sudo EXTERNAL_URL="http://gitlab.example.com" apt-get install gitlab-ce 这里的EXTERNAL_URL是你的Gitlab服务要使用的域名。如果你只使用http，或者后续要使用已有的Nginx，可以在这里使用http。如果使用https，gitlab会调用Let's encrtpy的服务为你的网站添加ssl证书。 1.1.3 登录Gtilab 进入你在安装阶段的域名，你会被重定向到密码重置界面。在这个页面你要设置管理员账户的密码，然后回到登录界面。在这个登录界面，使用root用户名和上一步设置的密码登录。 1.2 使用已有的Nginx 这个章节我们参考官方文档给出使用已有的Nginx的方法。 1.2.1 禁用Gitlab自带的Nginx 编辑/etc/gitlab/gitlab.rb文件，设置 1nginx['enable'] = false 1.2.2 设置外部服务器的用户 这一步是为了保证外部服务器用户能够访问gitlab。使用Nginx时，可以通过/etc/nginx/nginx.conf文件查看到nginx用户。一般情况下这个用户名是www-data。修改/etc/gitlab/gitlab.rb： 1web_server['external_users'] = ['www-data'] 然后使用sudo gitlab-ctl reconfigure来使得更改生效。 1.2.3 Trusted proxies 如果你的反向代理服务器和gitlab不是在同一台机器上，那么你还需要设置Trusted proxies。 1gitlab_rails['trusted_proxies'] = ['192.168.1.0/24', '192.168.2.1', '2001:0db8::/32'] 1.2.4 Nginx示例配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# gitlab socket 文件地址upstream gitlab &#123; # 7.x 版本在此位置 # server unix:/var/opt/gitlab/gitlab-rails/tmp/sockets/gitlab.socket; # 8.0 位置 server unix:/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket;&#125;server &#123; listen *:80; server_name gitlab.example.com; # 请修改为你的域名 server_tokens off; # don't show the version number, a security best practice root /opt/gitlab/embedded/service/gitlab-rails/public; # Increase this if you want to upload large attachments # Or if you want to accept large git objects over http client_max_body_size 250m; # individual nginx logs for this gitlab vhost access_log /var/log/gitlab/nginx/gitlab_access.log; error_log /var/log/gitlab/nginx/gitlab_error.log; location / &#123; # serve static files from defined root folder;. # @gitlab is a named location for the upstream fallback, see below try_files $uri $uri/index.html $uri.html @gitlab; &#125; # if a file, which is not found in the root folder is requested, # then the proxy pass the request to the upsteam (gitlab unicorn) location @gitlab &#123; # If you use https make sure you disable gzip compression # to be safe against BREACH attack proxy_read_timeout 300; # Some requests take more than 30 seconds. proxy_connect_timeout 300; # Some requests take more than 30 seconds. proxy_redirect off; proxy_set_header X-Forwarded-Proto https; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Frame-Options SAMEORIGIN; proxy_pass http://gitlab; &#125; # Enable gzip compression as per rails guide: http://guides.rubyonrails.org/asset_pipeline.html#gzip-compression # WARNING: If you are using relative urls do remove the block below # See config/application.rb under "Relative url support" for the list of # other files that need to be changed for relative url support location ~ ^/(assets)/ &#123; root /opt/gitlab/embedded/service/gitlab-rails/public; # gzip_static on; # to serve pre-gzipped version expires max; add_header Cache-Control public; &#125; # error_page 502 /502.html;&#125; 2 迁移 2.1 备份 迁移首先要做的是备份。在git学习------&gt; Gitlab如何进行备份恢复与迁移？这篇文章中详细讲述了备份的问题。我们这里介绍的是最为直接和简单的步骤。如果要更加详细的信息请阅读这篇参考。 备份使用如下命令： 1gitlab-rake gitlab:backup:create 备份会生成在/var/opt/gitlab/backups目录下。名称类似于1502357536_2017_08_10_9.4.3_gitlab_backup.tar。下面这些配置信息，没有包含在backup文件里面。需要手动迁移。 /etc/gitlab/gitlab.rb 配置文件须备份 /var/opt/gitlab/nginx/conf nginx配置文件 /etc/postfix/main.cfpostfix 邮件配置备份 备份命令的执行 2.2 在目标机器上安装gitlab 迁移过程中要求源机器和目标机器上安装的gitlab版本是相同的。如果不同，其实最好的做法是先将源机器上的gitlab升级到最新的版本。然后再生成备份。 如何查看Gitlab版本 2.3 上传备份 使用scp命令将备份文件上传到目标机器的/var/opt/gitlab/backups。 如果scp上传目标文件文件夹的权限不够，可以先上传到自己的home目录下，然后ssh登录到服务器使用sudo进行移动。 2.4 应用备份文件 首先为了避免潜在的权限问题，将备份文件的权限设置为777 1chmod 777 1502357536_2017_08_10_9.4.3_gitlab_backup.tar 然后停止gitlab的相关数据连接服务 12gitlab-ctl stop unicorngitlab-ctl stop sidekiq 然后用下面的命令读取备份： 1gitlab-rake gitlab:backup:restore BACKUP=1502357536_2017_08_10_9.4.3 在后续出现的所有询问中输入yes，等待执行完毕，即完成了迁移过程，接下来再次启动gitlab 1sudo gitlab-ctl start 3 删除 下面的删除过程在Ubuntu 16上得到验证： 3.1 移除gitlab服务 1sudo gitlab-ctl uninstall 3.2 清楚Gitlab产生的数据 1sudo gitlab-ctl cleanse 3.3 删除Gitlab生成的系统账户 1sudo gitlab-ctl remove-accounts 3.4 删除gitlab 1sudo dpkg -P gitlab-ce 3.5 其他文件的删除 除了上述操作，Gitlab使用的其他文件夹还需要手动删除，包括： /opt/gitlab: 包含了Gitlab的应用代码和依赖 /var/opt/gitlab: 包含了应用的数据和配置信息(gitlab-ctl reconfigure的写入内容) /etc/gitlab: omnibus gitlab的配置信息。这里的文件是唯一允许你手动编辑的部分 /var/log/gitlab: 日志文件 在你完成了开始的四个步骤后，这里的四个文件夹可以安全地手动删除。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>gitlab</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP:拥堵控制]]></title>
    <url>%2Fposts%2F60823%2F</url>
    <content type="text"><![CDATA[网络数据包如果一次发送太多，就会造成网络拥堵；如果发送太少，就浪费了带宽，延长了通信时间。TCP 协议有一个拥堵窗口机制，负责动态调整每次发送数据包的数量。本文通俗地解释了这种算法的细节。 本文翻译自Intro to Congestion Control 这个夏天，我一直在思考更好地解决网络拥塞问题的方法。在这篇文章中，我将会讨论为什么网络拥塞问题会出现，以及一些传统的解决办法。如果你有更深厚的兴趣，这个Juptier notebook包含了我用来获取相应结果的代码，以及对这些结果的分析。 什么是TCP 在我们开始正文讨论之前，我来先简要介绍一些信息在网络上流通的细节。 TCP协议被用来将信息从一台电脑经过英特网传输给另一台电脑。这个协议也是这篇文章所关注的协议。把TCP协议同其他协议（如UDP）区分开来的特征是，TCP确保了100%的传输成功率。也就是说如果你从一台电脑上发送了100kb的数据，那么你会在接收端准确地收到这100kb的数据。 TCP的这个特性非常强大，而这一特性也是很多网络应用采用TCP协议的原因。现有的Web应用和Email都是构筑于TCP协议之上。 TCP实现所有数据的可靠传输的核心原理是，对于从A端发送到B端的数据，B端会发送回一个ACK(Acknowlegement)信息给A端来告知自己收到了对应的信息。 TCP传输 另外还值得注意的是，TCP工作在IP协议之上，IP协议最多允许在一个包中包含1500个字节的数据，因此要发送100kb的数据，需要拆分成多个分段。根据TCP协议，每个分段都会收到对应的ACK。 如果发送者没有收到一个数据分段的ACK，其会重新发送这个分段。 什么时候会还产生拥塞 拥塞（Congestion）问题是由于网络传输延时导致的。信息传输速率会收到物理信道，如以太网线，蜂窝网络等，的制约。在因特网中，大部分独立设备都连接到这些信道上。 下图是一个典型场景： 拥塞产生的场景 在上面的示意图中，两个发送者要各自要传输1GB的数据。然而这两个发送者最终接入到了一个1GB的链接中。第二个链接的传输能力无法匹配上前两个链接的输入，故而不得不丢弃一部分数据包。如果发送者不主动调整自己的发送速度，那么会产生非常坏的情况。在TCP协议中，如果发送者发现一个数据分段没有送达，会重新发送这个数据包。那么拥塞情况会持续，两个发送者会无法完成发送过程。 为了让两个发送者能够成功传输各自的数据，他们需要共同减少发送数据的速率。如果只有一个发送者减少了发送的数据，而另一个仍然维持1GB的发送量，那么仍然会产生拥塞。在因特网的架构中，不会有一个中央控制系统来协调两者的发送速率。 迂回：什么是链接(link)？ 在我们深入到这个问题的解决方案前，我还想进一步讨论链接（link）的属性。关于网络链接，有下面三个重要的细节问题你需要知道： 延时（毫秒）：一个包从链接的一端发送到另一端需要的时间 带宽（mb/s）：链接每秒能够通过的比特数 队列：在链接正在工作时，等候发送包的队列的长度，以及在队列满时管理队列的策略 如果把链接比喻成水管，那么延时可以理解为管道的长度，带宽就是水管的周长。 链接模型 关于链接还有一个重要的统计参数时带宽延时积（bandwidth-delay product, BDP）。这个参数表现了体现了停留在链接中的数据量，可以理解为管道本身的容量。当链接中传输的数据量达到了BDP时，就可以说链接被充分利用了。如果发送端尝试发送比BDP更多的数据，那么链接的队列将会填满，并最终开始丢包。 方法 在给出方法之前，我们要思考一个问题：发送者如何知道产生了拥塞呢？如同我们之前提到的，因特网是一个分布式的系统，故而并没有一个位于中央的协调者来在下游链接产生拥塞的时候提醒发送者要减慢发送速度。 主要有两个指标：丢包率和传输往返时间。在拥塞发生时，链接的队列逐渐填满，开始发生丢包。如果一个发送者注意到了丢包现象，这就很可能意味着发生了丢包。另一个队列满负荷的现象是数据包在队列中等待的时间增加了，这会导致传输往返时间，即发送包到收到ACK的时间增加。 今天的一些拥塞控制机制考虑了上面两个指标，不过在一些比较早期的设计中，只使用到了丢包率这一个指标。 还需要注意的是，发送者可能并不提前知道传输链接的特性参数。例如，如果你访问&quot;http://www.google.com&quot;，那么你发送的数据包可能要经过很多不同性质的链接才能到达Google的服务器，而你的传输速率是收到其中最慢的链接的制约的。 因此，出了规避网络拥塞的能力，拥塞控制机制还需要能够探索可用带宽的具体大小。 拥塞窗口（Congestion Window） 理解任何拥塞控制机制的关键在于理解拥塞窗口的概念。拥塞窗口指的是在收到一个ACK前发送者能发送的包的数量。如果一个发送者的拥塞窗口被设置为2，这意味着在发送了两个包之后，它必须等到接收端回复的ACK之后才能继续发送。 拥塞窗口越大，发送者就能在相通的时间间隔内向接收端发送更多的数据包。为了更加直观的理解，假设网络传输的延时是88ms，拥塞窗口设置为10，那么在一轮往返传输（88 * 2 = 176ms）时间内可以发送10个数据包。而如果拥塞窗口设置为20，则相同的时间间隔内可以发送20个数据包。 不过当然，提升拥塞窗口的大小，也会提高发生拥塞的概率。拥塞控制算法的目标，就是计算出合适的拥塞窗口大小。 从理论角度来看，拥塞窗口的大小应当就是链接的BDP。 TCP Tahoe TCP Tahoe是在80年代设计出来的拥塞控制算法。那是拥塞问题才刚刚在因特网上出现。算法本身非常简单。增加拥塞窗口分为两个阶段： 第一阶段： Slow Start：算法的开始状态是Slow Start。在这个阶段拥塞窗口在每收到一个ACK就增加1。这种机制有效地在每轮往返传输成功后，将拥塞窗口的大小翻倍。如果拥塞窗口的大小是4，那么在同时会有4个包在传输路途中。当每个包的ACK返回时，拥塞窗口加一，即当这四个包的ACK都收到后，拥塞窗口会翻倍成为8。这个过程会一直持续到拥塞窗口达到阈值，ssthresh。 第二阶段：Congestion Avoidance：当拥塞窗口达到阈值ssthresh时，进入Congestion Avoidance阶段。在这一阶段，每轮往返传输后拥塞窗口加一。也就是说，在上面的例子中，当所有4个包的ACK收到后，拥塞窗口只会加1. 在这个阶段拥塞窗口的大小会大大减小。 当Tahoe检测到丢包后，会把ssthresh设置为当前拥塞窗口的一半，然后将拥塞窗口设置为1，算法重新回到Slow Start阶段。 丢包检测与快速重传 TCP发送端有两种方法来检测丢包现象： 发送端超时。发送端会给每个发送出去的数据包设置一个超时。如果在超时时限达到时尚未收到该包的ACK，则认为发生丢包，并重传改数据包，将拥塞窗口设置为1. 接受者发送回重复的ACK。在TCP中，接收端只会接收按照顺序发送的包。如果收到了不合顺序的包，接收端会返回他收到的最后一个符合顺序的包的ACK。例如，接收端收到了1，2，3，其后又收到了包5，那么接收端会再次回复3的ACK。在Tahoe中，如果发送端检测到重复的ACK，就意味着发生了丢包。这种机制被称为快速重传(Fast Retransmit)，因为这种机制不一定要等待到传输超时。 一些思考 在开头提到的Jupitor Notebook中，我实现了Tahoe，下图是拥塞窗口随着时间变化的曲线： Tahoe的拥塞窗口曲线 注意到上图的中变化曲线存在锯齿形的行为。开始的突增为Slow-Start阶段，后面的平缓部分为Congestion Avoidance阶段。急遽掉落到1则是由于丢包导致的。 为什么Tahoe要如此工作？ Tahoe在工作过程中不断增加拥塞门限的原因是因为网络条件会随着时间不断变化。例如如果另一个发送者开始在同一个信道上发送数据，这会导致可用带宽的降低，其他的发送者需要按照实际情况调整。相反，如果有一个发送者停止发送数据了，可用带宽会增加，这也需要其他发送者根据实际情况来调整。 这种方法其实还是存在很多问题，这也是Tahoe目前已经基本没人使用了。特别的，Tahoe需要很长的时间，尤其是在高带宽网络上，才能全面有效地利用可用带宽。这是因为在拥塞窗口增长到Slow Start门限以后，其增长就变得非常缓慢了。 另外一个问题是，发生丢包并不一定意味着网络发生了拥塞，例如Wifi信道下，本身信道就是可能发送丢失的。对于丢包产生剧烈的将拥塞窗口砍到1并不总是合适的做法。 最后一个问题是，Tahoe使用丢包这个因子来作为判断是否发生丢包的依据。然而由于拥塞发生了丢包，此时调整拥塞窗口已经太晚了。 其他的方法 80年代以后，涌现了不少新的算法来解决上面这些问题。我会在将来的文章中详细讨论这些方法： CUBIC：这个算法在2005年实现，目前是Linux系统的默认拥塞控制算法。如同Tahoe，这个CUBIC也是用丢包作为判断拥塞是否发生的依据。不同的是，CUBIC在高带宽网络下的性能要远高于Tahoe。不同于Tahoe在每一轮往返传输后将拥塞窗口增加1的做法，CUBIC如同其名，使用一个立方函数来确定窗口大小，从而实现拥塞窗口的快速增长。 BBR(Bufferbloat)：这是最近才被Google提出的新的算法。不同于CUBIC和Tahoe，这个算法使用延时来作为判断拥塞是否发生的标识。这背后的思路是延时是拥塞在导致丢包前就能起作用的判断因子。在实际丢包发生前就开始减少发送速率能够带来更高的吞吐率。 公平性 在研究拥塞控制算法时，一个有意思的问题是考虑不同的算法对于同一网络链接上的各个发送者是否公平。如果一个算法在发生拥塞时，没有缩减发送规模，而是按照之前相同的速率继续发送，那么这个算法就是不公平的。在这个结果中，如果同一个链接上 有一个发送者没有采用拥塞窗口控制，而另一个发送者使用Tahoe。从结果可以看到，在一分钟的时间内，Tahoe发送者几乎没法发送任何数据，因为它没有机会增加它的拥塞窗口。而固定窗口的发送者全占了发送信道。 尽管固定窗口发送者是一个不好的情形，这种算法可能具有对其他的算法的不公平地位，从而占据更多带宽。由于缺乏中央控制这，可能有贪婪的发送者蓄意采用固定窗口来谋取更大的带宽。这就是需要从博弈论的角度来研究拥塞控制算法了。 结论 拥塞控制算法是互联网的基础，同时也是在有限信息条件下进行分布式决策的一种迷人的实践。]]></content>
      <categories>
        <category>形而上</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>网络</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Make|自动生成依赖关系]]></title>
    <url>%2Fposts%2F56042%2F</url>
    <content type="text"><![CDATA[Make一般是在Unix环境下使用的自动化编译工具。他本身不是编译器，而是将众多C/C++源文件组织起来，确定其编译方式和编译顺序的工具。一旦我们写好的Makefile配置文件，那么无论多么复杂的工程我们都可以用一条make命令来解决。事实上，尽管通常和C/C++搭配起来使用，make也能应用到其他的编程语言之中。 在使用make过程中的第一个核心问题是处理文件依赖的问题。例如： 12foo.o : foo.c defs.h # foo模块 cc -c -g foo.c 这里foo.o依赖于foo.c和defs.h。当后面两个文件发生变化时，make会自动运行cc -c -g foo.c命令更新foo.o文件。但是，随着项目扩大。这种文件之间的依赖关系会变得非常复杂，一个小的改动可能会涉及到众多依赖关系的修改。因此有必要在项目的开始就引入自动构建依赖关系的工具链。 在跟我一起写Makefile:书写规则这篇教程中，提到了编译器的一个特性：大多数的C/C++编译器都支持一个&quot;-M&quot;的选项，即自动寻找源文件中包含的头文件，并生成一个依赖关系。例如如果我们执行 1cc -M main.c 其输出是： 1main.o: main.c defs.h 注意如果你用的是GNU的C/C++编译器，你得用&quot;-MM&quot;参数，不然，&quot;-M&quot;参数会把一些标准库头文件也引入进来。 这篇教程里面详细阐述了如果在Makefile中使用这一特性的方法，综合而来就是： 123456# 对于每个.c源文件，建立一个描述其依赖关系的.d依赖文件%.d: %.c @set -e; rm -f $@; \ $(CC) -M $(CPPFLAGS) $&lt; &gt; $@.$$$$; \ sed &apos;s,\($*\)\.o[ :]*,\1.o $@ : ,g&apos; &lt; $@.$$$$ &gt; $@; \ rm -f $@.$$$$ 上述命令中sed命令的作用是在依赖关系对中，在左侧加上.d文件本身。即 将 12&gt; main.o: main.c defs.h&gt; 转换成 12&gt; main.o main.d : main.c defs.h&gt; 然后将生成的依赖关系文件include进来 12sources = foo.c bar.cinclude $(sources:.c=.d) 在教程中还提到，这个include要放在默认目标之后，避免include载入的文件的目标替换了默认目标。 走完上面的流程，会得到一个类似的如下内容的文件： 1234567891011121314151617%.d: %.c @set -e; rm -f $@; \ $(CC) -M $(CPPFLAGS) $&lt; &gt; $@.$$$$; \ sed &apos;s,\($*\)\.o[ :]*,\1.o $@ : ,g&apos; &lt; $@.$$$$ &gt; $@; \ rm -f $@.$$$$sources = main.c foo.c bar.cobjs = $(sources:.c=.o)include $(sources:.c=.d)main: $(objs) $(CC) -o main $(objs).PHONY : cleanclean: @rm -f *.d *.o @rm -f ./main 不过按照这个Makefile第一次执行的时候会产生一个问题：第一次执行时，.d文件尚未生成，这里的include导入的文件不存在，会产生如下的错误信息 12Makefile:8: main.d: No such file or directormake: *** No rule to make target 'main.d'. Stop. 最后是通过面向google的debug找到了Autodependencies with GNU make这篇2001年的文章，细致地阐述了这个问题。解决的关键在于在include前面添加一个dash（-），其作用是：如果include的对象不存在，make继续执行，后续make会自动生成.d文件，然后执行include。这篇新的教程提供的完整Makefile示例如下（和前面的形式有不同，但是思路是一致的）： 1234567891011121314151617OBJS := foo.o bar.o# linkproggie: $(OBJS) gcc $(OBJS) -o proggie# pull in dependency info for *existing* .o files-include $(OBJS:.o=.d)# compile and generate dependency info%.o: %.c gcc -c $(CFLAGS) $*.c -o $*.o gcc -MM $(CFLAGS) $*.c &gt; $*.d# remove compilation productsclean: rm -f proggie *.o *.d]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于无神论者的笑话两则]]></title>
    <url>%2Fposts%2F39480%2F</url>
    <content type="text"><![CDATA[## 一 一个坚定的无神论者去世了，突然发现自己在一个昏暗的过道里。过道里有一个牌子，上写“通向地狱”。没办法，那就进地狱吧。他开门进去，几乎不敢相信自己的眼睛：阳光明媚，暖风宜人，白沙滩，棕榈树，每一百米一个酒吧，到处都是欢快的人们。他沿着沙滩漫步，突然发现一个长着马脚和尾巴的家伙坐在一个沙滩躺椅里。他走上前去问，你是魔鬼吗？魔鬼回答说是，并热烈欢迎新人到地狱。不久，想了解一下地狱的无神论者，两个沙包之间看到一个很大很深的坑，便好奇地往里看，结果吓坏了：坑底烧着熊熊大火，到处是哭天喊地的人，撒了疯的怪物披头盖脸地往人身上打。 无神论者疑惑地跑回魔鬼身边，痛心地问：后边沙包那里那个坑是怎么回事？魔鬼说：噢，他们哪，都是基督徒。他们非要这样，我也没什么办法…… 二 一个忠诚的共产党员死了，上帝不愿意在天堂接受无神论者的灵魂，于是把他送到地狱。一个月后，魔鬼大汗淋漓跑来说“你赶紧把那人带走吧，他差不多把我所有小鬼都发展成了少先队员！” 上帝就接受了。 又过了一月，魔鬼幸灾乐祸地问上帝“那共产党员怎样了？”上帝说：“首先请叫我同志” 非常惭愧，只讲了两个微小的笑话，谢谢大家 来源：https://www.zhihu.com/question/27030419/answer/121040045 续 宗教逻辑]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
        <tag>宗教</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[酵素]]></title>
    <url>%2Fposts%2F25658%2F</url>
    <content type="text"><![CDATA[今天在朋友们的群里又看到有朋友在谈论吃酵素的事情。这让我想到了2017年二月，我去东京交流访问，参观了日本最大的酵素生产商之一：中原株式会社。有意思的是，这家公司虽然是日本的公司，却是中国人创立的。之所以公司的名字叫做中原，是因为创始人是郑州人。当时接待我们的人中，有一个负责做产品研发的生物博士，筑波大学毕业，也是中国人。他带我们参观了公司总部顶楼的一个小型的检测间。有一个随行的朋友很实诚地问道：“酵素这个东西到底有没有用。”那名生物学博士倒也没直接回答，而是笑着说：”大家都是学工科的，都懂“。 酵素这个东西，其实就是酶的另一种说法。吃酵素的风气，也是从日本舶来的。不过在日本那边，酵素是作为”保健食品的“，因此，在酵素包装上面，是不能声称任何疗效的。日本的酵素从业者，不得不利用各种渠道在宣传刊物上宣传酵素成分的一些益处（还不能直接说产品），然后在包装上注明这些成分，以此来吸引消费者购买。不过在中国，法规不是这么健全，因此中国的酵素商家，宣传起酵素功效来，宛如过去街头卖大力丸一般，怎么牛逼怎么来。 某厂商的酵素宣传 其实，酵素就是酶，也就是蛋白质，进入到肠胃，也都被分解成氨基酸，和鸡蛋，肉类无异。故，吃酵素还不如吃鸡蛋，同等营养的情况下，鸡蛋更便宜。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
        <tag>智商税</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS绕过SIP安全机制限制的一种办法]]></title>
    <url>%2Fposts%2F36948%2F</url>
    <content type="text"><![CDATA[SIP(System Security Protection)是苹果在OSX EI Capitan及其后版本的操作系统中引入了一种新的安全机制。望文生义就可以看出，这个安全机制是用来维持系统的完整性，保护系统免收恶意软件的篡改。具体来说，SIP限制了root账户的权限范围，限制了root用户在对一些系统保护目录即其中文件的操作能力。 SIP的保护范围包括下列路径： /System /usr /bin /sbin OSX的预装应用 第三方应用可以继续操作的目录包括： /Applications /Library /usr/local 但是任何对于安全性加强都意味着对灵活性的削弱。例如，在SIP保护下，类似proxychains-ng的程序无法再给受保护的目录下的程序添加网络钩子(hook)。 proxychains ng (new generation) - a preloader which hooks calls to sockets in dynamically linked programs and redirects it through one or more socks/http proxies. 一般来说，很多解决方案都建议关闭SIP功能（例如proxychains-ng的issue中给出的方法：# issue78）。不过这样也意味着丧失了SIP提供的保护功能。这篇文章给出了一个妥协的做法。在保留SIP的保护的同时，为保护目录下的程序应用proxychains-ng（其他类似的应用场景也可以使用这个办法）。这个解决方案的思路其实很简单：既然保护目录下的程序我们不能动，那么我们把保护目录下的程序复制一份到其他目录下运行就可以。 首先创建一个新的文件夹： 1mkdir ~/.unprotected_apps 然后将这个路径添加到PATH环境变量的头部： 12# 可以添加到shell的配置文件中，如~/.bashrc或者~/.zshrcexport PATH="~/.unprotected_apps:$PATH" 然后将需要添加钩子的应用复制到这个目录下就可以了，例如： 12cp $(which ssh) ~/usr/bin/sshcp $(which curl) ~/usr/bin/curl]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>macOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shadowsocks上手配置]]></title>
    <url>%2Fposts%2F37347%2F</url>
    <content type="text"><![CDATA[Shadowsocks配置的一个非常便利之处在于，Shadowsocks支持将配置信息导出成二维码再在其他机器上导入。这节约了很多沟通成本。所以在开始这篇教程之前，你需要有一个Shadowsocks的配置信息。可以是具体参数，或者是一个配置二维码。 1. 客户端准备 Shadowsocks提供了绝大多数平台的客户端支持，甚至包括智能路由器。我们这里介绍最为常见桌面端的平台上的配置。 这里我提供了mac和win这两个主要平台截止到目前为止最新版的客户端下载： macOS客户端下载; win客户端下载. 其中，mac文件下载下来解压缩后，直接拖拽进入Application文件夹（应用文件夹），然后双击打开使用就可以了。win端的文件解压缩后是一个可以直接运行的绿色版（不需要安装）。将解压缩文件移动到一个稳妥的位置，然后双击打开Shadowsocks.exe文件就可以了（此时右下角会出现一个小飞机图标） 更加丰富的客户端下载：https://shadowsocks.org/en/download/clients.html 2. 导入配置 写这篇文章的时候我使用的是mac，因此后面的配置方法过程都以mac为例。mac和win上客户端的使用都是相通的。不同的是小飞机图标在mac中位于顶部，而在win中位于底部。 ShadowsocksX-NG右键菜单截图 右键点击小飞机图标可以看到如上图所示的菜单。其中 第一个section中，负责控制Shadowsock的开启和关闭，我这里显示的是已经开启了Shadowsocks，如果你的客户端代理还没有启动，点击一下&quot;打开 Shadowsocks&quot; 第二个section中，可以设置Shadowsocks的代理模式。其中PAC模式是最为常用模式。在这种模式下，Shadowsocks会根据一张预先订好的表，来判断你当前访问的网址是否被墙了。如果是就会通过代理访问这个网站，否则照常直接连接网站就可以了。与之相对的，全局模式是让所有的网站都通过代理进行访问。 第三个section中，可以进行服务器的配置。 如果你是使用二维码进行配置，那么，将二维码用预览打开，确保这个预览窗口位于最上层可见，然后点击菜单中的“扫描屏幕上的二维码”就可以导入服务器配置了。 如果你是使用详细配置信息进行配置，那么需要进入服务器 -&gt; 服务器设置，手动填写各个参数进行添加。 第四个section是用来配置本地代理和PAC的，对于这部分的详细讨论超出了这篇文章的范畴，我们会在后续的文章中进行讨论。 3. 手机端配置 由于政策原因，手机端APP，尤其是iOS的手机端APP的审查情况非常严重，基本上很少有APP能够长期屹立不倒。因此手机端APP的选择要实时来看。我自己使用的SuperWingy这个应用已经下架了（不过从已购里面还是可以下载的）。因此，大家发现还有什么可以用的手机端应用，就更新在评论里把。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源对象存储服务(OSS) Minio 及其在Hexo中的使用]]></title>
    <url>%2Fposts%2F5440%2F</url>
    <content type="text"><![CDATA[研究对象存储服务(OSS)是因为考虑到将来可能会有在博客上放出一些可供分享的文件下载的服务需求，直接使用现有图床，容易混杂乱。因此我考虑重新建立一个独立OSS存储服务。直接Google搜到了Minio这个框架，10k+的Star，就决定选择这个了。Minio框架有如下几个优势： 可以Docker部署，非常省事 文档完善 全面的平台支持 多种客户端语言支持（有完善的JS SDK） ## 1. Minio部署 使用Docker部署可以说是非常方便省事了。我的部署命令如下： 1234567docker create -p 9000:9000 \-e "MINIO_ACCESS_KEY=your-access-key" \-e "MINIO_SECRET_KEY=your-secret-key" \--name=minio \-v /path/to/minio/data:/data \-v /path/to/minio/config:/root/.minio \minio/minio server /data 其中的访问秘钥对需要替换成你自己设置的值。这一对值稍后会用于网页端的登录。然后用 1docker container start minio 来启动镜像。完成后就可以在http://domain.com:9000中访问到了，输入docker命令中的秘钥对来登录。 登录界面 而后你可以按照Lychee图床教程中的做法，添加Nginx反向代理和HTTPS支持。 2. Hexo中使用 部署完成后我才发现一个问题，那就是Minio生成的外链是强制有过期时间的，而且长度最多只七天。那我就不能像直接复制粘贴外链来使用了，同时，手动来每七天更新一次链接也是不可接受的。因此用Hexo脚本来自动实现了利用Minio的API接口来更新下载链接。脚本内容如下： 1234567891011121314151617181920212223'use strict';const Minio = require('minio');var hexo = hexo || &#123;&#125;;var fs = fs || require('fs');var yaml = yaml || require('js-yaml');var minio_client = minio_client || new Minio.Client(yaml.safeLoad(fs.readFileSync(__dirname + "/minio_key.yml", 'utf8')));hexo.extend.tag.register('minio', async (args, content) =&gt; &#123; var bucket = 'default', resource_name = ''; if (args.length == 1) &#123; resource_name = args[0]; &#125; else &#123; resource_name = args[1]; bucket = args[0]; &#125; var file_url = await minio_client.presignedGetObject(bucket, resource_name); return `&lt;a target="_blank" href="$&#123;file_url&#125;"&gt;$&#123;content&#125;&lt;/a&gt;`;&#125;, &#123;async: true, ends:true&#125;); 在博客工程的根目录下创建一个文件夹scripts,在其中创建一个js文件，如index.js，然后将上述脚本内容粘贴进去。然后在这个目录下创建设置文件，minio_key.yml，文件中需要包含如下信息： 1234endPoint: 'minio.domain.com'accessKey: 'your-access-key'secretKey: 'your-secret-key'useSSL: true # 是否使用https 然后还需要安装依赖 1npm install --save minio 至此我们完成了脚本的安装。脚本为我们提供了一个标签插件，其使用范例如下： 123&#123;% minio 'bucket_name' 'resource_name' %&#125;下载链接&#123;% endminio %&#125; 在使用Hexo进行静态页面渲染时，这部分内容会被自动渲染成下载链接： 1&lt;a target=&quot;_blank&quot; href=&quot;download_url&quot;&gt;下载链接&lt;/a&gt; 不过这种方法还是有一个显而易见的缺点：你需要是一个非常勤奋的作者，每周都来发布一次文章，不然旧文章的链接还是会失效。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Hexo</tag>
        <tag>Minio</tag>
        <tag>OSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shadowsocks持续优化]]></title>
    <url>%2Fposts%2F35429%2F</url>
    <content type="text"><![CDATA[## Fast TCP开启 如果双端都支持FastTCP，那么可以通过开启FastTCP来降低延时。服务端设置方法有两种，要门在config.json中添加fast_open为true，要么在执行ssserver带上--fast-open。然后在命令行中运行 1echo 3 &gt; /proc/sys/net/ipv4/tcp_fastopen 进一步优化 这个优化方法适合所有的shadowsocks版本，具体方法如下。创建文件/etc/sysctl.d/local.conf，并在文件中添加如下内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# max open filesfs.file-max = 51200# max read buffernet.core.rmem_max = 67108864# max write buffernet.core.wmem_max = 67108864# default read buffernet.core.rmem_default = 65536# default write buffernet.core.wmem_default = 65536# max processor input queuenet.core.netdev_max_backlog = 4096# max backlognet.core.somaxconn = 4096# resist SYN flood attacksnet.ipv4.tcp_syncookies = 1# reuse timewait sockets when safenet.ipv4.tcp_tw_reuse = 1# turn off fast timewait sockets recyclingnet.ipv4.tcp_tw_recycle = 0# short FIN timeoutnet.ipv4.tcp_fin_timeout = 30# short keepalive timenet.ipv4.tcp_keepalive_time = 1200# outbound port rangenet.ipv4.ip_local_port_range = 10000 65000# max SYN backlognet.ipv4.tcp_max_syn_backlog = 4096# max timewait sockets held by system simultaneouslynet.ipv4.tcp_max_tw_buckets = 5000# turn on TCP Fast Open on both client and server sidenet.ipv4.tcp_fastopen = 3# TCP receive buffernet.ipv4.tcp_rmem = 4096 87380 67108864# TCP write buffernet.ipv4.tcp_wmem = 4096 65536 67108864# turn on path MTU discoverynet.ipv4.tcp_mtu_probing = 1# for high-latency networknet.ipv4.tcp_congestion_control = hybla# for low-latency network, use cubic instead# net.ipv4.tcp_congestion_control = cubic 然后运行 1sysctl --system 应用上述设置。最后在启动脚本中，于ssserver前添加 12ulimit -n 51200 这个设置方法，会消耗比较多的内存，但是会换来速度的大幅上升。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shadowsocks：多用户账号独立，并限制用户连接数]]></title>
    <url>%2Fposts%2F7835%2F</url>
    <content type="text"><![CDATA[自己搭建了一个SS服务器以后，自然而然的会同身边的朋友共享。自然，身边的朋友一起用，大部分服务器配置都可以毫无压力的支撑。但倘若一传十十传百，最后成百上千的人一起用一个服务器，那就撑不住了。 当然你可以隔一段时间换一次密码，但是后面的麻烦事也不少（要同步更新不同设备上的设置，身边的朋友来问你新设置）。 几天我研究了一下，为ss服务器增加了多用户即为每个用户设置独立的连接数限制的方法，这样能够比较完美的解决同朋友共享服务器的问题了。 这里默认你已经知道怎么按照通常的方法安装和配置SS了。如果你不了解的话，网络上的文章很多的。 1. 多用户的实现 多用户的实现比较简单，Python和Go实现的服务器自带多用户支持。通常的配置我们一般是这么写 123456&#123; "server": "::", "server_port": "8888", "password": "yourpassword" // Other configs&#125; 只需要将配置文件按照下面的方式进行修改就可以实现多用户了。 123456789&#123; "server": "::", "port_password": &#123; "8881": "password1", "8882": "password2", "8883": "password3" &#125; // other configs&#125; 就可以了。之后不同的用户可以通过不同的端口访问，而每个端口都有独立的密码。 Further Reading: Reference 2. 限制用户连接 我在网上调查了一下实现限制用户连接的方法，很多都提到了通过iptables来进行设置。但是这种方法太过复杂，很容易出问题。后来我找到一个ss的补丁，可以比较好的解决这个问题。补丁地址是falssen/PySocket。 这个工程提供了一些其他的功能，但是我们这里只关注Limit_Clients文件夹下的socket.py这个文件。这个文件的原理是利用Python包导入的机制，用自定义的socket.py来替换默认的socket包，并在socket接口中植入一些新的功能。 按照READMe.md的提示安装好socket.py文件 &gt; 有很多朋友不知道这里要怎么处理socket.py文件。其实并不复杂。用which命令查看一下ss脚本安装的位置，一般情况下是/usr/local/bin/，那么你只需要把socket.py文件放到/usr/local/bin下面就行。这一操作的原理是，python在导入包时总是先检查当前目录。注意，如果修改了socket.py文件，需要重启进程才能生效。 然后修改文件中white_list和black_list两个变量。例如我自己使用的1017端口，我不希望添加限制，则将white_list设置为 1white_list = [1017] 我给朋友们用的是[1018]端口，我希望这个端口的连接数不要超过40个，则将black_list设置为 1black_list = &#123;1018:40&#125; 3. 注意 注意方法的实质是限制接入的客户端IP数量，因此，处在同一路由器下面的多台设备也会被识别为一台。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自建图床: Lychee]]></title>
    <url>%2Fposts%2F65048%2F</url>
    <content type="text"><![CDATA[之前用的图床sm.ms的图片突然挂了。不知道为何，图片还是能够上传，但是访问图片的链接会出ERR_SPDY_PROTOCOL_ERROR的错误。 ERR_SPDY_PROTOCOL_ERROR错误示意图 正好我自己的翻墙服务器的硬盘长期富余。虽然只有十几个G，搭建一个自己图床还是够用的。更重要的是，Dogital Ocean的服务器的流量非常多（我买的$5的服务器的流量有一个T）。我选择的开源图床框架是Lychee。这个框架支持Docker安装，可以省很多事情。 1. Docker安装Lychee 常规的安装方法可以参考官方文档。我这里只介绍Docker方式。如果你没有什么特别的需求，Docker方式应该是非常适合你的。 注：这篇教程只是对于我的操作过程的一个记录，因此对于一些依赖环境的安装没有面面俱到。这些问题，都可以面向google进行解决。 1.1 Docker环境准备 首先你要安装一个Docker环境。在Ubuntu上，安装非常简单： 12$ sudo apt update$ sudo apt install docker-io 有时还需要将你当前用户加入到docker组中，这样每次执行docker命令不需要加sudo了。这个操作可能在安装过程中自动完成了，如果你发现docker命令执行时提示有权限相关的问题，可以运行 1$ sudo adduser user docker 注意确保一下docker-compose也安装完毕了。我们需要通过docker-compose来将Lychee和数据库组装在一起。 12$ docker-compose -vdocker-compose version 1.17.1, build 6d101fb 1.2 安装Lychee 首先创建好目录树： 12345lychee|-- config|-- db|-- pictures|-- docker-compose.yml 其中，config和pictures分别用来存储Lychee的设置和图片文件。db文件夹则是用于数据库，这三个文件夹需要你手动创建。docker-compose.yml文件内容如下： 123456789101112131415161718192021version: '1'services: lychee: image: linuxserver/lychee links: - lychee-db:lychee-db volumes: - /path/to/lychee/config:/config - /path/to/lychee/pictures:/pictures ports: - 8000:80 lychee-db: image: mariadb:10 volumes: - /path/to/lychee/db:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=&lt;choose root password&gt; - MYSQL_DATABASE=&lt;db name&gt; - MYSQL_USER=&lt;username&gt; - MYSQL_PASSWORD=&lt;username&gt; 目前我没发现lychee的这个镜像支持用环境变量来配置数据库信息。所以上面对应的数据库信息后续需要在网页端手动输入。 然后在这个文件夹下运行 1$ docker-compose up -d 然后访问http://yourdoman.com:8000就可以访问了。 1.3 Lychee配置 在访问上述网页之后，Lychee会提示我们输入数据库信息。 Lychee 配置 注意这里的Database Host要填写lychee-db。其他的设置与上面的docker-compose.yml文件中的一致即可。 而后按照提示创建登录账户： 创建账户 2. Lychee Advanced 2.1 使用Nginx进行反向代理 Nginx配置文件如下： 12345678910111213server &#123; server_name imgs.codewoody.com; client_max_body_size 50M; location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:8000; &#125;&#125; 2.2 启用HTTPS 目前来看，Let's encrypt仍然是个人建站启用HTTPS的不二之选。其使用教程可以说是非常简明了，具体参考certbot。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>折腾</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这个博客是如何建立起来的]]></title>
    <url>%2Fposts%2F53793%2F</url>
    <content type="text"><![CDATA[在博客问题上我可是折腾了很多回了，先是尝试了wordpress（来来回回很多次），不过wordpress使用起来，感觉还是太“重”，很多东西配置起来非常麻烦(包括主题设置，甚至是Markdown支持)。后来迁移到简书上面，被国家政策教做人(一篇关于Shadowsocks的文章被屏蔽了，有种吃苍蝇的感觉)。思前想后，还是自己host自己的博客好。最终我是选择了Hexo + Github的方案，好处如下： 对Markdown支持比较好 不需要自己折腾服务器 用Git管理非常方便 在这篇文章里，我整理一下整个博客的搭建过程。 1. Hexo Setup Hexo是一款基于Node.js的静态博客框架，可以生成静态页面部署在Github和Heroku上面。Hexo的搭建过程如下： 申请域名 创建Github仓库 安装Hexo及其依赖 绑定域名 1.1 申请域名 虽然部署在Github上Github会提供一个免费的域名，但是如果有自己的独立域名的话，网站会更像&quot;博客&quot;一点。申请域名的地方有很多，我的域名是选用的阿里云的。传送门：阿里云-为了无法计算的价值。 1.2 创建Github仓库 在Github中创建一个名字为username.github.io的仓库，注意这里的username需要替换为你自己的用户名。例如我的仓库名字为huangy10.github.io。 &gt; 你可以尝试在这个仓库中添加一个名为index.html的文件，在其中接入hello world。然后访问http://username.github.io 就可以看到这个页面了。 &gt; 不过注意尝试之后删除这个仓库重新创建。后面我们在部署Hexo的时候最好让这个仓库是空的。 &gt; 1.3 安装Hexo及其依赖 1.3.1 安装Git，并配置好SSH秘钥 这里Github有全面的教程，传送门：https://try.github.io/ 1.3.2 安装Node.js Mac平台下面安装Node.js非常简单，可以通过Homebrew进行安装: 1brew install node 如果没有安装Homebrew，可以在Terminal中输入下面这个命令快速安装： /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 安装完成后可以通过node -v来验证安装是否成功，并查看安装版本。进一步通npm -v来检查npm也正确安装了。 1.3.3 安装Hexo 使用npm来安装Hexo： 1npm install -g hexo-cli 完成安装以后，挑选一个合适的路径，然后运行 1hexo init blog 这个命令会在当前文件夹中创建一个名为blog的文件夹。博客相关的文件都会存储在这个文件夹中。cd进入这个文件夹，然后运行 1234# 生成静态文件hexo g# 在本地运行一个测试服务器来伺服静态文件hexo s 然后在浏览器中访问http://localhost:4000 就可以访问自己的网站了。 博客初始页面 我们来看一下Hexo博客项目下的目录结构： Hexo目录结构 其中比较重要的是： _config.yml是整个项目的配置文件，YAML格式； public是发布的静态文件内容。注意这个文件会在hexo g命令后重新生成，其中内容会被重置； source是工程源文件，其中的_posts文件夹存储了博文的Markdown文件。其中的其他文件，则会在hexo g命令的作用下发布到public文件夹中； themes存储了博客的主题。在各个主题自己内部也有自己的_config.yml文件，用来定制化模板的参数。 1.4 Hexo部署 我们选择将Hexo部署到Github上。打开博客项目根目录下的_config.yml文件，跳到最后，修改 1234deploy: type: git repo: git-repo-path(ssh方式，不要用https) branch: master(不出意外就填写master) 保存退出。 然后我们需要安装一个git部署的工具: 1npm install hexo-deployer-git --save 然后运行 123hexo cleanhexo ghexo d 三个命令，就可以逐步完成清理之前的生成，重新生成静态文件，将静态文件部署到Github上。全部完成后访问username.github.io 。就可以看到站点了。 2. Hexo Advanced 2.1 自定义域名 使用github提供的免费域名还是不够fancy，我还是希望使用自己的域名。首先进入域名管理后台，添加两条记录。分别是 yourdomain.com 添加一条A记录，指向username.github.io对应的ip地址。（这个ip地址可以通过ping命令看到） www.yourdomain.com 添加一条CNAME记录，指向username.github.io 然后在本地博客工程中的public文件夹下，添加一个CNAME文件，文件中写入自定义的域名www.yourdomain.com。重新三连： 1hexo clean; hexo g; hexo d 这是输入https://www.yourdomain.com就可以访问自己的网站了（可能需要等一段时间让dns刷新） 2.2 更换主题 自己搭建博客的乐趣之一就是各种更换主题。Hexo有自己的主题市场：Themes。我选择的主题是laughing。这个主题比较简洁，而且支持响应式布局。不过，这个主题支持的多说这个评论平台已经关闭了。其安装过程如下（其他的主题的安装方式大同小异）： 首先安装主题依赖的pug模板引擎: 1npm install hexo-renderer-pug --save 然后将主题文件夹下载到themes目录： 12cd themesgit clone git@github.com:BoizZ/hexo-theme-laughing.git 最后修改博客项目根目录下的_config.yml文件： 1theme: hexo-theme-laughing 主题的配置方式可以参考主题的Github文档。需要注意的是，文档中所说的_config.yml文件是指的主题文件夹中的配置文件，而非博客项目根目录下的配置文件。 2.3 插件 Hexo提供了很多插件来增强博客的功能。这个部分我也正在研究。这里我列出一下目前我安装了的插件： hexo-addlink: 在文章末尾中添加本文的链接 hexo-generator-feed: 生成rss订阅 hexo-generator-sitemap: 生成站点地图]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2FREAMDE.html</url>
    <content type="text"><![CDATA[Woody Huang的个人博客 https://www.codewoody.com]]></content>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[知识库🛠]]></title>
    <url>%2Fknowledge-base%2Findex.html</url>
    <content type="text"><![CDATA[我希望用我的博客能够整理出一些呈体系话的知识库体系。这个页面是这个知识库的入口。以下是目录。注意因为知识还在不断的完善中，所以这些页面也是经常地处于WIP状态。这里的文章除了我自己撰写/翻译的内容外，还会转载一些优秀的博文。 Downloading Resources 下载资源汇总 Programming How to install 这里给出了多种软件的安装方法 python Academic 智能交通系统 History 中国近现代史部分的史料 中国中古代部分的史料 Science]]></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[博客更新]]></title>
    <url>%2Fupdate%2Findex.html</url>
    <content type="text"><![CDATA[2019.05.15 修复了搜索和Feed的问题：文章中存在不可见字符，导致atom.xml和search.xml的格式出错 替换了字体服务器的CDN，从//fonts.googleapis.com修改为//fonts.css.network 20190507 2 修改了Reference的样式。方法是在themes/next/source/css/_custom/custom.styl文件中添加如下内容： 12345678div#refs &#123; font-size: 13px; line-height: 1.1;&#125;div#refs p &#123; margin: 0;&#125; 1 通过上标提供短的参考信息的方法： 1&lt;sup title="Hover Text"&gt;?&lt;/sup&gt;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fknowledge-base%2Facademic%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[历史]]></title>
    <url>%2Fknowledge-base%2Fhistory%2Findex.html</url>
    <content type="text"><![CDATA[虽然这个部分的题目时历史，但是显然我一己之力无法写个世界通史出来。这一部分的文章的目的，在于梳理，汇总一些对中外历史上一些经常被拿出来讨论的点做一些探究。因此与其说是“历史”，其实不如说是历史梗集。]]></content>
  </entry>
  <entry>
    <title><![CDATA[科学]]></title>
    <url>%2Fknowledge-base%2Fscience%2Findex.html</url>
    <content type="text"><![CDATA[这里我们汇总一些好玩的科普知识内容，也许会涉及一些非常硬核的内容。]]></content>
  </entry>
  <entry>
    <title><![CDATA[下载资源]]></title>
    <url>%2Fknowledge-base%2Fresources%2Findex.html</url>
    <content type="text"><![CDATA[开发 iOS/Mac相关 Command Line Tools: 需要登录Apple Developer账户 书本]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何安装Python]]></title>
    <url>%2Fknowledge-base%2Fprogramming%2Fhow-to-install%2Fpython.html</url>
    <content type="text"><![CDATA[Ubuntu 安装Python python3.7 以下安装脚本在Ubuntu 16.04上测试通过，引用自How to Install Python 3.7 on Ubuntu 18.04 123456789101112sudo apt updatesudo apt install software-properties-commonsudo add-apt-repository ppa:deadsnakes/ppa# When prompted press Enter to continue:sudo apt install python3.7python3.7 -v# 这里安装后只有python3.7的命令可以用，为了使用python命令，使用如下命令sudo ln -s $(which python3.7) /usr/bin/python 在sudo add-apt-repository ppa:deadsnakes/ppa命令中可能出现/etc/apt/sources.list文件无法访问问题。如果你改用了国内镜像，其内容包含了中文的注释，那么这个错误可能是由于locale设置导致的。通过export LC_ALL=&quot;zh_CN.UTF-8&quot;可以消除这个问题。 注意，可以在安装pip的同时一并安装python3。详情见下面。 安装pip pip for python3.6 为Python3安装pip的方法如下： 12345$ sudo apt update$ sudo apt install python3-pip$ pip3 --versionpip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) 注意，按照这种方法会连通python3一起安装。如果你用上面的方法安装了python3.7，那么在运行上面的脚本以后会同时存在3.7和3.6两个版本的python. pip for python3.7 经过实验，比较保险可靠的是如下的方法： 首先按照前面的方法安装好Python3.7 执行下面的命令： 1curl -s -L https://bootstrap.pypa.io/get-pip.py | sudo python3.7 如果不需要在全局安装python3.7的pip工具，只是在虚拟环境中使用的话，直接使用python3.7创建虚拟环境就可以了，虚拟环境中的pip会自动安装。虚拟环境的创建方法范例见下： 1virtualenv -p $(which python3.7) env 注意这个方法安装的pip会强制覆盖已有的pip pip for python2 为python2安装pip的方法如下： 123sudo apt updatesudo apt install python-pippip --version 同样，上面的脚本也会自动安装python2及其他必要的依赖。 pip的国内镜像配置方法 我这里使用清华大学开源软件镜像站提供的资源。设置方法如下： 临时使用 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 注意，simple不能少, 是https而不是http 设置为默认 升级 pip 到最新的版本 (&gt;=10.0.0) 后进行配置： 12pip install pip -Upip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 如果您到 pip 默认源的网络连接较差，临时使用本镜像站来升级 pip： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U Reference How to Install pip for python 3.7 on Ubuntu 18? get-pip.py]]></content>
  </entry>
  <entry>
    <title><![CDATA[智能交通系统🚗]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Findex.html</url>
    <content type="text"><![CDATA[智能交通系统是将先进的信息技术、通讯技术、传感技术、控制技术及计算机技术等有效率地集成运用于整个交通运输管理体系，而创建起的一种在大范围内及全方位发挥作用的，实时、准确及高效率的综合的运输和管理系统。美国、日本、欧洲率先展开相应的研究并成为ITS发展的三强，此外加拿大、中国、韩国、新加坡、澳大利亚等国家的研究也具有相当规模。 现有的智能交通系统通信标准 WAVE(Wireless Access in Vehicular Environments) 研究话题 编队系统]]></content>
  </entry>
  <entry>
    <title><![CDATA[apt使用和配置的一些信息]]></title>
    <url>%2Fknowledge-base%2Fprogramming%2Fhow-to-install%2Fapt.html</url>
    <content type="text"><![CDATA[apt国内镜像 我一般都是使用清华大学开源软件镜像站提供的资源，对于Ubuntu16.04，设置方法如下： 修改文件/etc/apt/sources.list，不过之前最好将系统自带的该文件备份。新的文件的内容为： 16.0418.0414.04测试 ``` # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse 预发布软件源，不建议启用 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse ```undefinedundefined]]></content>
  </entry>
  <entry>
    <title><![CDATA[How to Install]]></title>
    <url>%2Fknowledge-base%2Fprogramming%2Fhow-to-install%2Findex.html</url>
    <content type="text"><![CDATA[How to Install这个章节整理了常用软件的安装方法。]]></content>
  </entry>
  <entry>
    <title><![CDATA[中国近现代史部分的史料]]></title>
    <url>%2Fknowledge-base%2Fhistory%2F%E4%B8%AD%E5%9B%BD%E5%8F%B2%2F%E8%BF%91%E4%BB%A3%E5%8F%B2%2Findex.html</url>
    <content type="text"><![CDATA[中国近代史比较复杂，对于近代史认识，受到当前政治格局的影响很大，自然也有很多谬误。这里整理的材料，力图尽量为每一个史实论断提供多角度的论证依据。 以下按照时间大致分类一下。有一些纲领总括的内容单独列出。 按时间分类 晚清 北洋 民国（抗战前） 原始史料集 九一八事变前蒋张关于东北问题的讨论 民国（抗战） 民国（解放战争时期） 一共（1978年以前） 二共（1978年以后） 工程类 长江改道工程 淠史杭自流灌溉工程 中国近代史的一些有用的书籍史料 大陆方面 台湾方面 欧美方面 日韩及其他]]></content>
  </entry>
  <entry>
    <title><![CDATA[中国中古史]]></title>
    <url>%2Fknowledge-base%2Fhistory%2F%E4%B8%AD%E5%9B%BD%E5%8F%B2%2F%E4%B8%AD%E5%8F%A4%E5%8F%B2%2Findex.html</url>
    <content type="text"><![CDATA[这里说的中古史其实不是历史学家们用的专门的概念。我这里用来指自秦朝统一后至1840年两千年时间内的历史。 秦汉 三国两晋南北朝 隋唐 五代 宋辽金 元 明 清]]></content>
  </entry>
  <entry>
    <title><![CDATA[1996年朝鲜潜艇渗透事件]]></title>
    <url>%2Fknowledge-base%2Fhistory%2F%E6%9C%9D%E9%9F%A9%2F%E4%BA%8C%E6%88%98%E5%90%8E%E6%9C%9D%E9%B2%9C%2F1996%E5%B9%B4%E6%9C%9D%E9%B2%9C%E6%BD%9C%E8%89%87%E6%B8%97%E9%80%8F%E4%BA%8B%E4%BB%B6.html</url>
    <content type="text"><![CDATA[江陵潜艇渗透事件（朝鲜语：강릉지역 무장공비 침투사건／江陵地域 武裝共匪 侵透事件）是指于1996年9月朝鲜人民军特种部队为收集韩国情报，而使用鲨鱼级潜艇渗透至韩国东海岸的江陵市附近。由于渗透用的潜艇搁浅，参与行动的26名朝鲜士兵不得不弃艇，并从附近的海滩躲入附近的山林中隐藏。大韩民国国军和警察随即对他们展开时长两个多月的追捕，26名朝鲜人中仅2人存活，而韩国共有16人丧生（包括军警和百姓）、27人受伤。朝鲜的此次渗透行动虽以失败而告终，但也使外人得以一见朝鲜特种部队的训练水平和作战能力，但此次行动又使因粮食援助而渐走向缓和的朝韩关系再次走进紧张局势。 事件中搁浅的鲨鱼级潜艇 韩国军队登上朝鲜间谍潜艇 韩军组织兵力开始追捕朝鲜潜艇上的特工人员 9月18日约17时，韩国士兵到达潜艇搁浅处西南8公里的一座330米高的山顶空地上，发现11名朝鲜特工的尸体。 朝鲜11名特工自杀现场，其中的10具尸体肩并肩，排列成一条直线，而另外一具尸体（海军部金东源上校）在不远处的另一边，上校的手枪还放在枪套里。这些身亡的朝鲜人都穿便装和白网球鞋。 被俘虏朝鲜间谍潜艇舵手李光素 进一步阅读： 朝韩秘密战：96年朝鲜特种部队对韩国秘密潜入 维基百科：江陵潜艇渗透事件]]></content>
  </entry>
  <entry>
    <title><![CDATA[Wireless Access in Vehicular Environments (WAVE)]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fwave%2Findex.html</url>
    <content type="text"><![CDATA[WAVE的全称是Wireless Access in Vehicular Environment，是目前车联网通信的标准。 WAVE协议内容及特点 WAVE标准中的Beacon性能简化分析 IEEE802.11 DCF中Basic Access和RTS/CTS机制的理论饱和吞吐率性能差异分析 WAVE中的SCH调度机制研究状况 通信范围模型探讨 ns3中的wave模块]]></content>
  </entry>
  <entry>
    <title><![CDATA[车辆编队体系研究]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fplatoon%2Findex.html</url>
    <content type="text"><![CDATA[前言 车辆编队(platoon)是指多辆车，借助于车载传感器，车载计算机，车间通信系统等手段排成列或者更复杂的结构。编队系统可以减少车间距，提高道路承载能力。另一方面，基于空气动力学的特点，编队内车辆的阻力(aerodynamic drag)能够降低，从而降低能耗[1][2]。 本文的主体框架基于[3]。 编队的优势 以编队方式形式有如下优势： 编队内的车间距更小，故而可以提升道路承载能力(road capacity)，缓解交通拥堵。 由于空气阻力降低，行驶的能耗也得到了降低，汽车排放量也减少了。 在先进技术的辅助下，编队形式也能提升交通安全，提升驾驶/乘坐的舒适性、 由于编队内的相对位置固定，车辆间的协作通信应用的效率能够改进，提升车联网的性能。 Platoon as VCPS 这里的VCPS是指Vehicular Cyber-Physical System，CPS一般译为“赛博物理系统”，这个概念着重强调计算机及网络系统，与物理世界中的传感器和促动器(Acturator)的互动与联合。汽车编队可以视为是一种赛博物理系统。 Platoon as VCPS 从VCPS的角度看待编队系统，可以发现汽车编队的运动性能(mobility)和网络性能(VANETs)是彼此耦合，互相影响的。 文章条目 多编队系统 基于Platoon的车间通信(V2V)机制设计 Reference [1] M. Amoozadeh, H. Deng, C.-N. Chuah, H. M. Zhang, and D. Ghosal, “Platoon management with cooperative adaptive cruise control enabled by vanet,” Vehicular Communications, vol. 2, no. 2, pp. 110–123, 2015. [2] Han-Shue Tan, R. Rajamani, and Wei-Bin Zhang, “Demonstration of an automated highway platoon system,” in Proceedings of the 1998 american control conference. ACC (ieee cat. No.98CH36207), 1998, vol. 3, pp. 1823–1827 vol.3. [3] D. Jia, K. Lu, J. Wang, X. Zhang, and X. Shen, “A survey on platoon-based vehicular cyber-physical systems,” IEEE communications surveys &amp; tutorials, vol. 18, no. 1, pp. 263–284, 2016.]]></content>
  </entry>
  <entry>
    <title><![CDATA[多编队系统]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fplatoon%2Fmulti-platoon.html</url>
    <content type="text"><![CDATA[综述 这里说的多编队系统是指研究道路上多个相邻的编队系统组成的系统。编队之间的互动，以及如何协调编队间互动与编队内互动是这一系统的主要挑战。这里的互动可以指通信层面，运动层面，或者是对二者进行联合设计。 Reference]]></content>
  </entry>
  <entry>
    <title><![CDATA[IEEE802.11 DCF中Basic Access和RTS/CTS机制的理论饱和吞吐率性能差异分析]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fwave%2Fperformance-of-bas-and-rts.html</url>
    <content type="text"><![CDATA[在Wireless Access in Vehicular Environments (WAVE)文章的3.4章节我们梳理了对IEEE802.11 DCF机制的性能推导。通过推导的结论我们发现使用了RTS/CTS机制之后，相比于Basic Access手段，饱和吞吐率的性能有了大幅度的改进，如下图： 饱和吞吐率与节点数量的关系 注意到在3.4.4中给出的一般形式的饱和吞吐率对于Basic Access以及RTS/CTS机制是一样。两种机制的主要区别在于\(T_c\)和\(T_s\)的区别，如下图所示： \(T_c\)和\(T_s\) 考虑到ACK, RTS, CTS这些包的体积非常小，因此Basic Access和RTS/CTS机制的区别在于\(T_s\)和\(T_c\)的相对大小不同。在Basic Access中两个值非常接近，而在RTS/CTS中两个值的差别非常大。 \[ S=\frac{P_sP_{tr}E[P]}{(1-P_{tr})\sigma+P_{tr}P_sT_s+P_{tr}(1-P_s)T_c} \] 上面的式子中，\(P_{tr}P_s=n\tau (1-\tau)^{n-1}\)随着\(n\)增加而递减。\(P_{tr}\)随着\(n\)增加而趋向于1，则分母项中，随着\(n\)升高，前两项都趋向于0，后一个趋向于\(T_c\)。可见\(T_c\)的值决定了最后下降的速度。 分母的形式其实很类似于两级lerp function，即从形状上来看，分母的取值从\(\sigma\)出发，逐渐靠近\(T_s\)，最终收拢到\(T_c\)。下图有助于理解： 上图其实是bezier曲线的原理示意图，出处是贝塞尔曲线原理（简单阐述）]]></content>
  </entry>
  <entry>
    <title><![CDATA[WAVE中的SCH调度机制研究状况]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fwave%2Fsch.html</url>
    <content type="text"><![CDATA[WAVE协议中指定了CCH和SCH两种信道。在这个页面我们梳理一下现有的学术界对于如何调度使用SCH信道的机制研究。 CCH分配SCH时隙方式 在SCH上采用TDMA的方式共享信道，]]></content>
  </entry>
  <entry>
    <title><![CDATA[WAVE标准中的Beacon性能简化分析]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fwave%2Fperformance-analysis-of-beacon.html</url>
    <content type="text"><![CDATA[在很多使用WAVE作为车联网通信标准的文献研究中[1] [2]，在遇到Beacon时通常会简化其退避过程。由于Beacon的生命周期有限，无法接受无止限的退避，因此，可以令其简化为只进行一轮退避，从而略过退避窗口指数增长的过程。此时Beacon的发送过程简化为一维马尔科夫过程。 非马尔科夫链方法 不使用马尔科夫链的方式整理自[1]. 这篇文章主要的贡献是设计了长度可变的CCHI。 System Model 在节点访问信道前，需要侦听信道处于空闲的时间超过DIFS。如果DIFS之后信道仍然空闲，那么节点可以立即开始发送。否则，节点需要进行一段退避(_Backoff Counter_)，Backoff Counter从\([0, W-1]\)中随机取出。\(W\)被称为退避窗口(_CW_)。Backoff Counter在每一段空闲的时隙结束后减一。当Backoff Counter递减至0时，节点开始访问信道。 考虑到： Backoff Counter的初始值不会为0； Guard Interval内信道被判定为繁忙。 因此在CCHI开始所有车辆都需要进行退避。 除了上面的描述之外，这篇文章的推导还包含如下前提假设： 完美的信道条件 通信链路对称 每个车辆的通信范围固定且相通 载波侦听范围和通信范围一致 CCH完全传输时间 假设各车退避过程独立，网络内任意节点之间的的距离不超过两跳?。 考虑概率\(P(l, n, w, k)\)表示\(n\)个车辆在CCHI开头产生了Beacon包，竞争窗口大小为\(w\)，第一次发送尝试为\(l-1\)，\(k\)辆车在第\(l\)个时隙尝试传输（\(k\leq n\)），此概率值为： \[\begin{equation} \label{P} \begin{aligned} P(l, n, w, k)&amp;=\left(1-\frac{l-1}{w}\right)^{n} \cdot \left( \begin{array}{c}{n} \\ {k}\end{array}\right)\left(\frac{1}{w-l+1}\right)^{k}\left(1-\frac{1}{w-l+1}\right)^{n-k} \\ &amp;= \left( \begin{array}{c}{n} \\ {k}\end{array}\right) \cdot \frac{(w-l)^{n-k}}{w^n} \\ &amp;= \left( \begin{array}{c}{n} \\ {k}\end{array}\right) \cdot \left(1 - \frac{l}{w}\right)^{n-k} \cdot \frac{1}{w^{k}}. \end{aligned} \end{equation}\] 考虑到Backoff Counter的最大值是\(w\)，完成所有Beacon包传输需要的平均时间为 \[\begin{equation} \label{T} T(w, n)=\sum_{l=1}^{w} \left\{\begin{array}{l}{P(l, n, w, 1)\left[T_{s}+T(w-l, n-1)\right]} \\ {+\sum_{k=2}^{n} P(l, n, w, k)\left[T_{c}+T(w-l, n-k)\right]}\end{array}\right\} \end{equation}\] 其中\(T_{s}=L / R_{d}+D I F S+\delta\)的代表了一次成功的传输持续的时间。其中\(L\)表示平均包大小，\(R_d\)表示系统的传输速率，\(\delta\)表示传输延时?。\(T_{c}=L / R_{d}+E I F S+\delta\)表示发生碰撞后的持续时间。 这里\(T_c\)的计算和我们在WAVE中提到的推导里的\(T_c\)的有点不同，这里可以认为是所有包的大小一致的特殊情形。 在\(\eqref{T}\)中，求和的第一项表示的是只有一个节点选择了最早的发送时刻，此时能够成功发送。后一项表示的是超过一个节点选择在最早的发送时刻传输，导致冲突。 记节点的总数是\(N\)，退避窗口的大小是\(W\)，则\(T(W,N)\)可以通过迭代求出。 当\(W\)和\(N\)比较小时，\(T(W,N)\)会小于50ms，那么多余的CCHI被浪费了。反之，当\(W\)和\(N\)比较大时，\(T(W,N)\)会大于50ms，那么会有包无法发送而丢失掉。 CCH丢包率 我们定义\(L(W,N)\)为在一个CCHI内发生碰撞的包的数量。则 \[\begin{equation} \label{L} L(w, n)=\sum_{l=1}^{w} \left\{\begin{array}{l}{P(l, n, w, 1) \cdot L(w-l, n-1)} \\ {+\sum_{k=2}^{n} P(l, n, w, k)[k+L(w-l, n-k)]}\end{array}\right\} \end{equation}\] 根据\(\eqref{P}\)和\(\eqref{L}\)，通过迭代计算可以给出丢包率： \[\begin{equation} \label{packet-loss-rate} P(W, N)=\frac{L(W, N)}{N} \end{equation}\] 数值计算\(P(W, N)\)的值发现，丢包率即便在通常的的\(W\)和\(N\)取值内也非常高。通过提升竞争窗口可以改善这个丢包率，但是考虑到事实上CCHI的长度是有限的，而提高竞争窗口会推高\(\eqref{T}\)的值。如果将总传输时间\(T(W,N)\)值推高到50ms以上，那么50ms后续的包都会丢掉。综上可见，丢包率的改善是有上限的，且这个上限可能并不会达到100%. 马尔科夫链方法 马尔科夫链主要参考是的是[2]. Reference [1] L. Zhang, Z. Liu, R. Zou, J. Guo, and Y. Liu, “A scalable csma and self-organizing tdma mac for ieee 802.11 p/1609. X in vanets,” Wireless Personal Communications, vol. 74, no. 4, pp. 1197–1212, 2014. [2] K. A. Hafeez, L. Zhao, B. Ma, and J. W. Mark, “Performance analysis and enhancement of the dsrc for vanet’s safety applications,” IEEE Transactions on Vehicular Technology, vol. 62, no. 7, pp. 3069–3083, 2013.]]></content>
  </entry>
  <entry>
    <title><![CDATA[通信范围模型探讨]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fwave%2F%E9%80%9A%E4%BF%A1%E8%8C%83%E5%9B%B4%E6%A8%A1%E5%9E%8B%E6%8E%A2%E8%AE%A8.html</url>
    <content type="text"><![CDATA[前言 我们这里探讨对于通信模型的认识。一般来说，最简单的通信模型是截断模型，即在一定的距离范围内通信总是可靠，而超出这一范围之外丢包率变成100%. 这里我们更加细致地讨论一下更为符合实际条件的通信范围模型[1]。 System Model 在进行理论分析前有必要给出我们使用的System Model。这里的System Model与[1]中的系统模型是一致的。 注意，在这里我们整理了所有参考文章中提出的系统模型假设，不过并不是所有的假设都会用于本文的分析内容。 对于VANETS中的安全类应用，车辆会广播两种类型的信息： Warning Message (Event driven): 事件驱动的警告信息。例如前方突然发生了碰撞，周围的车辆会产生对应的warning message来警告后续的车辆； Status Message。状态信息是周期性发送的，其中包含了车辆的运行状态（位置，速度，加速度，行驶方向等）。 这里Warning message使用最高的优先级AC3(参见EDCA)，周期性播报的状态消息则使用AC0. 在我们的模型中，假设状态消息产生的速度是\(\lambda_s\)，那么同步间隔SI长度为\(1/\lambda_s\)(每个同步周期产生一个状态包). 假设所有包的大小都是一致的，为\(L\)比特。为了简化分析假设整个SI都用于传输安全信息，即CCHI=SI。每辆车会随机选择SI中的一个时隙发送状态包，warning message则随着事件的发生随机产生。 假设所有的车辆发射功率都是\(P_t\)，对于接收方来说，若接收功率达到了门限\(P_{th}\)就认为能够成功地收到数据包。由于信道衰减的随机性，接收功率也会是随机的，这意味着通信范围也是一个随机变量。通信范围的累积概率函数(CDF)\(F_R(r)\)及其期望\(E[R]\)会在后续进行推导。 变量以及缩略语表 通信范围 传输模型 VANET的信道衰减在近距离上服从Rician分布，在长距离上趋近于Rayleigh分布。通过使用Nakagami分布可以综合这两种情况。Nakagami模型的概率密度函数（PDF）如下： \[\begin{equation} \label{1} P_{z^{2}}(x)=\left(\frac{m}{P_{r}}\right)^{m} \frac{x^{m-1}}{\Gamma(m)} e^{-\frac{m x}{P_{r}}}, \quad \text { for } x \geq 0 \end{equation}\] 其中，\(x\)为接收到的信号功率，\(\Gamma(\cdot)\)为Gamma函数，\(P_{r}=P_{t} K / r^{\alpha}\)为平均接收功率，\(r\)为通信距离（米），\(\alpha\)为路径衰减指数。\(K=G_{t} G_{r}\left(C /\left(4 \pi f_{c}\right)\right)^{2}\)，其中\(C\)为光速，\(f_c=5.9 \text{GHz}\)为载波频率，\(G_t\)和\(G_r\)分别为发送者和接受者的天线增益。\(m\)为衰减系数。 当\(m=1\)时，Nakagami退化成Rayleigh分布，当\(m=(k+1)^2/(2k+1)\)时，Nakagami分布可以近似为参数为\(k\)的Rician分布（\(k\) is the ratio of power in the line of sight to the power in the non-line of sight）。 通过\(\eqref{1}\)，我们可以计算出当接收能量超过\(P_{th}\)时，通信范围的CDF： \[\begin{equation} \label{2} F_{R}(r)=1-P\left(x \geq P_{th}\right)=1-\int_{P_{th}}^{\infty} P_{z^{2}}(x) d x \end{equation}\] 将\(\eqref{1}\)代入到\(\eqref{2}\)，并记\(u=(m x) / P_r\)，CDF可以写作： \[\begin{equation} \label{3} F_{R}(r)=1-\frac{1}{\Gamma(m)} \int_{\frac{m P_{\mathrm{th}}}{P_{r}}}^{\infty} u^{m-1} e^{-u} d u \end{equation}\] 使用下面的积分变换： \[\begin{equation} \int x^{n} e^{c x} d x=\left(\frac{d}{d c}\right)^{n} \frac{e^{c x}}{c} \end{equation}\] CDF可以写成： \[\begin{equation} \label{5} F_{R}(r)=1-\frac{1}{\Gamma(m)} \sum_{i=0}^{m-1} \frac{(m-1) !}{(m-1-i) !}\left(\frac{m P_{\mathrm{th}}}{P_{r}}\right)^{m-1-i} e^{-\frac{m P_{\mathrm{th}}}{P_{r}}} \end{equation}\] 有了CDF以后，我们就可以计算通信范围的平均值\(E[R]\)了： \[\begin{equation} \begin{aligned} E[R]=\frac{1}{\alpha \Gamma(m)} \sum_{i=0}^{m-1} &amp; \frac{(m-1) !}{(m-1-i) !} \\ &amp; \times \Gamma\left(m-1-i+\frac{1}{\alpha}\right)\left(\frac{m P_{\mathrm{th}}}{P_{t} K}\right)^{-\frac{1}{\alpha}} \end{aligned} \label{6} \end{equation}\] 上面推导的是能够成功通讯的范围。对于载波侦听的范围\(E[L_{CS}]\)可以通过\(\eqref{6}\)类似的过程进行推导。这里载波侦听范围指的是节点可以感知到包（但是未必能够成功收到此包）的范围。我们认为当接收功率达到阈值\(P_{CS}\)时即可。记\(P_{CS}=\rho P_{th}\)，其中\(\rho \in (0, 1]\)。则有下面的关系： \[\begin{equation} E\left[L_{CS}\right]=\frac{E[R]}{\sqrt[\alpha]{\rho}} \end{equation}\] Reference [1] K. A. Hafeez, L. Zhao, B. Ma, and J. W. Mark, “Performance analysis and enhancement of the dsrc for vanet’s safety applications,” IEEE Transactions on Vehicular Technology, vol. 62, no. 7, pp. 3069–3083, 2013.]]></content>
  </entry>
  <entry>
    <title><![CDATA[基于Platoon的车间通信(V2V)机制设计]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fplatoon%2Fplatoon-v2v.html</url>
    <content type="text"><![CDATA[编队内通信机制 为了维持编队结构，编队内部的成员需要周期性的交换其运动参数，即Beacon message dissemination。目前车联网通信的标准是WAVE。这里我们简要叙述一下WAVE的核心特点。 我们需要关注的WAVE的最为核心的两个标准是IEEE802.11p和IEEE1609.4。其中，前者规定了MAC层和物理层的标准，后者提供了多信道仿真的规范。WAVE在物理层上拥有7个信道，包括一个控制信道CCH和六个服务信道SCH。CCH上不能用于发送IP包。一般而言，控制帧和安全相关的重要信息在CCH上传输，而SCH用于传输非安全信息。依照IEEE1609.4的规定，信道访问时间被切分为100ms长的同步间隔SI，SI内部又分为50ms的CCH访问时间和50ms的SCH访问时间。 由于通信资源有限，当车流密度增大时，信道会出现拥堵，导致通信的可靠性(广播效率，丢包率)下降，进而威胁到行程安全。由于CCH信道只有一个且用于传输重要的控制和安全信息，所以研究者一般关注CCH上的通信性能可靠性。 另一方面，当车流密度很低时，CCH信道的资源会限制，不利用充分的信道利用。 以下是解决这些问题的常见策略： 引入非竞争协议（如TDMA）来保证Beacon性能 调节网络参数来优化性能（如调整Beacon间隔，竞争窗口大小等） 引入竞争协议 引入非竞争协议后，汽车一般会被分簇，在簇内由簇头来进行同步和通信资源分配，从而确保消息在发送时，发送者对于信道的独占，这样可以最大程度地保证消息发送的可靠性。不过，这种机制的缺陷还是很明显的： 协同和资源分配的机制一般比较复杂 引入了额外的Overhead开销 在大规模网络环境下难以使用，例如涉及到多跳问题时，TDMA的同步和资源分配机制的实现复杂度会远远高于单跳场景。很多文章都是假定簇的大小不会超过单跳范围来规避这个问题。 在大规模网络环境下，另一个问题是时隙资源不够划分之后采用如何策略来保持性能和网络公平性 动态调节网络参数 这类方法通常不需要对WAVE原有的协议作出大的修改，也不需要同步和协同，可以进行本地决策。不过其缺点在于为了得到网络环境与待调节的网络参数的关系，需要使用非常复杂的数学模型工具。 基于具体业务形态的设计 很多研究都是着眼于提升网络的整体性能，而非聚焦到具体的业务场景。例如在编队场景中，为了改善网络的性能，可以考虑编队的拓扑结构。这一方面的研究还比价少，不过这类研究也比较困难，因为车联网中的具体业务还很少，如何构合理地构建业务场景其实是缺乏一个坚定的共识的。 编队间通信机制 车里编队间的通信问题是一个更加宏观的问题。在编队内部我们可以拥有一个非常稳定的拓扑结构，因而可是实现一些需要很高的协同性支持的机制。而在编队间通信问题上，我们就需要处理一些传统车联网场景关注的问题，如VANET connectivity以及链路质量。同时，数据分发过程中的路由问题也需要考虑。 VANET connectivity 目前对于车联网中车间通信的网络连接性的问题还是主要围绕着自由车辆（即非编队车辆）进行[1]，一般的结论是随着车流量的增加VANET的连通性能够得到改善。[2]中讨论了基于编队系统的车联网连通性问题。作者从VCPS的角度来讨论这个场景下的问题，这意味着对于运动和网络性能的联合分析。这也是分析网络连通性的通常做法。 双向道路上的多编队系统示意图 Platoon Dynamics 在[3]中发现，在采用predeccessor-following的控制策略，即每辆车只知道前方车辆的运动参数的情况下，越接近编队尾部的车辆的运动扰动就会越大。因此，编队内的车辆除了需要知道前方车辆的运动参数以外，还需要了解编队Leader的状态l，即predeccessor-leader策略[4]，方能维持一个比较稳定的车间距。Leader的状态可以通过VANET通信来获取。这一思路延续下来的产物是CACC[5]。 Architecture 这里介绍的是[2]中提出的架构。 基于platoon-based VCPS的角度，我们可以将问题拆解成下面两个过程： networking/communication process; the platoon mobility process. 这里要讨论的架构的主要作用，就是描述这两个过程的运作，及两者的相互作用。这里提到的相互作用可以通过下面两个例子来说明： 考虑碰撞预警应用，其中每辆车周期性的发送自己的运动信息，提醒周围的车辆避免碰撞。当编队规模小，编队间距大的时候，通信过程中的冲突和碰撞少，因此延时和丢包率性能好。反之，如果编队规模大，编队间距小，则通信情况会恶化。 考虑CACC系统，CACC依赖VANET来交换周围车辆的运动信息。这里CACC可以建模成网络控制的控制系统。当丢包率提升时，控制系统的可靠性也会下降[6]。 Architecture for platoon-based VCPSs Mobility Specification 自由车辆 我们考虑双向一车道的场景，即不存在超车的场景。编队系统由自由随机运动的交通流(Individual Random Trafic Flow)构建而来。为了建模这个自由随机运动的场景，我们选择Time Headway这个统计量来作为描述车流分布的基础统计量。 Time Headway定义为同向行驶的两辆相邻的车通过同一个观察点的时间差。一般认为Time Headway是独立同分布的随机变量。对于Time Headway的研究非常早了，也有非常多的模型可以使用。其中比较经典的是指数分布模型，正则分布模型，Gama分布，Log-normal分布。[7]中提出，在车流量大约在700到1700 (vph)之间时，Log-normal模型的比较适用。并且，在NGSIM Trajectory Data数据集中，Time Headway也是服从Log-normal分布的。因此在这里我们采用Log-normal分布来描述Time Headway: \[\begin{equation} f\left(t_{h} ; \mu, \sigma, \tau\right)=\frac{1}{\sqrt{2 \pi} \sigma\left(t_{h}-\tau\right)} \exp \left(-\frac{\left(\log \left(t_{h}-\tau\right)-\mu\right)^{2}}{2 \sigma^{2}}\right), \quad t_{h}&gt;\tau \end{equation}\] 其中，\(t_h\)是Time Headway，\(\tau\)为Time Headway的最小值，\(\mu\)为规模参数(Scale Parameter)，\(\sigma\)为形状参数(Shape Parameter)。根据这个分布可以得到均值和方差： \[\begin{equation} \mu\left(T_{h}\right)=\tau+e^{\mu+\frac{1}{2} \sigma^{2}} \end{equation}\] \[\begin{equation} \sigma^{2}\left(T_{h}\right)=e^{2 \mu+\sigma^{2}}\left(e^{\sigma^{2}}-1\right) \end{equation}\] 在交通流的稳定状态下，假设车辆速度大致相等且为常数\(V_{stb}\)，那么可以得到Distance Headway: \[\begin{equation} s_{h} \approx v_{s t b} t_{h} \end{equation}\] 显然，\(s_{h}\)也是服从Log-normal分布的。另外，我们假设结成编队以后，在一个编队内部的车辆的速度是相同的，为\(v_{stb}\)。 编队内运动 上面考虑的是自由车辆模型，考虑车辆组合成编队的情况。首先作如下的符号定义： 符号定义 其中： \(P^i\)表示第i个编队 \(C_j^i\)表示在第i个编队中第j辆车。 \(s_j^i\)为编队内的车辆\(C_{j-1}^{i}\)和\(C_{j}^{i}\)的间距 \(S_i\)为编队\(P^{i-1}\)和\(P_i\)之间的距离。 为了简便，在后面的讨论中只涉及单编队时我们隐去\(i\)上标。完整的符号表如下： 变量查询表 编队内部的运动模型最常用的是car-following model，可以有效地描述ACC-equipped编队运动[8]。在这里我们认为除了Leader之外所有的车辆都遵从car-following model。具体的，我们选择Intelligent Driver model(IDM)[9]: \[\begin{equation} \label{5} s_{j}^{*}(t)=s_{0}+v_{j}(t) T_{0}+\frac{v_{j}(t) \Delta v_{j}(t)}{2 \sqrt{a b}} \end{equation}\] \[\begin{equation} \label{6} a_{j}(t)=a\left[1-\left(\frac{v_{j}(t)}{v_{0}}\right)^{4}-\left(\frac{s_{j}^{*}(t)}{s_{j}(t)}\right)^{2}\right] \end{equation}\] 其中\(s^*_j(t)\)为与前车的目标间距。根据\(\eqref{5}\eqref{6}\)，可以推导出稳定状态（即\(a_j(t)\)为0）的车间距: \[\begin{equation} s_{s t b}=\frac{s_{s t b}^{*}}{\sqrt{1-\left(\frac{v_{s t b}}{v_{0}}\right)^{4}}}=\frac{s_{0}+v_{s t b} T_{0}}{\sqrt{1-\left(\frac{v_{s t b}}{v_{0}}\right)^{4}}} \end{equation}\] 对于编队的Leader，我们则假定他们的行驶速度处于稳定状态，即速度为恒定为\(v_{std}\)。 Networking specification 假设所有车辆的通信距离相通，为\(R\)。在网络连通性问题上，只考虑通信距离这一最主要因素。只要辆车间距小于\(R\)，两者之间的通信就被认为是是可靠的。 在[2]中，作者还进一步修改了MAC层的行为： 对于单播包(Unicast)，当发送尝试访问信道时，若信道繁忙，将发送过程推迟一段时间，这段时间由竞争窗口决定（即一轮backoff）。如果没有收到ACK，那么将竞争窗口加倍； 对于广播报，不进行重传，只进行一轮backoff。 网络拓扑方面做出如下假设：编队内的车辆可以直接进行通信，Leader负责管理编队，同时扮演网关的功能（接收前方编队的消息并转发给编队成员）。编队的Leader可能被选为Message Carrier来转发来自对向行驶的车辆信息。编队尾部的车负责和后方的编队通信。 拓扑结构 基于上述拓扑，设定对应的车辆通信行为规则： 编队内：所有消息可以直达 编队间：如果编队之间可以进行通信（即前一个编队的尾部车辆可以和后一个编队的头部车辆通信），那么直接由尾部车辆向后方广播消息即可。如果编队之间不能直接通信，那么尾部车辆会先从对向车道中选择距离后方编队最近的车辆来转发信息。如果这样的转发者也无法直接和目的节点通信，那么他会继续向其前方边度转发消息。直到无法继续转发或者被最终目的节点收到为止。这个过程如下图所示： 编队间信息传递过程示意图 Connectivity Analysis 连通性分析的核心在于得到编队间距离的分布。 编队间距离分布 为了简化分析，假设所有编队的组成都是一样的。即拥有相同的编队大小和IDM参数（此时我们可以再次略去编队上标号）。基于“符号定义”那张图，编队间距可以表示为： \[\begin{equation} S=S_{L}-L \end{equation}\] 进而有如下引理： Lemma1: Assume all platoons are formed uniformly and controlled by IDM, inter-platoon spacing is lognormal distributed in the traffic steady state with all platoon leaders driving at the same velocity \(v_{stb}\). (证明过程略) 基于这个定理即证明过程的推导可以得到下面的关系： \[\begin{equation} f_{S_{L}}(x)=\frac{1}{\sqrt{2 \pi} \sigma_{D}(x-L)} \exp \left(-\frac{\left(\log (x-L)-\mu_{D}\right)^{2}}{2 \sigma_{D}^{2}}\right), \quad x&gt;L \end{equation}\] \[\begin{equation} f_{S}(x)=\frac{1}{\sqrt{2 \pi} \sigma_{D} x} \exp \left(-\frac{\left(\log (x)-\mu_{D}\right)^{2}}{2 \sigma_{D}^{2}}\right), \quad x&gt;0 \end{equation}\] 编队间通信延时 相邻编队的时隙重叠问题 若编队内采用了TDMA的方式进行通信，那么相邻的编队之间的时隙可能发生重叠。在[10]中，作者提出了一种解决这一问题的方法，流程如下： 相邻编队的时隙分配机制设计 在[10]中，作者考虑了编队和自由车辆同时出现的场景，其中编队内发送Beacon时采用TDMA的方式，编队时隙位于CCHI的头部，自由车辆在这一段时间内不允许发送。如果相邻的编队的距离靠的太近（如上图(a)所示)，那么就可能发生时隙碰撞。在这个场景下，如上图(b)中显示，编队A和编队B的时隙分配发生了重叠。重叠时隙内的汽车发送时，会由于CSMA的退避机制导致延时增长以及碰撞。 另一个潜在的问题是隐藏终端问题。即便两辆车彼此之间不会感知到对方，但是在两者通信范围的重叠区域，两者的包会发生冲突，从而无法被重叠区域内的车辆收到。如同上图(c)中的情形，尽管A0和B0两两者互相无法感知对方，如果两者同时发包，中间重叠区域的A1 ~ A4都无法收到。 为了解决这两个问题，作者提出了一种Self-Configuring的时隙分配方式。编队的Leader需要首先确定是否在相邻的编队间出现了时隙重叠，这个确认过程如下：在每个CCHI，编队的Leader会广播一个包含了时隙分配信息的包。如果编队成员正确地收到了此包，那么他们遵循Leader的安排在对应的时隙发包。此时，在正常情况下，Leader应该受到所有成员发送的Beacon。反之，如果Leader在连续若干个CCHI（至少两个）内都发生了Beacon丢包，Leader就会推定这种丢包现象是由于时隙重叠导致的。 在Leader确认发生了时隙重叠以后，Leader会自适应地重新安排TDMA时隙，暂时选择重叠的时隙的下一个时隙。当没有重叠的时隙安排时，Leader会重置到最初的时隙安排。 上一段话中描述的重新安排重叠时隙的方式翻译自原文，不过我觉得这里有点问题： 时隙重叠是对称的，即A编队和B编队的时隙分配发生重叠时，双方的Leader都能检测到冲突，双方同时开始调整，反而使得原来的重叠时隙空出来。 “选择重叠的时隙的下一个时隙”这个做法也含糊。按照情况分析，应该是将时隙序列整体往后移动一个时隙的长度，但是发生重叠的时隙可能不止一个，而且重叠的时隙并不一定连续，那如何处理呢？ 我对这里的这个机制持怀疑态度，作者在文章中也并未从理论和仿真的角度验证这个机制的有效性。出于严谨，读者最好参阅原文中的Section V-C 我们还需要考虑下面的两个问题： 当两个编队A和B同向行驶且距离较近时，前方的编队A保持其时隙结构不变，后面的编队B将其时隙延迟到A的时隙之后（如上图(e))来避免碰撞和隐藏中断问题。这是因为在前车数据的重要性要比后车的重要性高。另一个原因是后方编队的Leader检测时隙重叠的速度更快（例如前方编队的Leader不会受到隐藏终端问题的影响）。 当编队A和B是反向行驶交会时，当两个Leader的间距小于通信距离，Leader发送的包会发生冲突。根据文章中的机制设计，在CCH信道上编队的TDMA时隙结束后的CSMA通信过程中，Leader还有一个机会发送Beacon包。此时，哪方先收到了对方的Beacon，哪方延后其时隙安排。 用混合协议解决编队间通信 这里的混合协议是指TDMA和CSMA的混合协议。 Reference [1] S. Yousefi, E. Altman, R. El-Azouzi, and M. Fathy, “Analytical model for connectivity in vehicular ad hoc networks,” IEEE Transactions on Vehicular Technology, vol. 57, no. 6, pp. 3341–3356, 2008. [2] D. Jia, K. Lu, and J. Wang, “On the network connectivity of platoon-based vehicular cyber-physical systems,” Transportation Research Part C: Emerging Technologies, vol. 40, pp. 215–230, 2014. [3] P. Seiler, A. Pant, and K. Hedrick, “Disturbance propagation in vehicle strings,” IEEE Transactions on automatic control, vol. 49, no. 10, pp. 1835–1842, 2004. [4] R. Rajamani, S. Choi, J. K. Hedrick, and B. Law, “Design and experimental implementation of control for a platoon of automated vehicles,” in Proceedings of the asme dynamic systems and control division (1998), 1998. [5] P. Fernandes and U. Nunes, “Platooning with ivc-enabled autonomous vehicles: Strategies to mitigate communication delays, improve safety and traffic flow,” IEEE Transactions on Intelligent Transportation Systems, vol. 13, no. 1, pp. 91–106, 2012. [6] C. Lei, Van EenennaamE., W. K. Wolterink, G. Karagiannis, G. Heijenk, and J. Ploeg, “Impact of packet loss on cacc string stability performance,” in 2011 11th international conference on its telecommunications, 2011, pp. 381–386. [7] D.-H. Ha, M. Aron, and S. Cohen, “Time headway variable and probabilistic modeling,” Transportation Research Part C: Emerging Technologies, vol. 25, pp. 181–201, 2012. [8] A. Kesting, M. Treiber, and D. Helbing, “Enhanced intelligent driver model to access the impact of driving strategies on traffic capacity,” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 368, no. 1928, pp. 4585–4605, 2010. [9] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in empirical observations and microscopic simulations,” Physical review E, vol. 62, no. 2, p. 1805, 2000. [10] B. Liu, D. Jia, K. Lu, D. Ngoduy, J. Wang, and L. Wu, “A joint control–communication design for reliable vehicle platooning in hybrid traffic,” IEEE Transactions on Vehicular Technology, vol. 66, no. 10, pp. 9394–9409, 2017.]]></content>
  </entry>
  <entry>
    <title><![CDATA[ns3中的wave模块]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fwave%2Fns3%E4%B8%AD%E7%9A%84wave%E6%A8%A1%E5%9D%97.html</url>
    <content type="text"><![CDATA[前言 ns-3 is a discrete-event network simulator for Internet systems, targeted primarily for research and educational use. ns-3 is free software, licensed under the GNU GPLv2 license, and is publicly available for research, development, and use. 简而言之，ns3是一款基于C++的离散事件网络模拟器，其内部实现了很多常见的网络协议。因此学术界通常使用ns3来作为论文仿真分析的框架。我们在这篇文章里梳理一下ns3中关于WAVE部分的内容。 WAVE的初始化 下面的代码给出了一种WAVE机制的初始化方法： 123456789101112131415161718192021222324252627282930313233343536373839404142/* * lossModelName: 信道损失模型，默认值是 * ns3::LogDistancePropagationLossModel */auto lossModelName = m_config-&gt;Get ("lossModel");double freq = 5e9;YansWifiChannelHelper waveChannel;waveChannel.SetPropagationDelay ("ns3::ConstantSpeedPropagationDelayModel");if (lossModelName == "ns3::TwoRayGroundPropagationLossModel") &#123; waveChannel.AddPropagationLoss (lossModelName, "Frequency", DoubleValue (freq), "HeightAboveZ", DoubleValue (1.5));&#125; else if (lossModelName == "ns3::LogDistancePropagationLossModel") &#123; waveChannel.AddPropagationLoss (lossModelName);&#125; else &#123; waveChannel.AddPropagationLoss (lossModelName, "Frequency", DoubleValue (freq));&#125;/** * propagationLossModel: 默认是None */auto propagationLossModel = m_config-&gt;Get ("propagationLossModel");if (propagationLossModel != "None") &#123; waveChannel.AddPropagationLoss (propagationLossModel);&#125;// Create the channel using settings aboveauto wavePhy = YansWavePhyHelper::Default ();wavePhy.SetChannel (waveChannel.Create ());wavePhy.SetPcapDataLinkType (WifiPhyHelper::DLT_IEEE802_11);/** * 发射功率，单位为dbm */auto txp = m_config-&gt;Get&lt;double&gt; ("txp");wavePhy.Set ("TxPowerStart",DoubleValue (txp));wavePhy.Set ("TxPowerEnd", DoubleValue (txp));QosWaveMacHelper waveMac = QosWaveMacHelper::Default ();WaveHelper waveHelper = WaveHelper::Default ();auto phyMode = m_config-&gt;Get ("phyMode");waveHelper.SetRemoteStationManager ("ns3::ConstantRateWifiManager", "DataMode",StringValue (phyMode), "ControlMode",StringValue (phyMode));m_devices = waveHelper.Install (wavePhy, waveMac, m_nodes); 这段代码中我们逐一创建了信道模型，物理层，链路层，最后通过waveHelper将各个部分组装在一起，并且安装到节点上： 1m_devices = waveHelper.Install (wavePhy, waveMac, m_nodes); waveHelper waveHelper-&gt;Install的核心代码如下： 1234567891011121314151617181920212223242526272829303132Ptr&lt;Node&gt; node = *i;Ptr&lt;WaveNetDevice&gt; device = CreateObject&lt;WaveNetDevice&gt; ();device-&gt;SetChannelManager (CreateObject&lt;ChannelManager&gt; ());device-&gt;SetChannelCoordinator (CreateObject&lt;ChannelCoordinator&gt; ());device-&gt;SetVsaManager (CreateObject&lt;VsaManager&gt; ());device-&gt;SetChannelScheduler (m_channelScheduler.Create&lt;ChannelScheduler&gt; ());for (uint32_t j = 0; j != m_physNumber; ++j) &#123; Ptr&lt;WifiPhy&gt; phy = phyHelper.Create (node, device); phy-&gt;ConfigureStandard (WIFI_PHY_STANDARD_80211_10MHZ); phy-&gt;SetChannelNumber (ChannelManager::GetCch ()); device-&gt;AddPhy (phy); &#125;for (std::vector&lt;uint32_t&gt;::const_iterator k = m_macsForChannelNumber.begin (); k != m_macsForChannelNumber.end (); ++k) &#123; Ptr&lt;WifiMac&gt; wifiMac = macHelper.Create (); Ptr&lt;OcbWifiMac&gt; ocbMac = DynamicCast&lt;OcbWifiMac&gt; (wifiMac); // we use WaveMacLow to replace original MacLow ocbMac-&gt;EnableForWave (device); ocbMac-&gt;SetWifiRemoteStationManager ( m_stationManager.Create&lt;WifiRemoteStationManager&gt; ()); ocbMac-&gt;ConfigureStandard (WIFI_PHY_STANDARD_80211_10MHZ); device-&gt;AddMac (*k, ocbMac); &#125;device-&gt;SetAddress (Mac48Address::Allocate ());node-&gt;AddDevice (device);devices.Add (device); 首先创建WAVE设备，并且设置ChannelCoordinator, VsaManager, ChannelScheduler，逐一最后一个是通过是通过工厂对象创建的。工厂对象默认创建的ChannelScheduler是ns3::DefaultChannelScheduler(事实上ns3内部只有这一个具体的ChannelCoordinator实现). 在接下来的循环中，waveHelper利用参数中传入的物理层Helper来创建物理层对象。这里的m_physNumber默认为1。对于物理层，这里进一步设置了信道带宽标准（10MHz），将信道号设置为CCH信道号。 在第二个循环中，waveHelpher创建WAVE的七个信道，这里的m_macsForChannelNumber默认来自ChannelManager::GetWaveChannels ()。在循环体内，对于每一个WAVE信道，wave进行如下操作： 启用WAVE支持(这一部分详细阅读以下面的waveMac章节) 设置RemoteStationManager 设置标准为WIFI_PHY_STANDARD_80211_10MHZ 最后为设备分配MAC地址。 wavePhy YansWavePhyHelper继承了YansWifiPhyHelper。相比于父类，wavePhy其实主要修改了自带的日志输出范式，对于功能主干影响不大。我们来看关键的YansWifiPhyHelper::Create函数。 12345678910Ptr&lt;WifiPhy&gt;YansWifiPhyHelper::Create (Ptr&lt;Node&gt; node, Ptr&lt;NetDevice&gt; device) const&#123; Ptr&lt;YansWifiPhy&gt; phy = m_phy.Create&lt;YansWifiPhy&gt; (); Ptr&lt;ErrorRateModel&gt; error = m_errorRateModel.Create&lt;ErrorRateModel&gt; (); phy-&gt;SetErrorRateModel (error); phy-&gt;SetChannel (m_channel); phy-&gt;SetDevice (device); return phy;&#125; 这个函数只是按部就班地设置对应的属性，没有特别的处理。 waveMac waveMac直接使用了QosWaveMacHelper的默认设置。这个部分Helper配置的部分极少。可以认为是直接从构造函数创建OcbWifiMac。 OCB，即Outside Context of BSS，即脱离BSS的组织形式，让节点直接直接进行通信的范式。 OcbWifiMac 的注释可以提供进一步的说明： In OCB mac mode,synchronization, association, dis-association and authentication of normal wifi are not used for wireless access in vehicular environments. Although Timing Advertisement frame is a specific management frame defined in 802.11p. It is mainly used by IEEE Std 1609.4 for channel switch synchronization. However in simulation nodes are supposed to have GPS synchronization ability, so we will not implement this feature. 关于OcbWifiMac::EnableForWave的说明： 在waveHelper的处理中，对创建的OcbWifiMac对象调用了EnableForMac函数。这个函数的主要作用是，将OcbWifiMac的m)low底层MAC实现，从MacLow替换为WaveMacLow。 WAVE中的Tx 通过WaveNetDevice直接发包方式下的路径 这里我们使用直接从WaveNetDevice的发送接口进行发包的方法。例如： 12345678auto sender = DynamicCast&lt;WaveNetDevice&gt; (m_devices.Get (0));auto receiver = DynamicCast&lt;WaveNetDevice&gt; (m_devices.Get (1));const Address dest = receiver-&gt;GetAddress ();SeqTsHeader seqTs;seqTs.SetSeq (1);packet-&gt;AddHeader (seqTs);// 0x0800是IP协议号sender-&gt;Send (packet, dest, 0x0800); 注意在执行上面的发送前还需要对WAVE进行信道配置，否则无法发送。信道配置不需要在每次发送前配置，只需要在设置发生变更的时候修改设置即可。配置的示例代码如下： 123456789101112auto schInfo = SchInfo (SCH1, immediateAccess, EXTENDED_ALTERNATING);auto txProfile = TxProfile (SCH1);auto sender = DynamicCast&lt;WaveNetDevice&gt; (m_devices.Get (0));auto receiver = DynamicCast&lt;WaveNetDevice&gt; (m_devices.Get (1));Simulator::Schedule ( Seconds (0), &amp;WaveNetDevice::RegisterTxProfile, sender, txProfile);Simulator::Schedule ( Seconds (0), &amp;WaveNetDevice::RegisterTxProfile, receiver, txProfile);Simulator::Schedule ( Seconds (0), &amp;WaveNetDevice::StartSch, sender, schInfo);Simulator::Schedule ( Seconds (0), &amp;WaveNetDevice::StartSch, receiver, schInfo); WaveNetDevice::Send 这里的调用入口是WaveNetDevice::Send(ns3::Ptr&lt;...&gt; packet, const ns3::Address &amp;dest, uint16_t protocol). 这个函数中做了如下处理: 检查m_txProfile，在WAVE中的，要发送数据必须要先注册一个TxProfile，这个结构提供了上层对于物理底层参数的控制能力。 检查还是否可以访问m_txProfile中指定的信道。 检查m_txProfile中的其他参数，并生成一个HigherLayerTxVectorTag的PacketTag添加到包中。 添加LlcSnapHeader，此Header中包含了协议类型（如是IP包，协议为0x0800）。 根据m_txProfile中制定的信道编号，获取对应的WifiMac(实质是OcbWifiMac)，将包压入其队列。这一级调用见下一个子部分。 OcbWifiMac::Enqueue 这个函数里的处理主要分为两个部分：一是m_stationManager的相关处理，二是QoS相关的处理。同时，MAC帧头WifiMacHeader也在这里生成。 注意，在WAVE中，QoS功能是启用的，即GetQosSupported返回true。那么发送队列的选择会由EDCA机制来控制: 1m_edca[QosUtilsMapTidToAc (tid)]-&gt;Queue (packet, hdr); 其中m_edca的类型为EdcaQeueus（typedef std::map&lt;AcIndex, Ptr&lt;QosTxop&gt; &gt; EdcaQueues;），本质从EDCA的Access Category index映射到对应对应队列的字典。 QosTxop::Queue QosTxOp继承自Txop。Queue这个函数没有改动。在Txop::Queue内，传入的包被纳入m_queue这个内部队列，然后StartAccessIfNeeded被调用来尝试访问信道。 m_queue是WifiMacQueue类型。这个队列实现了802.11协议中的超时功能。在包被取出时，队列检查其时间戳，如果超时会丢弃这个包。 ChannelAccessManager::StartAccessIfNeeded StartAccessIfNeeded是开始信道访问尝试的入口，这个过程涉及众多函数，我们在这里统一梳理。 在StartAccessIfNeeded函数中，若 m_currentPacket为空，即当前没有正在发送的包。 m_queue不是空，即还有包等待发送 IsAccessRequested()为false，为了避免请求信道重复 m_low-&gt;IsCfPeriod为false，底层mac是否处于可供发送的状态 (CF: Contention-Free) 以上条件全部得到满足，那么Txop会通过m_channelAccessManager-&gt;RequestAccess来请求访问信道。 这里的m_channelAccessManager是从RegularWifiMac::m_channelAccessManager赋值而来，不同的同一个mac层的不同Txop共享。 ChannelAccessManager会处理和退避相关的事宜： 如果信道可以访问，调用ChannelAccessManager::DoGrantAccess 不管在任何情况下，调用ChannelAccessManager::DoRestartAccessTimeoutIfNeeded，这一步是为了调度一下次对信道的访问尝试（例如如果本次访问信道失败，则重新更新退避信息后，在一定间隔后再次访问信道）。 在DoGrantAccess中，最终成功访问到信道的Txop的NotifyAccessGranted函数会被调用。 信道的访问规则使我们关注的重点，尤其是CCH和SCH的信道访问控制。经过测试发现，在CCHI时请求SCH信道时，不会调用DoGrantAccess。在CCHI发起SCH请求时，m_sleeping为true，因此会在RequestAccess函数开头即返回，这里也不会通过DoRestartAccessTimeoutIfNeeded来调度下一次信道访问（毕竟信道睡眠中）。可见WAVE的Aternative Accessing在MAC层是通过让OcbWaveMac周期性地睡眠实现的。这一功能主要实现于DefaultChannelScheduler中。 QosTxop::NotifyAccessGranted QosTxop覆盖了NotifyAccessGranted的实现。在这函数里面，WifiMacHeader中的一些参数进行了重新设置。主要包括： SeqNo 禁止分段（即将大包拆解成小包传输） NoRetry 注意，对于QoS数据包，ACK在这里被禁用： 12345678if (m_currentHdr.IsQosData () &amp;&amp; m_currentHdr.IsQosBlockAck ())&#123; m_currentParams.DisableAck ();&#125;else&#123; m_currentParams.EnableAck ();&#125; 在涉及参数配置以及AMSDU等方面的设置完成后调用m_low-&gt;StartTransmission。前面我们提到过，这里的m_low为WaveMacLow类型。 WaveMacLow::StartTransmission WSA包的发送 发送WSA包时，配置方法与发送IP包的是一致的，具体到发送上，餐卡wave-simple-device.cc里面的示例: 12345Ptr&lt;Packet&gt; wsaPacket = Create&lt;Packet&gt; (100);Mac48Address dest = Mac48Address::GetBroadcast ();const VsaInfo vsaInfo = VsaInfo (dest, OrganizationIdentifier (), 0, wsaPacket, SCH1, 100, VSA_TRANSMIT_IN_BOTHI);Simulator::Schedule (Seconds (1.0), &amp;WaveNetDevice::StartVsa, sender, vsaInfo);Simulator::Schedule (Seconds (3.0), &amp;WaveNetDevice::StopVsa, sender, SCH1); VsaInfo VsaInfo的定义如下： 12345678910struct VsaInfo&#123; Mac48Address peer; ///&lt; peer OrganizationIdentifier oi; ///&lt; OI uint8_t managementId; ///&lt; management ID Ptr&lt;Packet&gt; vsc; ///&lt; VSC uint32_t channelNumber; ///&lt; channel number uint8_t repeatRate; ///&lt; repeat rate enum VsaTransmitInterval sendInterval; ///&lt; send interval&#125; 其中： peer: 为发送目标地址，一般是广播地址 oi: 服务提供者的组织ID managementId: manage id vsc: 需要发送的包内容 channelNumber: 目标信道，这里指需要指定的SCH信道 repeatRate: 发送速度，即每秒多少个 sendInterval: 这是一个枚举类型，指定了包应该在哪些时隙上发送。其取值包括： VSA_TRANSMIT_IN_CCHI VSA_TRANSMIT_IN_SCHI VSA_TRANSMIT_IN_BOTHI WaveNetDevice::StartVsa 传入参数vsaInfo中规定了发送WSA包的必要信息。在WaveNetDevice::StartVsa函数中，设备检查了vsaInfo的数据的完整性，然后交给m_vsaManager-&gt;SendVsa来进行发送。在VsaManager中，CCHI和SCHI的访问控制，以及txVector的控制信息都在这里实现，同时，此包被赋予了EDCA最高优先级(AC_V0)。然后交给OcbWifiMac-&gt;SendVsc来执行发送。通过这个接口发送的包会被标记为管理包。 WAVE中的信道管理 ChannelCoordinator负责控制信道切换的时机，让所有的节点的信道切换同步，而ChannelScheduler负责执行具体的信道切换。在DefaultChannelScheduler::SetWaveNetDevice中，ChannelCoordinator和ChannelScheduler得以联系起来： 12m_coordinationListener = Create&lt;CoordinationListener&gt; (this);m_coordinator-&gt;RegisterListener (m_coordinationListener); ChannelCoordinator在调用其DoInitialize函数内部完成初始化之后即通过StartChannelCoordination函数开启往复调用的信道协调同步过程。这个过程首先进入的是Guard Interval(NotifyGuardSlot)。 1234567891011121314151617181920voidChannelCoordinator::NotifyGuardSlot (void)&#123; NS_LOG_FUNCTION (this); Time guardSlot = GetGuardInterval (); bool inCchi = ((m_guardCount % 2) == 0); if (inCchi) &#123; m_coordination = Simulator::Schedule (guardSlot, &amp;ChannelCoordinator::NotifyCchSlot, this); &#125; else &#123; m_coordination = Simulator::Schedule (guardSlot, &amp;ChannelCoordinator::NotifySchSlot, this); &#125; for (ListenersI i = m_listeners.begin (); i != m_listeners.end (); ++i) &#123; (*i)-&gt;NotifyGuardSlotStart (guardSlot, inCchi); &#125; m_guardCount++;&#125; 在这个函数里面根据inCchi来决定调度NotifyCchSlot还是NotifiySchSlot。在函数最末，通告所有的Listener，Guard Interval开始了。后续的NotifyCchSlot和NotifiySchSlot的做法也是类似的。 在DefaultChannelScheduler::NotifyGuardSlotStart中，开头的Guard Interval长度被设置为繁忙mac-&gt;MakeVirtualBusy (duration);。实际的信道切换过程也在这个函数中通过调用DefaultChannelScheduler::SwitchToNextChannel来进行。由于Guard Interval区间内被设置为繁忙，所以当设备结束睡眠时，检测信道繁忙，故而会开始执行退避。 这里有一个疑问：ChannelCoordinator内部的Schedule调度完全是独立进行的，如果采用了Extended Access，即CCHI会提前结束，那么原定的Guard Interval还是会被设置么？]]></content>
  </entry>
  <entry>
    <title><![CDATA[Wireless Access in Vehicular Environments (WAVE)]]></title>
    <url>%2Fknowledge-base%2Facademic%2Fits%2Fwave%2Fwave.html</url>
    <content type="text"><![CDATA[WAVE的全称是Wireless Access in Vehicular Environment，是目前车联网通信的标准。 WAVE与其他车联网通信中的技术/标准之间关联 在车联网研究中常见的其他技术关键词还包括: DSRC IEEE802.11p IEEE1609.x 物理层特点 WAVE在物理层使用IEEE802.11p的标准。其物理层的一些关键信息如下。 信道划分 Slot Allocation WAVE使用的频段为5.85GHz至5.925GHz。这一频段被进一步划分为7个信道，包括1个控制信道CCH (Control CHannel), 和6个服务信道SCH(Service CHannel)，具体的信道划分如下所示: FCC规定的WAVE信道划分 5.850GHz - 5.855GHz 部分为保留部分(reserved) 信道178为CCH控制信道，带宽为10MHz 信道172, 174, 176, 180, 182, 184为SCH服务信道，带宽都为10MHz 信道174和176，信道180和182可以合并为20MHz带宽的信道 信道172和184，即最外侧的两个信道用于传输与生命财产关联的安全信息。特别的，根据FCC 06-110，信道172用于V2V的安全信息传输，信道184则是高发射功率，长距离通信，用于公共安全信息的传输，例如避免路口碰撞事故。 WAVE协议规定了SCH和CCH的划分，但是对于在不同SCH的不同业务规则的要求不是强制的。 信道类型 CCH-控制信道 CCH信道只有一个，位于整个WAVE频谱的居中位置，带宽为10MHz。 CCH信道上禁止发送IP消息，主要用来发送一些至关重要要的信息（常常是安全相关的），以及服务广告(Service advertising)消息WSA(WAVE Service Advertisement). SCH-服务信道 SCH信道共六个，主要用来发送与基于IPv6(或者WSMP)的非安全信息。SCH信道调度使用机制的研究参见WAVE中的SCH调度机制研究状况 关于信道的业务类型 WAVE协议并未对于各个信道上运行的业务的具体类型做出具体的强制要求。目前来看除了禁止在CCH上发送IP协议包以外，并未有其他更多的强制要求。例如，WAVE标准并未划分专门的安全信道。前文提到CCH上会发送安全相关的信息，SCH主要用来发送非安全信息，但是在实际使用中SCH上仍然可以用啦发送安全信息。在美国172信道甚至被视为安全信息的专用通道。管理信息同样也可以在SCH上传输。 信道协调(Channel coordination)与时间同步(Time Synchronization) WAVE中的多信道访问以IEEE1609.4为标准。 信道协调 在使用WAVE进行通信时，时域和频域资源都进行了划分。WAVE设备可以在不同的信道上切换(Alternative radio channel access)。这使得单设备节点可以在SCH信道上交换信息的同时，可以监听CCH信道。 多信道访问通过共同点时间基准来进行协调。具体而言，WAVE规定由一个CCH间隔(CCHI)，跟着一个SCH间隔(SCHI)组成一个同步间隔SI。在CCHI，所有节点在CCH上通信；在SCHI，节点在指定的SCH上进行通信。在CCHI和SCHI开头设置了一段保护间隔(GI)。在保护间隔内，设备在进行信道切换，被认为无法接收数据包。 根据IEEE 1609.4-2010标准，CCHI和SCHI的长度均为50ms，保护间隔为4ms，如下图所示 Channel Interval Structure 标准允许将信道间隔（CI）设置为可变的长度，但是并没有给出对应的实现机制。 上述的接入方式被称为Alternating channel access，除此之外，还包含了continuous channel access, immediate access, extended access。其中，immediate access允许设备在WSA消息中识别出一个应用请求时，立刻切换到SCH信道。extended access则允许设备延长访问SCH的时间。 More Channel Accessing Schemes 装备有多个通信设备的节点可以并行地访问CCH和SCH 时钟同步 各个节点的时钟同步到UTC时间的标准秒（这点在上图得到了反映）。UTC时间可以由GPS给出。 IEEE802.11-2012标准中的Timing Advertisement frame可以用于节点间的时钟同步。 注意，严格的时钟同步对于通信来说并非是必须的。信道协调只需要精确到1s的边界就可以了。不过严格的时钟同步可能对于安全目的来说是必须的。 MAC层特点 WAVE的MAC层的核心DCF(Distributed Coordination Function)技术，DCF是基于指数退避算法的CSMA/CA机制，这一机制的内容可以从以下几方面来讨论: 指数退避机制 需要发送包的节点监听信息，如果信道空闲的时间超过了DIFS (Distributed Interframe Space)，那么节点开始发送，否则如果信道繁忙，则继续保持监听信道。 当节点得以占用信道准备发送时，节点会在发送前进行一个随机的退避（Collision Avoidance），来减少和其他节点冲突的概率。另一方面，为了避免信道长时间被特定的节点占用（信道被长时间占用的现象被称为Channel Capture），一个节点在两次发包之间也要有一个随机的退避。 出于性能原因，CSMA采用了离散的退避时间。紧跟在DIFS之后的时间被划分为多个时隙，每个节点只能在每个时隙的开始的阶段发送数据包。时隙的大小\(\sigma\)，等于每个节点探测到另一个节点发送的包需要的延时。这一参数取决于物理层，包含传输延时，TX/RX切换时间，以及通知MAC层信道状态需要的时间。 在每个包传输时，退避时间在\((0, \omega-1)\)中随机选出，其中\(\omega\)被称为竞争窗口(Contetion Window)。竞争窗口的大小与传输失败的次数有关。初始时\(\omega=CW_{min}\)。每经过一次传输失败，竞争窗口的大小加倍，直到达到最大值\(CW_{max}\)。 退避的状态由一个退避计数器(backoff counter)来控制。在开始退避时，为退避计数器随机分配一个退避窗口范围内的整型值。之后每一个时隙，如果信道空闲退避计数器就递减。当检测到信道繁忙时，退避计数器的递减过程会停止。当信道再次变得空闲时，退避计数器需要检测信道空闲超过DIFS之后才会再次开始递减。当计数器递减至0时，节点会尝试发送。如果失败，竞争窗口会加倍。并重复前述过程。 ACK CSMA中并不依赖对于自己的传输的监听来检测碰撞。因此引入了ACK机制来通知包发送成功（显然只有D2D方式的通信才会有ACK，在广播包不会产生ACK）。这种机制被称为是Basic Access。接收端在收到包之后经过一段SIFS（Short Inter-Frame Space）立即发送ACK。SIFS比DIFS要短，因此在ACK发送前不会有节点能够占用信道。如果在一定事件后发送节点仍然没有收到ACK，则判断此包丢失。 RTS/CTS RTS/CTS的全称是Request To Send / Clear To Send。这套机制的作用是两方面的： 减少在传输较大体积的包过程中的冲突 解决隐藏终端问题 RTS/CTS的工作过程如下。节点在发送数据包之前，先向目标节点发送一个RTS请求，目标街点收到RTS之后判断当前是否能够进行RTS中请求的包传输，如果可以，目标节点会响应一个CTS。发送者在收到CTS之后才获准发送数据包。RTS和CTS中包含了待传输的包的长度信息，这些信息可以被周围的节点截图，从而更新其Network Allocation Vector (NAV)。 MAC层性能的理论推导 针对MAC的层的性能推导，基本源自于下面Performance analysis of the IEEE 802.11 distributed coordination function[1]. 这篇文章给出了在一定的驾驶条件下DCF机制的饱和吞吐率 饱和吞吐率 一般而言，随机接入协议的一个共同问题是，随着业务量的上升，系统的整体throughput会逐渐上升至maximum throughput，之后继续提高业务量，系统整体的吞吐率并不会继续上升，反而会下降。因此，在实际场景中，系统不可能长时间的保持在最大吞吐率的水平。最大吞吐率也不太适合用作评估接入机制性能的指标。 随着业务量的进一步上升，吞吐率会进入一个比较稳定的水平。这个水平被称为饱和吞吐率（Saturation Throughput） 饱和吞吐率 假设条件 完美的信道条件 不考虑隐藏终端问题 不考虑Channel capture 节点数量固定，且达到满载，即，在一个包发出去之后，总有另一个包等待发送 包传输概率 节点数量为\(n\)，在饱和的情况下，每个节点在每次传输成功以后都可以立即生成一个数据包以供发送。每个包在开始发送前都需要经历一个随机的退避。我们定义\(b(t)\)为节点的退避计数器状态，\(t\)和\(t+1\)表示连续的两个时隙的起始。这里的“时隙”与前面的定义不同，在前文中我们提到过，在信道状态为busy时，退避计数器的递减过程是停止的。我们在这里不将这些部分的时间纳入时隙中，时隙被定为退避计数器相邻两次递减的时间差，这意味着，slot times和系统的时钟是不同步。两个连续的时隙的起始时刻相差，并不总是​\(​\sigma\)，而且可能包含一个完整的发送过程。 由于退避计数器与传输历史有关，那么随机过程\(b(t)\)显然是非马尔科夫的。记 \[ W=CW_{min} \\ CW_{max} = 2^mW \] 然后记： \[ W_i=2^iW \] \(i\in(0,m)\)被称为退避阶段(backoff stage)。我们记\(b(t)\)为表述退避阶段在\(t\)时刻取值的随机变量。我们的一个核心的近似是一个核心的近似是，假设在对于每一次传输尝试，无论重传次数如何，每个包的碰撞概率都是 constant and independent 的，此概率值为​\(p\)​。直观的来看，这个假设在​\(W\)和\(n\)增加时会更加精确。这一概率值又可以被称为条件碰撞概率（conditional collision probability），即此概率为待发送包观察到的碰撞概率。 一旦独立性得到满足，且​\(p\)​是一个常数值，我们可以把一个二元随机过程​\(\{s(t),b(t)\}\)建模成离散时间的马尔科夫链。其状态转移图为 马尔科夫链状态转移图 概率转移矩阵中的非零元素为： \[ \left\{ \begin{array}{ll} P\{i, k | i, k + 1\} = 1 &amp; k\in(0, W_i - 2), i\in(0, m) \\ P\{0, k | i, 0\} = (1 - p) / W_0 &amp; k\in(0, W_0 - 1), i\in(0, m) \\ P\{i, k|i-1, 0\} = p / W_i &amp; k\in(0, W_i - 1), i\in(1, m) \\ P\{m, k | m, 0| = p/W_m &amp; k\in(0,W_m -1) \end{array} \right. \] 第一个等式代表在每个slot time的开头要递减backoff counter. 第二个等式代表在一次成功的传输以后，紧随一个新的packet 第三个等式代表退避完成之后信道仍然无法占用增加backoff stage。 第四个等式也是代表退避完成之后信道仍然无法占用，不过backoff stage已经最大 记\(b_{i,k}=\lim_{t\to\infty}P\{s(t)=i, b(t)=k\}, i\in(0, m), k\in(0, W_i-1)\)代表了马尔科夫链的稳态分布，下面给出该马尔科夫链的闭式解： 首先: \[ b_{i-1}\cdot p=b_{i,0} \to b_{i_0}=p^ib_{0,0},\ \ 0&lt;i&lt;m \\ b_{m-1,0}=（1-p)b_{m,0}\to b_{m,0}=\frac{p^m}{1-p}b_{0,0} \] 马尔科夫链的规律，对于每个\(k\in(1,W_i-1)\) \[ b_{i,k}=\frac{W_i-k}{W_i}\cdot \left\{ \begin{array}{ll} (1-p)\sum_{j=0}^{m}b_{j,0} &amp; i=0 \\ p \cdot b_{i-1,0} &amp; 0 &lt; i &lt; m \\ p \cdot (b_{m-1, 0} + b_{m,0}) &amp; i=m \end{array} \right. \] 在平稳情况下，考虑\(\sum_{i=0}^{m}b_{i,0}=b_{0,0}/(1-p)\)，则有 \[ b_{i,k}=\frac{W_i-k}{W_i}b_{i,0},i\in(0,m),k\in(0,W_i-1) \] 经过上面的推导，我们可以把\(b_{i,k}\)表达为\(b_{0,0}\)的函数，同时通过概率的正则化条件可以求解\(b_{0,0}\)： \[ 1=\sum_{i=0}^{m}\sum_{k=0}^{W_i-1}b_{i,k}=\sum_{i=0}^{m}b_{i,0}\sum_{k=0}^{W_i-1}\frac{W_i-k}{W_i}=\sum_{i=0}^{m}b_{i,0}\frac{W_i+1}{2}\\ =\frac{b_{0,0}}{2}\left[W\left(\sum_{i=0}^{m-1}(2p)^i+\frac{(2p)^m}{1-p}\right) + \frac{1}{1-p}\right] \] 故 \[ b_{0,0}=\frac{2(1-2p)(1-p)}{(1-2p)(W+1)+pW(1-(2p)^m)} \] 然后我们可以得到一个节点在任意时刻尝试发送的概率 \[ \tau=\sum_{i=0}^{m}b_{i,0}=\frac{b_{0,0}}{1-p}=\frac{2(1-2p)}{(1-2p)(W+1)+pW(1-(2p)^m)} \] 注意\(m=0\)是0时，即即退避的窗口是固定的时候， \[\tau=\frac{2}{W-1}\] 注意到\(p\)的含义其实是，在一个时隙中一个节点正在发送时，剩下的\(n-1\)个节点至少有一个在发送，则 \[ p=1-(1-\tau)^{n-1} \] 这里的\(\tau\)和\(p\)构成了一种非线性方程组，可以用数值方法求解。 吞吐率计算 记\(S\)为归一化的系统吞吐率，定义为信道中用于传输数据的时间的比例。在一个随机选取的时隙上，记\(P_{tr}\)为至少有一个节点尝试发送的概率，则 \[ P_{tr}=1-(1-\tau)^n \] \(P_s\)表示传输成功的概率，则 \[ P_s=\frac{n\tau(1-\tau)^{n-1}}{P_{tr}}=\frac{n\tau(1-\tau)^{n-1}}{1-(1-\tau)^n} \] 我们可以把\(S\)表示成 \[ S=\frac{E[\text{payload information transmitted in a slot time}]}{E[\text{length of a slot time}]} \] 记包的平均传输时间是\(E[P]\)，俺么在一个slot time里面用于传输的平均时间长度是\(P_{tr}P_{s}E[P]\)。slot time的平均大小是 \[ (1-P_{tr})\sigma+P_{tr}P_sT_s+P_{tr}(1-P_s)T_c \] 其中，\(T_s\)是信道检测为繁忙后的平均slot time长度（其内部包含了一整个包传输的时间），\(T_c\)为信道发生碰撞以后被识别为繁忙的时间。那么 \[ S=\frac{P_sP_{tr}E[P]}{(1-P_{tr})\sigma+P_{tr}P_sT_s+P_{tr}(1-P_s)T_c} \] 注意上面的推导过程中我们其实并未指定特定的接入机制。对于特定的接入机制，我们只需要确定与之相关的\(T_s\)和\(T_c\)。 \(T_s\)和\(T_c\)的确定 只考虑存在ACK机制下， \[ \left\{\begin{array}{l} T_{s}^{bas}=H+E[P]+SIFS+\delta+ACK+DIFS+\delta\\ T_c^{bas}=H+E[P^*] + DIFS +\delta \end{array}\right. \] 其中\(H=PHY_{hdr}+MAC_{hdr}\)为物理层和MAC层的帧头传输时间。\(\delta\)是传输延时，\(E[P^*]\)是the average length of the longest packet payload involved in a collision. 如果所有的包的大小都是一样的，那么\(E[P^*]=P\)。在更一般的情况下所有包的大小可以被认为是独立同分布的。记这个分布为\(F(\cdot)\)，则 \[ \begin{aligned} E[P^*] &amp;=E[E[\max(P_1,\dots,P_k|k]]\\ &amp;=\frac{\sum_{k=2}^{n}\left(\begin{array}{c}n\\k\end{array}\right)\tau^{k}(1-\tau)^{n-k}\int_{0}^{P_{max}}(1-F(x))^kdx}{1-(1-\tau)^n-n\tau(1-\tau)^{n-1}} \end{aligned} \] 如果忽略三个或者更多包碰撞的情况，那么上面的式子可以简化为： \[ E[P^*]=\int_0^{P_{max}}(1-F(x)^2)dx. \] \(T_c\)是由未参与到碰撞中的节点感知到的信道繁忙时间，我们忽略了两个或者更多个的碰撞节点在感知信道前需要等待ACK超时这个因素，所以实际中的\(T_c\)要不这里给出的计算结果要大。 再考虑在RTS/CTS机制下： \[ \left\{ \begin{array}{l} T_s^{rts}=RTS+SIFS+\delta+CTS + SIFS + \delta + H + E[P] + SIFS + \delta+ACK+ DIFS + \delta \\ T_c^{rts} = RTS + DIFS + \delta \end{array} \right. \] 对于Beacon包的特殊分析 很多文献中对于Beacon包的发送过程做了简化，即发送过程中只进行一轮退避，没有退避窗口的指数增长过程。此时Beacon包的发送状态成为一维马尔科夫过程。这部分问题参见：WAVE标准中的Beacon性能简化分析 结果分析 饱和吞吐率与节点数量的关系 从上图中我们可以发现，对于Basic Access方式，饱和吞吐率和节点数量呈负相关，也即和节点密度呈负相关。这意味着节点数量越少，饱和吞吐率越高。相反，对于RTS/CTS机制而言，在节点密度达到一定的程度以后，饱和吞吐率就和节点数量无关了。 这个性能差异我们在IEEE802.11 DCF中Basic Access和RTS/CTS机制的理论饱和吞吐率性能差异分析这篇文章中进行了详细的分析。 Reference [1] G. Bianchi, “Performance analysis of the ieee 802.11 distributed coordination function,” IEEE Journal on Selected Areas in Communications, vol. 18, no. 3, pp. 535–547, 2000.]]></content>
  </entry>
  <entry>
    <title><![CDATA[淠史杭自流灌溉工程]]></title>
    <url>%2Fknowledge-base%2Fhistory%2F%E4%B8%AD%E5%9B%BD%E5%8F%B2%2F%E8%BF%91%E4%BB%A3%E5%8F%B2%2F%E7%8E%B0%E4%BB%A3%2F%E5%B7%A5%E7%A8%8B%2F%E6%B7%A0%E5%8F%B2%E6%9D%AD%E8%87%AA%E6%B5%81%E7%81%8C%E6%BA%89%E5%B7%A5%E7%A8%8B.html</url>
    <content type="text"><![CDATA[原文出处（来自知乎-马前卒的回答） 淠【pì】史杭自流灌溉工程。是1958年大跃进时想出来的疯狂项目。61年曾被点名要求下马停工，但当地的干部和民工最终咬牙坚持到底，在大别山外围建出了一个超级丰产区。 在毛主席 “一定要把淮河修好” 的伟大号召下，安徽人民先后在大别山区修建了佛子岭、梅山、响洪甸、磨子潭、龙河口五大水库，拦蓄大别山区上游来水，减轻淮河干流防洪压力。五大水库的修建，拓展了安徽省委、省政府的治水思路，为综合利用水资源，彻底解除江淮丘陵地区的旱涝灾害，在科学规划的基础上作出了战略决策，决定兴建淠史杭沟通工程，连接五大水库，沟通淠河、史河、杭埠河三大自然河流，建设完整、系统、科学的灌溉网络。1958年6月，淠史杭工程指挥部正式成立。 1958年8月19日，灌区工程总指挥赵子厚在淠河灌区渠首横排头开挖第一锹土，拉开了淠史杭工程的建设序幕，百万安徽人民开始了建设淠史杭工程的伟大征程。当时，正值国家三年自然灾害，资金困难，技术匮乏，材料短缺，安徽人民发扬“自力更生、艰苦奋斗”精神不卑不弃，坚忍不拔。面对原始的施工工具，他们开展技术革新，创造了专攻切岭工程的“洞室爆破法”，发明了专攻“麻僵土”的劈土法，研制了垂直运输土工具“倒拉器”，以自己的聪明才智克服了工程建设中的千难万险；面对短缺的建筑材料，他们无私奉献，自制炸药，自建水泥厂，捐献木料，保证了骨干工程和重要建筑物的建筑用材；在1958—1972年14年间，他们胼手胝足，肩挑手抬，艰苦奋战，以铁锹、十字镐、独轮车等原始工具，大干4亿个工日，开挖了2.5万公里渠道，完成近6亿\(m^3\)土方，建成了横跨江淮、沟通三河、造福皖豫的特大型灌区，创造了新中国水利史上的人民丰碑。一代文豪郭沫若在参观淠史杭工程时赋诗称赞：“人民力量不寻常”。 淠史杭灌区工程示意图 淠史杭灌区工程示意图 水脉图和地形图对照 水脉图和地形图对照，可以看到这是一个结合山区水库和平原灌溉网，沟通不同水系的自流灌溉项目。这个工程主体也是靠人力和农业社会的物资完成的。五六十年代的中国也实在没有多少工业物资可投入。 1958年，入夏以后，连续70多天无透雨，塘坝干涸，田地龟裂，庄稼枯死，500多万亩农田成灾，尚有152万亩不能下种，全区有200万人投入抗旱。 1958年8月19日，淠史杭工程开工典礼在六安县苏埠镇南５公里的横排头隆重举行，这里曾是当年徐向前创造“苏家埠48天战役”大捷的战场，而今为了创造建设新中国的奇迹，六安人民再一次在这里誓师，向大自然宣战。中共六安地委第一书记杜维佑为典礼剪彩，专员兼淠史杭工程指挥赵子厚发布开工命令，挥锹破土。 开工初期，国家没给一分钱，地区财政也拿不出钱，所需物质基本上靠群众自筹。据统计，在一、二两个工期，群众自筹石料14.22万立方米，木材2.16万立方米，树材22万多棵，圆竹65万公斤，毛竹12万多根，旧钢铁437吨。开山炸石，没有炸药，就发动群众刮墙土熬硝，自制土炸药。没有水泥，就自建了三个水泥厂，用石臼舂，用碾子碾，石料破碎以后，用筛子筛，用手工搅拌。木料不足，许多群众把家中盖房子的木头、门板都献出来了，有的甚至把给老人准备的棺材都拆散抬到工地。11条干渠全面铺开，最高上工人数达67.8万，民工们挑着粮食和铺盖，从四面八方聚集到工地，每个民工一个工日补助0.25-0.5公斤口粮和一毛钱菜金。 下图是当年的施工场面和奖状。 施工图 奖状 淠史杭工程的影响有多大呢？ 中国淠河、史河、杭埠河3个毗邻灌区的总称。位于安徽省中西部和河南省中南部，地处大别山余脉的丘陵地带，横跨长江、淮河两大流域，总面积1.31万平方千米。灌区水源来自佛子岭水库、响洪甸水库、磨子潭水库、梅山水库和龙河口水库。总库容66亿立方米。该灌区于1958年开工 ，1959年开始灌溉农田，以后逐年续建配套，1987年干渠以上工程完工，支渠以下工程还在进行。工程以灌溉为主，兼有发电、航运、水产养殖、城市用水的综合利用工程，包括：淠河、史河、杭埠河三大渠首枢纽工程，7级渠道的2条总干渠，11条干渠，19条分干渠，总长1384千米；1.3万条支渠、斗渠、农渠，总长2.26万千米；大小渠系建筑物6万多座；中小型调节水库1066座和21万口塘坝，有效库容12.3亿立方米；抽水站、补水站总装机容量14.1万千瓦。该工程实现了40万公顷农田的自流灌溉，实灌面积达58万公顷，并解除了淠、史河下游的洪灾，减轻了淮河干流的洪灾。. 四五十万公顷农田从旱地变成旱涝保收的水浇地，这个说法貌似不容易形成直观概念。我们可以和名声更大的都江堰灌区（不只是都江堰枢纽）对比。到1950年的时候，都江堰灌区修了2000多年，总灌溉面积是18万公顷（解放后扩建到60万公顷）。这还依赖于川西极有利的地形。淠史杭工程用14年时间在贫瘠的大别山外围搞了三个都江堰灌区出来，不依赖电力的自流灌溉面积也相当于2个都江堰灌区。效果不言而喻。 灌区枢纽之一，颇有都江堰的味道 灌区图片 灌区工程的建设和运行，大大改善了皖西、皖中地区的农业生产条件，区域内的灌溉面积由兴建时的120万亩发展到现在的1000万亩，粮食年产量由兴建时12.3亿公斤提高到目前的50亿公斤左右，灌区17个县（区）有7个是国家级商品粮生产基地县，正常年份提供的商品粮不少于18亿公斤。淠史杭灌区是安徽乃至全国的粮食主产区，它以占安徽1/10的土地面积、1/6的耕地面积、1/5的有效灌溉面积生产1/4的粮食产量、1/3的水稻产量，奠定了安徽粮食主产省的重要地位（河南效果另计）。]]></content>
  </entry>
  <entry>
    <title><![CDATA[长江改道工程]]></title>
    <url>%2Fknowledge-base%2Fhistory%2F%E4%B8%AD%E5%9B%BD%E5%8F%B2%2F%E8%BF%91%E4%BB%A3%E5%8F%B2%2F%E7%8E%B0%E4%BB%A3%2F%E5%B7%A5%E7%A8%8B%2F%E9%95%BF%E6%B1%9F%E6%94%B9%E9%81%93%E5%B7%A5%E7%A8%8B.html</url>
    <content type="text"><![CDATA[原文出处（来自知乎-马前卒的回答） “万里长江，险在荆江”，荆江是长江中游的一段，上起湖北枝城，下迄湖南城陵矶。长江从三峡奔腾而下，进入“荆江”河段后水流湍急，走向摆动不定，导致河流蜿蜒曲折，仅从藕池口至城陵矶之间直线距离只有80公里，长江河流长度却达247公里，其间大弯小曲不下20余处。而监利县上车湾长江段（即环绕湖南华容集成垸河段），水道环绕则要四十多公里，而陆地4公里就可达到彼端。因此荆江又有“九曲回肠”之称。这样弯曲的河道既不利于航行，又由于水流不畅，洪水期极易泛滥成灾。 早在1936年我国水利专家章锡绶先生就提出“扬子江之患，荆河为甚，而上车湾实荆河之最险处，宜速积极开凿对岸之引河，以作根本之解决。”否则 荆江堤防若溃决，长江一泻千里，江汉平原将尽成泽国，而汉口市场，亦遭池鱼之叹。 章锡绶先生在长江上车湾河段实施裁弯截直工程的提议，经当时国内水利专家们和美国水利总工程师史笃培等详细研究，实地勘估，都认为确有必要实施，并制定了详细的工程方案和预算报告呈送至国民政府。但因当时国家处于连年内乱外患，对这一长江改道工程在国民党政府手中一直未能解决。 三十二年后，章锡绶先生提出的这个荆江裁弯治理的蓝图，终于在新中国政府手中得以实施。1968年底，国家正式批准实施长江上车湾河段改道工程。该工程是从湖南华容县境的长江天字一号段起，向南至洪山头段为止，开挖出5公里长、宽100多米、约20米深的人工河道，将这一段长江进行裁弯截直。为了实施这一工程 ，湘鄂两省政府动员了数万农民开挖河道。1968年底，不少长沙知青到华容插队落户“接受贫下中农再教育”，其中许多知青曾参加这一著名的长江改道开挖新河的劳动。 我下放所在的集成垸红旗大队，是离长江改道工地最近的几个大队之一。我到生产队的第三天，生产队长就安排了我们几个男知青去参加长江改道工程的劳动。我们到达工地，见数万民工如蚂蚁般的在狭长的荒洲上劳动，人山人海，场面很是壮观。当时开挖新河道工程已完成了一大半，河道已初见雏形。开挖河道就是将河床的泥土运至新河道两岸百余米的地方堆积成堤坝，那时没有使用一台所谓挖土机、推土机等机器设备，完全是用人工手挖肩挑。河道底部的泥土粘而含有水分，因此挖土不用锄头而用当地特有的一种挖土工具--铁锨铲，象切豆腐一样，把一大块一大块泥土切割，再装入筐萁运走。我们知青刚去还不会用铁锹就只能用肩去挑土了。其实挑土也很辛苦，从河床底部要将泥土挑到倒堆积土处，路途长，坡又陡，当时运土是采用分级接力运输办法，即一人挑一担土向上运送一段，另一人又将这担土接力运送上去。由于前几天刚下过一场大雪，运土的路都很泥泞，一不小心，鞋就陷进泥浆中拔不出来。知青们大多是刚从事这么繁重的劳动，整天用肩挑泥土，在泥泞中爬坡，感到很辛苦。我一、二天后肩膀就压红肿了，收工后躺在工棚里就不想爬起来………… 到1969年春节，新河道挖掘完工了，仅留下靠长江两端的河道未挖通，暂时以抵挡江水。我们集成垸的知青回长沙过春节，或去砖桥公社办事，来回都要从新开挖的河道中爬下爬上走过。直到那年3月份长江的江汛来了，长江水位达到一定标准后，有关部门动用炸药将新河道两端的泥坝炸毁。随着几声沉闷的爆炸声，滚滚长江从新河道奔流而下，新河道启用了。汹涌湍急的江水冲刷着河道两岸，更一步拓宽着河道，裁弯截直的河道才发挥作用。以后当地群众把这一段新开的长江河段称为“新河”，以后设立的渡口又叫“新河渡口”。 原文附图，红色部分就是农民和下乡知青一铲一铲挖出来的新长江。 然而这还不是全部的荆江截弯工程。看下面的百度地图和卫星图： 地图 卫星地图 连绵的长江牛轭湖（即截弯后的旧河道），约有一半是六七十年代战天斗地的成就，是无数农民和知青用肩膀挑出来的地理景观。这才是在太空中也能看得到的人力工程奇迹，从工程方面比较，长城瞬间就被比下去了]]></content>
  </entry>
  <entry>
    <title><![CDATA[九一八事变前蒋张关于东北问题的讨论]]></title>
    <url>%2Fknowledge-base%2Fhistory%2F%E4%B8%AD%E5%9B%BD%E5%8F%B2%2F%E8%BF%91%E4%BB%A3%E5%8F%B2%2F%E6%B0%91%E5%9B%BD%2F%E5%8E%9F%E5%A7%8B%E5%8F%B2%E6%96%99%E9%9B%86%2F%E4%B9%9D%E4%B8%80%E5%85%AB%E4%BA%8B%E5%8F%98%E5%89%8D%E8%92%8B%E5%BC%A0%E5%85%B3%E4%BA%8E%E4%B8%9C%E5%8C%97%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%A8%E8%AE%BA.html</url>
    <content type="text"><![CDATA[其一 原文链接：知乎-理水的文章 日人对我国东北虎视眈眈已久。1931年夏以来，在东北，日本先后挑起万宝山事件和中村事件，在华北，挑动石友三叛乱，策动阎锡山回晋，迫使张学良抽调东北军精锐入关弹压局面。在这种情况下，蒋介石、张学良都预感到东北将有非常事件，并多次就东北问题进行讨论，最后达成采用“以不抵抗为抵抗”，寄望国联、依靠外交解决的战略方针。结合对岸新公布的资料，大致梳理一下九一八前蒋、张对于东北问题的讨论历程。 进入8月，东北局势日益紧张，驻沈独立第七旅王以哲派人到北平向张学良汇报，时任该旅参谋长的赵镇藩在《日军进攻北大营亲历记》中回忆： 基于上述种种情况，王以哲和我反复进行了研究，认为如果万一发生事变，我旅必将首当其冲。为了研究对策，遂于8月间召集第七旅上校以上军官和情报人员共同分析研究。大家一致判断必然要发生事故，当即将所得材料加以整理，交王以哲携赴北平向张学良报告，并请求将关内东北军调回一部分，以防万一。不久王以哲回沈阳，向我们传达了张学良的应变指示。王以哲说：“张副司令已经派人将情况报告了蒋介石，蒋指示暂不抵抗，准备好了再干，一切事先从外交解决；并告诉张学良要效法印度甘地对英国不合作的办法来应付日本，遇事要退让，军事上要避免冲突，外交上要采取拖延方针。”接着又接到张学良转来蒋介石的铣(8月16日)电，主要内容是：采取不抵抗政策，竭力退让，避免冲突，千万不要“逞一时之愤，置国家民族于不顾，希转饬遵照执行”等语。 虽然铣电原件至今尚未发现，但能够证伪该电的材料也并没有，甚至在九一八前，尚未见到一封蒋介石给张学良关于东北事件的电文。而近年来新披露的资料，恰好可以和铣电互相照应，可以说有力的佐证了该电的存在。 关于铣电，虽然原件未曾发现，但是九一八事变前日军方面曾接到了实际上一份“铣电”。这部分内容，参考其二。 8月24日，感到事态日趋严重的张学良致电陈群并转蒋介石，提出： 近来对日外交性情紧迫，彼国朝野上下公然密谋侵占我东北（彼方谓为满蒙），势甚积极，不可终日。弟曾尽力设法以谋疏解，终鲜效果，所有一切经维寅兄电达左右，荷蒙鉴誉，转呈总座，至深佩感。近数日来，情况益紧，……日人方面属有意动作，现已揭开面目，必将另造事端以为借口。似此情形，恐非退避所能了事。弟为此事，日夜焦虑，我兄卓识尽筹，对日外交研究有素，当此危急之时，我方应用何法以为应付，尚祈详赐指示并请密陈总座决定方策。弟意以为对立各种悬案应即与之开诚谈判，能解决者即解决之，其绝对不能许其要求者即拒绝之。为此了一件即少一件，而彼方即少一攻击之目标，是为釜底抽薪之计。总座明烛，几先对此必有良谋，亟望与外交方面负责人员切实商讨，指示遵行，不胜企祷。 这里的“似此情形，恐非退避所能了事”一句，显然意指之前蒋、张达成过“退避了事”的应对方略，这内容同铣电中提到的“采取不抵抗政策，竭力退让，避免冲突”是一致的。 电文1 电文2 8月28日，在官修《事略稿本》中，记载了蒋介石收到中村事件后的方应： 公叹曰：日人之侵略，其将日益加进乎 史略稿本原文 然而，蒋介石、张学良并没有调整应对策略。 9月6日，张学良发出给臧式毅、荣臻的“鱼电”，重申了铣电的要求 ： 现在日方外交渐趋吃紧，应付一切，亟宜力求稳慎。对于日人，无论其如何寻事，我方务当万方容忍，不可与之反抗，致酿事端。即希迅速密令各属，切实遵照注意为要。张学良。鱼子秘印。 不过张学良越发感到形势的严峻，两天之后的9月8日张学良发出给蒋介石的“限即刻到，不得停留”的齐申电，一方面表示已按蒋意下达鱼电（“特饬文武地方官竭力避免”），但同时报告日军有大规模入侵动向，希望蒋介石考虑应对措施： 日人于朝鲜暴动案发生后，百计寻事，特饬文武地方官竭力避免。近为中村失踪之事，由驻沈总领事严重交涉，语多挟制，东京方面陆军人员尤为激昂，显有借端侵略状态。我方已派人前往肇事地点详查，良不能亲自回辽，万分焦急。……内忧外患，应付殊难，仅密奉闻，敬乞指示。 电文3 按说张学良的齐申电规格已经相当之高（“限即刻到，不得停留”），用词已相当激动（内忧外患，应付殊难，仅密奉闻，敬乞指示），以张国府二把手的地位，蒋介石不可能没有表示，然而却没有齐申电的复电，这显然不合情理。但如果考虑何柱国在《“九一八”沈阳事变前后》记录的蒋张在石家庄的一次会面，那么蒋介石在接到张学良的齐申电后，决定和张面谈，就显得顺理成章： 张学良于九月十一日，在北平接到蒋介石的密电，叫他于十二日去石家庄与蒋会晤。那天上午，蒋、张分乘两辆专车，蒋介石的专车自汉口开来，张学良的专车从北平开来，就这样两个人都未下车，把两辆专车合拢后，在车厢里举行了秘密会谈。那时我正驻防在石家庄，得讯后提早在车站外围派部队布防警戒，以保安全；但专车及车站范围内，则均由蒋、张自带的卫队负责。 会谈后，张学良亲自告诉我，蒋介石对他说：“最近获得可靠情报，日军在东北马上要动手，我们的力量不足，不能打。我考虑到只有提请国际联盟主持正义，和平解决。我这次和你会面，最主要的是要你严令东北全军，凡遇到日军进攻，一律不准抵抗，如果我们回击了，事情就不好办了，明明是日军先开衅的，他们可以硬说是我们先打他的，他们的嘴大，我们的嘴小，到那时就分辩不清了。”过了一星期，九一八事变果然爆发了。 接下来，在官修《事略稿本》中，记载了蒋介石9月13日收到关东军高唱“击滅东北政权”的反应： 又闻日人高唱击滅东北政权，公曰：日人欲图东北，而其狡诈手段如此。其谁欺，欺天乎。独恨我国内之正多事耳。呜呼，岂天欲亡我中华乎。 事略稿本原文 9月14日，在官修《事略稿本》中，记载了蒋介石在得知日本进一步举动后方应，蒋介石考虑良久，仍然只是“严密注意”，寄望于这是日本的恫吓之举： 公考虑久之，曰：“日人之鬼魊诈险，吾当严密注意之也。” 事略稿本原文 可见，虽然各方情报让蒋介石虽然也感觉到事态严重，但蒋最后仍感“独恨我国内之正多事耳”，仍维持了既定的不抵抗立场。 不料，仅仅4天之后，蒋、张不愿意面对的“九一八”事变，还是发生了。 其二 原文链接：知乎-理水的文章 摘自《“九一八”全史》第一卷和第五卷（上）。 1、9月事变前陆海空总司令部给东北边防军司令长官公署密电。 密电内容 原文脚注标明密电内容如下：译自《日本外务省档案》，IMT523，《关东厅警务局长致外务次官信》，见该档案胶卷T66。 2、11月2日到5日江桥抗战决策过程 密电内容 密电内容 密电内容]]></content>
  </entry>
</search>
